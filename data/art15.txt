115
Statically Bounded-Memory Delayed Sampling for
Probabilistic Streams
ERIC ATKINSON, MIT, USA
GUILLAUME BAUDART, INRIA, √âcole normale sup√©rieure ‚Äì PSL University, France
LOUIS MANDEL, MIT-IBM Watson AI Lab, IBM Research, USA
CHARLES YUAN, MIT, USA
MICHAEL CARBIN, MIT, USA
Probabilistic programming languages aid developers performing Bayesian inference. These languages provide
programming constructs and tools for probabilistic modeling and automated inference. Prior work introduced
a probabilistic programming language, ProbZelus, to extend probabilistic programming functionality to
unbounded streams of data. This work demonstrated that the delayed sampling inference algorithm could be
extended to work in a streaming context. ProbZelus showed that while delayed sampling could be effectively
deployed on some programs, depending on the probabilistic model under consideration, delayed sampling is
not guaranteed to use a bounded amount of memory over the course of the execution of the program.
In this paper, we the present conditions on a probabilistic program‚Äôs execution under which delayed sampling
will execute in bounded memory. The two conditions are dataflow properties of the core operations of delayed
sampling: the ùëö-consumed property and the unseparated paths property. A program executes in bounded
memory under delayed sampling if, and only if, it satisfies the ùëö-consumed and unseparated paths properties.
We propose a static analysis that abstracts over these properties to soundly ensure that any program that
passes the analysis satisfies these properties, and thus executes in bounded memory under delayed sampling.
CCS Concepts: ‚Ä¢ Theory of computation ‚Üí Program analysis; Streaming models; ‚Ä¢ Software and its
engineering ‚Üí Data flow languages.
Additional Key Words and Phrases: Probabilistic programming, reactive programming, streaming inference,
semantics, program analysis
ACM Reference Format:
Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin. 2021. Statically BoundedMemory Delayed Sampling for Probabilistic Streams. Proc. ACM Program. Lang. 5, OOPSLA, Article 115
(October 2021), 42 pages. https://doi.org/10.1145/3485492
1 INTRODUCTION
Probabilistic programming languages aid developers performing Bayesian inference [Atkinson et al.
2018; Bingham et al. 2019; Cusumano-Towner et al. 2019; Ge et al. 2018; Gelman et al. 2015; Goodman
et al. 2008; Goodman and Stuhlm√ºller 2014; Gordon et al. 2014; Huang et al. 2017; Mansingkha et al.
2018; Milch et al. 2007; Narayanan et al. 2016; Nori et al. 2015; Pfeffer 2009; Tran et al. 2017]. These
languages provide programming constructs and tools for probabilistic modeling and automated
inference. Researchers have developed probabilistic programming languages for several domains,
Authors‚Äô addresses: Eric Atkinson, MIT, USA; Guillaume Baudart, INRIA, √âcole normale sup√©rieure ‚Äì PSL University,
France; Louis Mandel, MIT-IBM Watson AI Lab, IBM Research, USA; Charles Yuan, MIT, USA; Michael Carbin, MIT, USA.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,
contact the owner/author(s).
¬© 2021 Copyright held by the owner/author(s).
2475-1421/2021/10-ART115
https://doi.org/10.1145/3485492
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
arXiv:2109.12473v1 [cs.PL] 26 Sep 2021
115:2 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
including data science [Gelman et al. 2015], machine learning [Bingham et al. 2019; Tran et al.
2017], scientific simulation [Baydin et al. 2019], and real-time control [Baudart et al. 2020].
Probabilistic Programming with Streams. In this paper, we consider programs that accept inputs
and compute outputs at discrete time steps, with the outputs of each step flowing into the environment to affect future inputs to the program. Mathematically, one can model these programs as
computations that operate on and produce infinite streams. Computing with streams is a common
computational model for applications in real-time control, such as robotics and avionics [Cola√ßo
et al. 2017]. For example, control for an airplane fly-by-wire system can be implemented as a
program transforming a stream of altitude measurements into a stream of commands to the engine.
Baudart et al. [2020] introduced a probabilistic programming language, ProbZelus, to enable probabilistic programming in this domain of computations on streams. A key innovation of ProbZelus
was to demonstrate that delayed sampling [Murray et al. 2018] could be extended to work with
streams to provide high-quality inference procedures. Delayed sampling is an inference algorithm
that combines both exact and approximate inference; it takes advantage of exact inference when
efficient known closed-formed solutions exist and falls back on sampling-based, approximate inference when required. Specifically, delayed sampling combines Bayesian networks ‚Äì graphs that
encode exact distributions of probabilistic models ‚Äì with particle filtering [Del Moral et al. 2006] ‚Äì
an approximate inference algorithm.
The challenge in adapting delayed sampling to computations on streams is that such computations
run for indefinite periods of time and are often subject to stringent limits on resources, such as
memory. Baudart et al. [2020] showed that, in many cases, only a finite number of nodes in
delayed sampling‚Äôs graph data structures were reachable at any given time, and the rest could not
influence the computation in the future and could be removed from memory. However, this behavior
depends on the probabilistic model under consideration; delayed sampling is not guaranteed to
maintain a bounded amount of memory for all programs. The result is then that though probabilistic
programming languages are designed to hide the complexities of developing probabilistic inference
algorithms, certain combinations of a model and the inference algorithm will result in undesirable
behaviors that the developer did not anticipate. Moreover, the developer has no means to reason
about these behaviors except by inspecting the implementation of the inference algorithm.
Bounded-Memory Delayed Sampling. In this paper, we formalize semantic conditions under which
applying delayed sampling to probabilistic programs with streams will execute in bounded memory.
The two conditions are dataflow properties of the core operations of delayed sampling: assume,
observe, and value, which respectively add a new random variable to the delayed sampling graph,
observe a random variable, and evaluate a random variable to produce a sampled value. The ùëö-
consumed property states that all variables introduced with assume are eventually consumed by an
observe or a value, or are passed to other assumes resulting in new variables that are themselves
(ùëö‚àí1)-consumed. An unseparated path is a sequence of random variables, each passed as parameter
to the assume operation of the next, where no variable is passed to an observe or value operation.
The unseparated paths property states that no variable maintained in the program state starts an
unseparated longer than some fixed bound ùëõ. A program executes in bounded memory under
delayed sampling if, and only if, it satisfies the ùëö-consumed and unseparated paths properties.
Static Analysis. We propose a static analysis that checks the ùëö-consumed and unseparated paths
properties to soundly ensure that any program that passes the analysis satisfies these properties,
and thus executes in bounded memory under delayed sampling.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:3
Contributions. In this paper, we present the following contributions:
‚Ä¢ We introduce and formalize the ùëö-consumed and unseparated paths properties, and show these
are necessary and sufficient for a program to have bounded-memory execution.
‚Ä¢ We present a static analysis to check these properties, and prove that the analysis is sound.
‚Ä¢ We implement the analysis and evaluate it against several probabilistic inference benchmarks.
Our results show that for eight of nine benchmarks, the analysis determines whether the semantic
properties necessary for bounded-memory execution are satisfied, and we identify the precision
limitation of conservative static analysis on the remaining benchmark.
This work brings probabilistic programming to control settings with the new benefit of static
guarantees on the system‚Äôs resource consumption. To the best of our knowledge, our work is
the first to develop a resource analysis for a probabilistic program in relation to its probabilistic
programming system‚Äôs underlying inference algorithm.
The remainder of the paper is structured as follows. In Section 2, we give an example program
to illustrate the concepts in the paper. In Section 3, we present the syntax and semantics of a
language for probabilistic programming with streams, adapted from the ùúáùêπ language from Baudart
et al. [2020]. In Section 4, we review background on delayed sampling, based on the contributions
from Murray et al. [2018] and Baudart et al. [2020]. In Section 5, we present the ùëö-consumed and
unseparated paths semantic properties. In Sections 6 and 7, we present and evaluate the static
analysis. Sections 8 and 9 summarize related work and present conclusions.
2 EXAMPLE
Figure 1 presents the example of a robot designed to navigate to a desired position target using
measurements obs from a noisy position sensor. The robot issues a command u that indicates the
acceleration to apply to change its position. The robot (1) estimates its current position with a
probabilistic model kalman and (2) uses this estimate to compute the command u with a deterministic
controller (e.g., a Linear-Quadratic Regulator [Sontag 2013], the implementation of which we
have elided for simplicity). We present the example in ùúáùêπ , a purely functional core calculus for
probabilistic programming with streams.
1 val kalman = stream {
2 init = 0.0;
3 step (pre_x, obs) =
4 let x = sample (gaussian (pre_x, 1.0)) in
5 let () = observe (gaussian (x, 1.0), obs) in
6 (x, x)
7 }
8 val robot = stream {
9 init = (0.0, init controller, infer kalman);
10 step ((c, k), (obs, target)) =
11 let x_dist, k' = unfold (k, obs) in
12 let u, c' = unfold (c, (target, mean (x_dist))) in
13 (u, (c', k'))
14 }
Fig. 1. ùúáùêπ program with main stream function robot.
The program is a set of stream function definitions that each consist of (1)
an initializer, and (2) a step function
that given the previous state and an input value produces an output value and
a new state [Mealy 1955]. The operators init and infer instantiate a stream
function by creating an internal state. A
stream function can be applied to an input stream to generate an output stream
with the operator unfold, which applies
the step function using the internal state
and the input values. Unlike init, the
step function of an instance created using infer performs probabilistic inference and thus returns at each iteration
a distribution of outputs and a distribution of states.
The main stream function, robot, has a state composed of two stream function instances: c the
deterministic controller, and k the kalman probabilistic model. The robot initializer creates these
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:4 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
two instances (L.9). The transition function of instance k performs probabilistic inference to infer
a distribution of the robot‚Äôs state x_dist and an updated instance k' (L.11). Then the transition
function of instance c computes a command u to go toward the destination target using statistics
of the position distribution and an updated instance c' (L.12). The transition function of robot
returns the command u and the updated state (L.13).
2.1 Probabilistic Model
The stream function kalman specifies a hidden Markov model [Baum and Petrie 1966], a common
probabilistic model for tracking applications in which the goal is to estimate the trajectory of an
object given noisy measurements of the object‚Äôs position.
The stream function‚Äôs state consists of a latent random variable, pre_x, that denotes the position
of the robot at the previous iteration. The robot‚Äôs state is latent in that the robot is unable to directly
observe its position; instead it must leverage a noisy measurement or observation of its position to
infer a probability distribution over its potential states.
Inside the definition of kalman, the program models the latent nature of x by sampling the
current position from a Gaussian distribution centered around its previous position pre_x (L.4).
The program models the observation by taking the observed sensor value as input, obs, and
supplying it as an input to the observe operator. In this example, the observe specifies that obs is
an observation from a Gaussian distribution centered around the position x. The observe operator
conditions the program‚Äôs execution on the observed value (L.5) in that it adjusts the distribution
that will be inferred for x.
The sequence of diagrams in Figure 2 illustrates the evolution of a representation of the hidden
Markov model over the first four iterations of the program. Each light grey node denotes a latent
random variable for pre_x or x at a given iteration. Each dark grey node denotes an observation
at the given iteration. Each solid black arrow signifies a dependence between random variables
as in a traditional Bayesian network representation of a probabilistic graphical model [Koller and
Friedman 2009]. Of note, each observation at each iteration depends on the current position and
the robot‚Äôs state at a given iteration depends only on its position at the previous iteration.
2.2 Inference with Delayed Sampling
The kalman probabilistic model is not sufficient for the robot to reason about its position. Instead, the
robot must perform inference on the model to compute a posterior distribution of x conditioned on
its observations. As mentioned, the infer operator in the robot stream function applies inference
to the probabilistic model it receives as input. In this paper, we study delayed sampling [Baudart
et al. 2020; Murray et al. 2018] as the algorithmic implementation of the infer operator.
Delayed sampling is an extension of a particle filtering algorithm that leverages symbolic execution to reason about the relationship between random values and perform exact inference if
possible. A particle filter estimates the posterior distribution from a set of particles, i.e., independent
executions of the model. For each particle, delayed sampling operates by dynamically maintaining
a graph ‚Äî i.e., a Bayesian network ‚Äì that records the dependence relationships between the random
variables in the program (Figure 2). The key idea is that rather than sample a concrete value for each
random variable in the program (e.g., x), delayed sampling instead returns a reference to a node
in the graph. This node contains a closed-form representation of the distribution that the sample
operator sampled from, along with the distribution‚Äôs dependence on other random variables in
the program. If a symbolic computation fails, delayed sampling can fall back to a particle filter by
drawing concrete values for the random variables.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:5
x
obs
(a) iteration 1
pre_x
‚úó
‚úó
x
obs
(b) iteration 2
‚úó
‚úó
pre_x
‚úó
‚úó
x
obs
(c) iteration 3
‚úó
‚úó
‚úó
‚úó
pre_x
‚úó
‚úó
x
obs
(d) iteration 4
Fig. 2. The evolution of the delayed sampling graph for the hidden Markov model in Figure 1 (kalman) as
implemented by Baudart et al. [2020]. Each node denotes either a value (dark gray) or a distribution (light
gray). A plain arrow denotes a dependency in the underlying Bayesian network. A dotted arrow denotes a
pointer in the implementation of the delayed sampling graph. Each label indicates the program variable that
corresponds to a node. An ‚úó on a node denotes that the node is not reachable from the program state.
2.3 Bounded-Memory Delayed Sampling
A key concern when applying delayed sampling to streams, which may execute for an indefinite
number of iterations, is if the size of the delayed sampling graph is bounded from above by a fixed
constant for all iterations of the program. If not, then the delayed sampling graph may not consume
bounded memory and the program may exhaust its resources if permitted to execute indefinitely.
In general, bounding memory use is challenging because the underlying Bayesian network can
in fact be unbounded. Nevertheless, a delayed sampling implementation can maintain bounded
memory for some programs, depending on the operation of said programs. In this subsection, we
review the delayed sampling implementation presented by Baudart et al. [2020] which can execute
in bounded memory for some programs.
Bounded-Memory Example. Figure 2 shows how delayed sampling maintains bounded memory
for the program in Figure 1. For each particle, the delayed sampling implementation must keep in
memory all the nodes that are reachable from any node referenced in the program state. The dashed
lines in Figure 2 visualize the reachability relation, where the node each line points to is reachable
from the node the line points from. As the program evolves its state and changes the variables the
state contains, nodes in the delayed sampling graph may become unreachable, marked ‚úó.
Figure 2a shows the delayed sampling graph after the first iteration. The graph consists of two
nodes: one introduced by sampling the variable x, and one introduced by the observation of obs.
At the end of the step, both are in the program state and reachable.
Figure 2b shows the delayed sampling graph after the second iteration. The program has added
two nodes to the graph for sampling x and observing obs. The nodes left over from the first iteration
are still in the graph, but are no longer reachable.
Figures 2c and 2d show the delayed sampling graph at iterations 3 and 4 respectively. In each
case, the most recently introduced nodes for x and obs are reachable, and the nodes from the
previous iterations are unreachable. In general, the program ensures that at any iteration, the most
recently introduced nodes are reachable, and the rest are unreachable. Because there are at most
two reachable nodes for all iterations, inference executes in bounded memory.
Unbounded-Memory Example. Figure 3 presents an example of a program that does not execute
in bounded memory. This is a modified version of kalman from Figure 1 that samples an initial
latent position i from a Gaussian distribution and keeps a reference to this random variable in the
state. Figure 4 shows how the program in Figure 3 fails to maintain bounded memory.
Figure 4a shows the delayed sampling graph after the first iteration. The graph consists of three
reachable nodes introduced by sampling the variables i and x and by the observation of obs.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:6 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
val kalman_first = stream {
init = (true, 0.0, 0.0);
step ((first, i, pre_x), obs) =
let (i, pre_x) =
if first then (let i = sample (gaussian (0.0, 1.0)) in (i, i))
else (i, pre_x) in
let x = sample (gaussian (pre_x, 1.0)) in
let () = observe (gaussian (x, 1.0), obs) in
(x, (false, i, x))
}
Fig. 3. Model with unbounded memory consumption.
i x
obs
(a) iteration 1
i
‚úó
x
obs
(b) iteration 2
i
‚úó ‚úó
x
obs
(c) iteration 3
i
‚úó ‚úó ‚úó
x
obs
(d) iteration 4
Fig. 4. The evolution of the delayed sampling graph for the variant of a Kalman probabilistic model in Figure 3.
Nodes and edges have the same meaning as in Figure 2.
ùúè2 = x1 f nil :: y1 f x1 :: obs y1 :: x2 f x1 :: y2 f x2 :: obs y2
iteration 1 iteration 2
Fig. 5. A depiction of a trace of the program in Figure 1. The figure depicts the trace ùúè2 at the end of iteration 2.
The trace is a ::-separated list of primitive operations, where each primitive operation is a sampling operation
f or an observation operation obs. In this diagram, we use xùëõ and yùëõ to refer to the random variables
introduced at iteration ùëõ by, respectively, sampling x and observing obs in Figure 1.
Figure 4b shows the delayed sampling graph after the second iteration. The program has added
two nodes to the graph for sampling x and observing obs. Since the variable i is in the program
state, the node between i and x is reachable.
Figures 4c and 4d show that in the next iterations two new nodes are introduced at each step and
one remains reachable. The primary observation to note is that the number of introduced nodes
increases at every iteration. Therefore, there is no bound on the size of the delayed sampling graph
and, hence, the program does not execute in bounded memory.
2.4 Analyzing Delayed Sampling
In this paper, we present an analysis that can show that the program in Figure 1 maintains bounded
memory while the program in Figure 3 does not. For that, we define two dataflow properties that
encode whether a program executes in bounded memory: the unseparated paths property and the
ùëö-consumed property. We then show how these properties can be verified using a static analysis.
Traces. We formalize the dataflow properties as properties of traces. A trace is a recording of the
important features of a program execution. In our case, a trace records all sampling and observation
operations that the program has executed, as well as the variables that were involved in these
operations. Figure 5 illustrates a trace of the execution of the program in Figure 1.
Unseparated Paths. An unseparated path in a trace is a sequence of variables {ùë•ùëñ }, where the
trace specifies that each variable ùë•ùëñ was sampled from its predecessor ùë•ùëñ‚àí1 and no ùë•ùëñ
is observed.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:7
ùúè2 = i f nil :: x1 f i :: y1 f x1 :: obs y1 :: x2 f x1 :: y2 f x2 :: obs y2
iteration 1 iteration 2
ùë£2 = (false, i, x2)
Fig. 6. A depiction of a trace of the program in Figure 3. The figure depicts the trace ùúè2 and the value of
the program state ùë£2 at the end of iteration 2. In this diagram, we use i, xùëõ, yùëõ, respectively, to refer to the
random variable introduced by sampling i, the variable introduced at iteration ùëõ by sampling x, and the
variable introduced at iteration ùëõ by observing obs in Figure 3. We have highlighted the elements of the
unseparated path between ùëñ and x2 in green.
The unseparated paths property states that there is a uniform bound ùëê so that for all iterations no
variable in the program state starts an unseparated path with more than ùëê variables in it.
Figure 6 illustrates the trace for the program in Figure 3. This program carries the variable i in
the program state, and because the trace specifies that x1 was sampled from i, and x2 was sampled
from x1, the sequence i, x1, x2 is an unseparated path with 3 variables. In general, at iteration ùëõ,
the program in Figure 3 maintains that i is in the program state and starts an unseparated path
with length ùëõ + 1. Because no bound can exist on the length of this path for an arbitrary number of
iterations, this program fails the unseparated path property.
ùëö-consumed. A variable is ùëö-consumed if it is no more than ùëö sampling operations away from a
variable that is consumed by an observe statement. The ùëö-consumed property states that there is a
uniform bound ùëö such every variable introduced by a sampling operation is ùëö-consumed for some
ùëö ‚â§ ùëö. We note that the traces in Figures 5 and 6 satisfy the ùëö-consumed property, because every
variable is at most 2-consumed. For all ùë°, yùë°
is 0-consumed because it is directly observed, and xùë°
is
1-consumed because yùë°
is sampled from xùë° and yùë°
is 0-consumed. The variable i is 2-consumed
because x1 is sampled from i, and x1 is 1-consumed.
The Outlier benchmark presented in Section 7 is an example of a program that fails the ùëö-
consumed property, and thus does not execute in bounded memory. This program sometimes
observes values close to the true latent state but otherwise observes values from an outlier distribution. When the program observes a value from the outlier distribution, it fails to observe any
dependencies of the latent state, and thus cannot guarantee that the latent state is ùëö-consumed.
Over time, if the program performs latent state updates that remain unobserved (due to the program
always observing from the outlier distribution), the lack of this guarantee results in there being no
uniform bound ùëö under which the latent state could be ùëö-consumed.
Analysis. Our goal is ultimately to analyze whether a given program executes in bounded memory.
As we show in Section 5, a program execution maintains bounded memory if and only if it satisfies
both the unseparated path and ùëö-consumed properties. This reduces the problem of analyzing
the bounded-memory behavior of a program to analyzing these dataflow properties. Our analysis
utilizes an abstract delayed sampling graph, formally defined in Section 6, with the key aspects of
these properties. For ùëö-consumed, the abstract graph maintains a set of variables that have been
introduced but not yet consumed, and for unseparated paths, it maintains an upper bound on their
length. For example, the abstract graphs for the trace in Figure 6 are given in Figure 7.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:8 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
ùúè2 = i f nil :: x1 f i :: y1 f x1 :: obs y1 :: x2 f x1 :: y2 f x2 :: obs y2
iteration 1 iteration 2
ùëö-consumed i x1 y1 x2 y2
unseparated paths (i, i), 0 (i, x1), 1 (i, y1), 2 (i, x1), 1 (i, x2), 2 (i, y2), 3 (i, x2), 2
Fig. 7. A depiction of the abstract graphs of the program in Figure 3, with the same trace as Figure 6. At
each operation, we depict the ùëö-consumed abstract graph, a set of nodes that have been introduced but
not consumed. Because this set is empty at the end of any iteration, the program satisfies the ùëö-consumed
semantic property. The unseparated paths abstract graph is a mapping, for each unseparated path in the
graph, from its endpoints to its length. We depict the longest path in the mapping. After each iteration, this
longest path continues to lengthen, so the program does not satisfy the unseparated paths semantic property.
3 LANGUAGE MODEL
In this section, we present a semantics for probabilistic programs with streams using the language
ùúáùêπ . We have adapted ùúáùêπ from Baudart et al. [2020]‚Äôs core calculus for probabilistic programs and
extended it with syntax for explicit streams.
3.1 Syntax
The syntax of the ùúáùêπ language is defined according to the following grammar:
program ::= ùëë
‚àó ùëö
ùëë ::= val ùëù = ùëí | val ùëì = fun ùëù -> ùëí | val ùëö = stream { init = ùëí ; step(ùëù,ùëù) = ùëí }
ùëí ::= ùë£ | op(ùë£) | ùëì (ùë£) | if ùë£ then ùëí else ùëí | let ùëù = ùëí in ùëí
| init(ùëö) | unfold(ùë•,ùë£) | sample(ùë£) | observe(ùë£,ùë£) | infer(ùëö)
ùë£ ::= ùëê | ùë• | (ùë£,ùë£)
ùëù ::= ùë• | (ùëù,ùëù)
A program is a set of value, function, and stream function definitions followed by the name of the
main stream function. A stream function ùëö is composed of an initial state (init) and a transition
function (step). Given a state and an input, the transition function returns an output and a new
state. An expression is either a value (constant, variable, or pair), the application of a primitive
operator (arithmetic operator, distribution, etc.), a function call, a conditional, or a local definition.
The expression init(ùëö) creates an instance of a stream function, and unfold(ùë•,ùë£) applies the
instance ùë• of a stream function on an input and returns the next element and the updated instance.
Finally, the set of expressions comprises the probabilistic operators sample, observe, and infer.
Nested inference and higher-order functions on streams are not allowed in the language. We require
that arguments for all syntactic operators are values to simplify the presentation of the semantics.
Since new variables can always be introduced to capture the value of any expression, this choice
does not reduce the expressiveness of the language.
3.2 Semantics
The execution of a program ùëù = ùëë
‚àóùëö comprises three steps. First, declarations ùëë
‚àó
are evaluated to
produce an environment ùõæ which contains the definition of the main stream function ùëö. Second,
an instance of the stream function ùëö is created.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:9
‚ü¶val ùë• = ùëí‚üßùõæ = ùõæ [ùë• ‚Üê ‚ü¶ùëí‚üßùõæ ]
‚ü¶val ùëì = fun ùëù -> ùëí‚üßùõæ = ùõæ [ùëì ‚Üê (ùúÜùë£. ‚ü¶ùëí‚üßùõæ+ [ùë£/ùëù ])]
‚ü¶val ùëö = stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }‚üßùõæ
= ùõæ [ùëö ‚Üê stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
]
‚ü¶init(ùëö)‚üßùõæ = let stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
‚Ä≤ = ‚ü¶ùëö‚üßùõæ in
let ùë†init = ‚ü¶ùëíinit‚üßùõæ
‚Ä≤ in (ùë†init, ùúÜ(ùë†, ùë£). ‚ü¶ùëí‚üßùõæ
‚Ä≤+ [ùë†/ùëùstate,ùë£/ùëùinput ])
if ùëí is deterministic
‚ü¶init(ùëö)‚üßùõæ = let stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
‚Ä≤ = ‚ü¶ùëö‚üßùõæ in
let ùë†init = ‚ü¶ùëíinit‚üßùõæ
‚Ä≤ in (ùë†init, ùúÜ(ùë†, ùë£). {[ùëí]}ùõæ
‚Ä≤+ [ùë†/ùëùstate,ùë£/ùëùinput ])
if ùëí is probabilistic
‚ü¶unfold(ùë•,ùë£)‚üßùõæ = let ùë£state, ùëì = ‚ü¶ùë•‚üßùõæ in
let ùë£output, ùë£‚Ä≤
state = ùëì (ùë£state, ‚ü¶ùë£‚üßùõæ ) in
(ùë£output, (ùë£
‚Ä≤
state, ùëì ))
‚ü¶infer(ùëö)‚üßùõæ = let stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
‚Ä≤ = ‚ü¶ùëö‚üßùõæ in
let ùë†init = ‚ü¶ùëíinit‚üßùõæ
‚Ä≤ in (ùõøùë†init , infer(ùúÜ(ùë†, ùë£). {[ùëí]}ùõæ
‚Ä≤+ [ùë†/ùëùstate,ùë£/ùëùinput ]))
where infer(ùëì ) = ùúÜ(ùúé, ùë£). let ùúá = ùúÜùëà . ‚à´
ùëÜ
ùúé(ùëëùë†)ùëì (ùë†, ùë£) (ùëà ) in
let ùúà = ùúÜùëà . ùúá(ùëà )/ùúá(‚ä§) in
(ùúã1‚àó (ùúà), ùúã2‚àó (ùúà))
Fig. 8. Deterministic semantics of ùúáùêπ (complete definition in Figure 16).
Third, the instance is iteratively applied on an input stream (ùëñùëõ)ùëõ‚ààN to produce an output
stream (ùëúùëõ)ùëõ‚ààN, defined in the following way:
‚ü¶ùëù‚üß(ùëñ)ùëõ = ùëúùëõ where ùëù = ùëë
‚àóùëö ùõæ = ‚ü¶ùëë
‚àó‚üß‚àÖ
ùë†0 = ‚ü¶init(ùëö)‚üßùõæ ùëúùëõ, ùë†ùëõ+1 = ‚ü¶unfold(ùë†ùëõ,ùëñùëõ)‚üßùõæ ‚àÄùëõ ‚â• 0
Figure 8 defines the semantics of declarations and deterministic expressions ‚ü¶¬∑‚üß. The declarations
build the evaluation environment ùõæ which maps names to values, functions, and stream functions.
The semantics of deterministic expressions corresponds to a first order functional language with
new constructs to handle streams and the infer(¬∑) operator (the complete definition is given in
Figure 16 of Appendix A). The expression init(ùëö) creates an instance of the stream function ùëö: a
pair corresponding to the current state, and the transition function. The current state is initialized
with the value of the init field. The expression unfold(ùë•,ùë£) executes the transition function of
the instance ùë• on its current state and the input ùë£. This expression produces a pair composed of the
transformed value and the updated instance.
The ideal semantics of ùúáùêπ probabilistic expressions {[¬∑]} is a measure-based semantics similar
to the one presented by Staton [2017] (the complete definition is given in Appendix A). Given an
environment ùõæ, an expression is interpreted as a measure {[ùëí]}ùõæ : Œ£ùê∑ ‚Üí [0, ‚àû), that is, a function
which associates a positive number to each measurable setùëà ‚àà Œ£ùê∑ , where Œ£ùê∑ denotes the Œ£-algebra
of the domain of the expression ùê∑ (i.e., the set of measurable sets of possible values). sample(ùë£)
returns the distribution ‚ü¶ùë£‚üßùõæ . observe(ùë£1,ùë£2) weights execution paths using the likelihood of
the observation ‚ü¶ùë£2‚üßùõæ w.r.t. the distribution ‚ü¶ùë£1‚üßùõæ (for a distribution ùúá we denote its probability
density function as ùúápdf). Local definitions are interpreted as integration, and we use the Dirac
delta measure to interpret deterministic expressions.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:10 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
{[ùë£]}ùõæ = ùúÜùëî,ùë§ . (‚ü¶ùë£‚üßùõæ , ùëî,ùë§)
{[op(ùë£)]}ùõæ = ùúÜùëî,ùë§ . (app(op, ‚ü¶ùë£‚üßùõæ ), ùëî,ùë§)
{[ùëì (ùë£)]}ùõæ = ùúÜùëî,ùë§ . ùõæ (ùëì ) (‚ü¶ùë£‚üßùõæ ) (ùëî,ùë§)
{[let ùëù = ùëí1 in ùëí2]}ùõæ = ùúÜùëî,ùë§ . let ùë£1, ùëî1,ùë§1 = {[ùëí1]}ùõæ (ùëî,ùë§) in {[ùëí2]}ùõæ+ [ùë£1/ùëù ] (ùëî1,ùë§1)
{[if ùë£ then ùëí1 else ùëí2]}ùõæ = ùúÜùëî,ùë§ . let ùëè, ùëîùëè = value(‚ü¶ùë£‚üßùõæ , ùëî) in
if ùëè then {[ùëí1]}ùõæ (ùëîùëè,ùë§) else {[ùëí2]}ùõæ (ùëîùëè,ùë§)
{[unfold(ùë•,ùë£)]}ùõæ = ùúÜùëî,ùë§ . let ùë£state, ùëì = ‚ü¶ùë•‚üßùõæ in
let (ùë£output, ùë£‚Ä≤
state), ùëî‚Ä≤ùë§
‚Ä≤ = ùëì (ùë£state, ‚ü¶ùë£‚üßùõæ ) (ùëî,ùë§) in
( (ùë£output, (ùë£
‚Ä≤
state, ùëì )), ùëî‚Ä≤ùë§
‚Ä≤
)
{[sample(ùë£)]}ùõæ = ùúÜùëî,ùë§ . let ùëã, ùëî‚Ä≤ = assume(‚ü¶ùë£‚üßùõæ , ùëî) in (ùëã, ùëî‚Ä≤
,ùë§)
{[observe(ùë£1,ùë£2)]}ùõæ = ùúÜùëî,ùë§ . let ùëã, ùëîùë• = assume(‚ü¶ùë£1‚üß, ùëî) in
let ùë£, ùëîùë£ = value(‚ü¶ùë£2‚üß, ùëîùë• ) in
let ùëî
‚Ä≤ = observe(ùëã, ùë£, ùëîùë£ ) in ((), ùëî‚Ä≤
,ùë§ ‚àó ùúápdf(ùë£))
Fig. 9. Delayed sampling semantics. Probabilistic expressions are functions from a graph and a weight to a
triplet (value, graph, weight).
The infer(ùëö) operator creates an instance of a probabilistic stream: the initial state is a Dirac
delta distribution on the initial state of ùëö, and the transition function is infer(ùëì ) where ùëì is the
transition function of ùëö. The body of ùëì (the expression ùëí) is interpreted with the probabilistic
semantics which defines a measure over pairs of output values and states. The function infer(ùëì )
takes as arguments a distribution of states ùúé and an input ùë£ and returns a distribution of outputs
and a distribution of new states. These two distributions are obtained by integrating the transition
function ùëì along the distribution ùúé of possible states (domain ùëÜ) to build a measure ùúá which is then
normalized to build a distribution ùúà of pairs (outputs, states). The distribution ùúà is then split into a
pair of marginal distributions using the pushforward of ùúà across the projections ùúã1 and ùúã2.
4 DELAYED SAMPLING
In this section, we present the details of delayed sampling that underpin this work. This is a new
formalization of results that were presented by Murray et al. [2018] and Baudart et al. [2020].
Delayed sampling is a semi-symbolic algorithm combining exact inference and ‚Äì when exact
computation fails ‚Äì approximate inference with particle filtering [Del Moral et al. 2006]. A particle
filter launches multiple executions of the model. Each execution ‚Äî or particle ‚Äî is associated to a
weight. In the operational semantics, sample(ùëë) statements draw samples from the corresponding
distributions, and observe(ùë•,ùëë) statements update the weight to reflect the quality of the samples.
At the end of the executions the results of all the particles are normalized according to their weights
to form a categorical distribution that approximates the posterior distribution of the model.
In delayed sampling, each particle contains a graph of random variables and their dependencies
that can be used to compute closed-form distributions. Observations can be incorporated by
analytically conditioning the network. If symbolic conditioning fails, inference falls back to a
particle filter, drawing concrete samples for required random variables.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:11
4.1 Operational Semantics
The definition of infer in Figure 8 makes use of an intractable integral. The delayed sampling
semantics replaces this integral by a discrete sum over the set of particles of the particle filter.
Compared to traditional particle filtering, delayed sampling performs exact computations when
possible. Thus, we extend valuesùë£ with symbolic terms. Symbolic terms include random variables (ùëã)
‚Äî the nodes of the delayed sampling graph ‚Äî and applications of operators.
ùë£ ::= ... | ùëã | app(op, ùë£)
The semantics in Figure 9 rely on the following high-level operations to update the graph ùëî.
ùë£
‚Ä≤
, ùëî‚Ä≤ = value(ùë£, ùëî) samples all the random variables in ùë£ to produce a concrete value.
ùëî
‚Ä≤ = observe(ùëã, ùë£, ùëî) conditions the graph on the fact that the random variable ùëã takes the value ùë£.
ùëã, ùëî‚Ä≤ = assume(ùëë, ùëî) adds and returns a new random variable ùëã with distribution ùëë.
Probabilistic semantics. The semantics of a probabilistic expressions are defined in Figure 9. The
semantics of an expression {[ùëí]}ùõæ,ùëî,ùë§ takes two additional arguments: ùëî, the delayed sampling graph,
and ùë§, the weight for the particle filter, and returns a symbolic value, an updated graph, and
an updated weight. Operator application op(ùë£) introduces a symbolic expression app(op, ùë£). if
uses the value operation to sample a concrete value for the condition. sample(ùë£) introduces a
new random variable in the graph with distribution ùë£. observe(ùë£1,ùë£2) introduces a fresh random
variable ùëã with distribution ùë£1, and conditions the graph on the fact that ùëã takes the value ùë£2.
Inference. Given a transition function ùëì , a distribution over states ùúé from the previous iteration,
and inputs ùë£ùëñ
, the infer operator computes a distribution of outputs and new distribution over
states for the next iteration. First, the inference draws ùëÅ states from ùúé. Each of theses states ùë†ùëõ is
associated with a delayed sampling graph ùëîùëõ. Second, the transition function ùëì returns a symbolic
output value ùë£ùëõ, a new state ùë†
‚Ä≤
ùëõ
, the updated graph ùëî
‚Ä≤
ùëõ
, and the importance weight ùë§ùëõ. Third, the
distribution(ùëúùëõ, ùëî‚Ä≤
ùëõ
) function returns a distribution of values without altering the graph, and the
new distribution over states is a Dirac delta distribution on the pair (ùë†
‚Ä≤
ùëõ
, ùëî‚Ä≤
ùëõ
). Finally, results are
accumulated in a mixture distribution using the weights ùë§ùëõ and this distribution is split into a
distribution of values and a distribution of next states.1
‚ü¶infer(ùëö)‚üßùõæ = let stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
‚Ä≤ = ‚ü¶ùëö‚üßùõæ in
let ùë†init = ‚ü¶ùëíinit‚üßùõæ
‚Ä≤ in (ùõø(ùë†init,‚àÖ), infer(ùúÜ(ùëùstate, ùëùinput). {[ùëí]}ùõæ
‚Ä≤))
where infer(ùëì ) = ùúÜ(ùúé, ùë£ùëñ). let ùúá = ùúÜùëà . √ç
ùëÅ
ùëõ=1
let ùë†ùëõ, ùëîùëõ = draw(ùúé) in
let (ùëúùëõ, ùë†‚Ä≤
ùëõ
), ùëî‚Ä≤
ùëõ
,ùë§ùëõ = ùëì (ùë†ùëõ,ùë£ùëñ)(ùëîùëõ, 1) in
let ùëëùëõ = distribution(ùëúùëõ, ùëî‚Ä≤
ùëõ
) in
ùë§ùëõ ‚àó ùëëùëõ (ùúã1 (ùëà )) ‚àó ùõøùë†
‚Ä≤
ùëõ,ùëî‚Ä≤
ùëõ
(ùúã2 (ùëà ))
in (ùúã1‚àó (ùúá), ùúã2‚àó (ùúá))
4.2 Graph Manipulation
We now describe the graph manipulation functions that are required to define the high-level
operations value, assume, and observe used in the semantics of Figure 9. Lund√©n [2017] and Murray
et al. [2018] provide detailed explanations of these operations.
Notation. In this section and those that follow, frv(ùë£) denotes the free random variables of a
program value ùë£, i.e., the set of variables used in the symbolic expression ùë£.
1we write ùë§ùëñ = ùë§ùëñ /
√çùëÅ
ùëñ=1 ùë§ùëñ for the normalized weights.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:12 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Graph Data Structure. A delayed sampling graph ùëî is defined by a tuple (ùëâ , ùê∏, ùëû) where ùëâ is a set
of vertices ‚Äì the random variables, ùê∏ is a set of directed edges ‚Äì the dependencies between random
variables, and ùëû is a relation mapping each node to a state: Initialized, Marginalized, or Realized.
A node Initialized(ùëùùëã |ùëå ) represents a random variable ùëã with a conditional distribution ùëùùëã |ùëå
where ùëå is the unique parent of ùëã. A node Marginalized(ùëùùëã ) represents a random variable ùëã with a
marginal distribution ùëùùëã . A Marginalized node has at most one parent. If there is a parent node, the
distribution ùëùùëã incorporates its distribution. A node Realized(ùë£) represents a random variable ùëã
associated to a concrete value ùë£. By construction, a delayed sampling graph is a forest ‚Äì a set of
trees (each node has at most one parent).
value. The operation value(ùë£, ùëî) converts the symbolic expression ùë£ into a concrete value by
sampling all the random variables in ùë£. All these random variables become Realized nodes in the
graph, and the distributions depending on these variables are updated.
value(ùë£, ùëî) = (ùë£, ùëî) if ùë£ is a concrete value
value(app(op, ùë£), ùëî) = let ùë£
‚Ä≤
, ùëî‚Ä≤ = value(ùë£, ùëî) in (op(ùë£
‚Ä≤
), ùëî‚Ä≤
)
value(ùëã, ùëî) = let ùëâ , ùê∏, ùëû = ùëî in
if ùëû(ùëã) = Realized(ùë£) then (ùë£, ùëî)
else let ùëâ
‚Ä≤
, ùê∏‚Ä≤
, ùëû‚Ä≤
[ùëã ‚Üê Marginalized(ùúá)] = graft(ùëã, ùëî) in
let ùë£ = draw(ùúá) in
(ùë£, (ùëâ
‚Ä≤
, ùê∏‚Ä≤
, ùëû‚Ä≤
[ùëã ‚Üê Realized(ùë£)]))
If ùë£ is already a concrete value, there is nothing to do. If ùë£ is the application of an operator, value
recursively samples a concrete value for the argument and applies the operator to this value. Ifùë£ is a
random variable ùëã that is already realized, value returns the corresponding value. Otherwise, value
(1) calls the graft function defined in Appendix C to marginalize ùëã and all its ancestors, (2) draws a
sample from the marginalized distribution, and (3) returns this value and turns ùëã into a Realized
node. Note that graft might have to realize some nodes since it marginalizes all its ancestors and a
marginal node has a single marginalized child. During marginalization, graft also removes edges
between Marginalized nodes and their Realized child if any.
assume. The operation assume(ùë£, ùëî) adds a new random variable ùëã with distribution ùë£ in graph ùëî.
assume(ùë£, ùëî) = let (ùëâ , ùê∏, ùëû) = ùëî in
let ùëã = fresh(ùëâ ) in
if frv(ùë£) = ‚àÖ then (ùëã, (ùëâ ‚à™ {ùëã}, ùê∏, ùëû[ùëã ‚Üê Marginalized(ùë£)]))
else if frv(ùë£) = {ùëå } ‚àß conj(ùë£, ùëå, ùëî) then
(ùëã, (ùëâ ‚à™ {ùëã}, ùê∏ ‚à™ {(ùëã, ùëå)}, ùëû[ùëã ‚Üê Initialized(ùë£)]))
else
let ùë£
‚Ä≤
, (ùëâ ,‚Ä≤ ùê∏
‚Ä≤
, ùëû‚Ä≤
) = value(ùë£, (ùëâ ‚à™ {ùëã}, ùê∏, ùëû)) in
(ùëã, (ùëâ
‚Ä≤
, ùê∏‚Ä≤
, ùëû‚Ä≤
[ùëã ‚Üê Marginalized(ùë£
‚Ä≤
)]))
The distribution ùë£ is a symbolic expression which can be a marginal distribution that does not
depends on other random variables ‚Äî e.g., app(bernoulli, 0.5)‚Äî or a conditional distribution ‚Äî e.g.,
app(bernoulli, ùëå) where ùëå is a random variable. If ùë£ is a marginal distribution, assume just adds a
new marginalized node in the graph. If ùë£ is a conditional distribution, assume tries to keep track of
the dependency between ùëã and a random variable used in ùë£ (the delayed sampling graph is a forest
where each node has at most one parent).
The value ùë£ thus represents a distribution ùëùùëã |ùëå where ùëã depends on a unique random variable ùëå.
If the distribution ùëùùëã |ùëå and ùëùùëå are conjugate (conj(ùë£, ùëå, ùëî)) ‚Äî e.g., app(bernoulli, ùëå) with ùëå ‚àº
beta(ùõº, ùõΩ) ‚Äî marginalization and conditioning are tractable operations, and assume adds an edge
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:13
between ùëå and a new initialized node ùëã to the graph. Otherwise, symbolic computation is not
possible; assume calls value to sample a concrete value, thus breaking the dependency, and adds a
new independent Marginalized node to the graph.
observe. The operation observe(ùëã, ùë£, ùëî) assigns the concrete value ùë£ to ùëã and updates the distributions depending on ùëã accordingly.
observe(ùëã, ùë£, ùëî) = let (ùëâ , ùê∏, ùëû) = graft(ùëã, ùëî) in (ùëâ , ùê∏, ùëû[ùëã ‚Üê Realized(ùë£)])
Similarly to value, the observe operation uses the function graft to marginalize the variable ùëã and
then turns ùëã to a Realized node associated with the value ùë£.
4.3 Memory Usage
Baudart et al. [2020] proposed an implementation of delayed sampling where an Initialized node
only has a pointer to its parent, a Marginalized node only has a pointer to its unique Marginalized
or Realized child, if any, and a Realized node has no pointers to its parent or any of its children.
Garbage Collection. A node in the delayed sampling graph can be safely removed if none of the
program variables depend on its value. We assume the existence of a garbage collection routine
that deallocates the nodes of the graph that are not reachable as soon as possible.
Definition 4.1 (Reachability). Given a set of root variables ùëü and a delayed sampling graph
ùëî = (ùëâ , ùê∏, ùëû), the set of reachable variables ‚Äì written reachable(ùëî, ùëü) ‚Äì is defined as follows:
ùëÖ = {(ùëã, ùëå) |
(ùëã, ùëå) ‚àà ùê∏ ‚àß ùëû(ùëã) = Initialized
‚à®

(ùëå, ùëã) ‚àà ùê∏
‚àß ùëû(ùëã) = Marginalized ‚àß (ùëû(ùëå) = Marginalized ‚à® ùëû(ùëå) = Realized)

reachable(ùëî, ùëü) = {ùëå | (ùëÖ
‚àó
(ùëã, ùëå)) ‚àß ùëã ‚àà ùëü ‚àß ùëå ‚àà ùëâ }
where ùëÖ
‚àó denotes the reflexive transitive closure of the relation ùëÖ.
If we consider the graph in Figure 2b, reachable(ùëî, {x}) = {x}. In the example of Figure 4b, we
have reachable(ùëî, {i, x}) = {i, pre_x, x}, where pre_x is the gray node in between the nodes for i
and x. Reachability is the core property used in Definition 5.1 to define what it means for a program
to run in bounded memory.
Graph Expansion. The only operation that increases the size of the graph is assume which
introduces new nodes. The operations value and observe can only marginalize and realize nodes.
If ùëî
‚Ä≤
is the graph resulting from the application of value or observe on a graph ùëî, ùëî and ùëî
‚Ä≤ have
the same structure but Initialized nodes can be Marginalized or Realized, and Marginalized nodes
can be Realized. The reachability relation of the graph implies that value and observe reduce the
number of dependencies in the delayed sampling graph, that is, reachable(ùëî
‚Ä≤
, ùëü) ‚äÜ reachable(ùëî, ùëü).
Initialized and Marginalized Chains. Two patterns can yield unbounded memory consumption.
First, it is possible to keep adding nodes without realizing them (via observation or sampling),
thus forming initialized chains. An initialized chain is a sequence of initialized nodes, each of
which holds a pointer to its parent and thereby expands the number of random variables that are
reachable. Second, it is possible that nodes are only indirectly used to realize one of their children.
These marginalized nodes can form marginalized chains. A marginalized chain is a sequence of
marginalized nodes, each of which holds a pointer to its child and thus expands the number of
random variables that are reachable. The last node of a marginalized chain may be realized.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 202
115:14 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
5 SEMANTIC PROPERTIES
In this section, we define conditions under which delayed sampling executes in bounded memory.
We define these conditions as properties of executions. An execution is a sequence of pairs of a state
and a delayed sampling graph (ùë†ùëõ, ùëîùëõ)ùëõ‚ààN, where each state is a semi-symbolic value as defined in
Section 4.1. An execution defines the sequence of states and graphs a model ‚Äî i.e., an argument of
an infer ‚Äî goes through.
The inference step function infer(ùëì ) in ‚ü¶infer(ùëö)‚üß may operate over multiple executions
of ùëì (see Section 4.1). However, infer(ùëì ) executes in bounded memory if every execution of ùëì is
bounded-memory. This is because infer(ùëì ) always updates its state by mapping ùëì over states and
graphs from the distribution at the previous iteration. Thus, any state and graph in the distribution
at the next iteration must have come from some execution of ùëì , and if all executions of ùëì are
bounded-memory, all states and graphs in the distribution must have bounded memory. We have
formalized this in more details in Appendix E.1.
Based on this notion of execution, we introduce two notions of bounded-memory executions
of delayed sampling, and semantic properties which are necessary and sufficient for boundedmemory execution. In Section 5.1 we present a low-level definition of bounded memory that directly
corresponds to how the delayed sampling runtime executes. In Section 5.2 we present an alternative
high-level definition in terms of dataflow properties of the high-level delayed sampling operators:
the ùëö-consumed and unseparated paths properties. In Section 5.3 we show that the high-level and
low-level formulations are equivalent. In particular, Section 5.3 shows a correspondence between the
ùëö-consumed property and a bound on the length of initialized chains, as well as a correspondence
between the unseparated paths property and a bound on the length of marginalized chains.
5.1 Low-Level Bounded Memory
A program executes in bounded memory if the delayed sampling graph maintains a bounded
number of reachable variables over time. We formalize this as follows:
Definition 5.1 (Low-level Bounded-Memory). An execution (ùë†ùëõ, ùëîùëõ)ùëõ‚ààN of a model is low-level
bounded-memory if
‚àÉùëò. ‚àÄùëõ ‚â• 0 |reachable(ùëîùëõ, ùë†ùëõ)| ‚â§ ùëò ‚àó |frv(ùë†ùëõ)|
This definition states that at each iteration, the size of the set of reachable nodes in the delayed
sampling graph may be at most a constant multiple of the number of free random variables in
the state. We do not consider the runtime to violate bounded memory in the trivial case that the
program state is intrinsically unbounded, i.e., when |frv(ùë†ùëõ)|ùëõ‚ààN is unbounded. Such a program
would not execute in bounded memory under any inference algorithm; even a particle filter would
require unbounded memory to store the program state.
5.2 High-Level Definitions
In this section, we present an alternative high-level definition of bounded memory that is easier
to reason about. The high-level definition is in terms of dataflow properties of delayed sampling
operations. We have formalized these dataflow properties by augmenting the delayed sampling
operations with tracing. A trace is defined as follows:
ùúè ::= ùúè :: ùúè1 | nil
ùúè1 ::= ùëã f ùëã | ùëã f nil | eval(X) | obs(ùëã)
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:15
assume(ùë£, (ùëî, ùúè)) = let ùëã
‚Ä≤
, ùëî‚Ä≤ = assume(ùë£, ùëî) in
Ô£±Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥
Ô£≥
ùëã
‚Ä≤
, (ùëî
‚Ä≤
, ùúè :: ùëã
‚Ä≤ f nil) frv(ùë£) = ‚àÖ
ùëã
‚Ä≤
, (ùëî
‚Ä≤
, ùúè :: ùëã
‚Ä≤ f ùëã) {ùëã} = frv(ùë£) ‚àß conj(ùë£, ùëã, ùëî)
ùëã
‚Ä≤
, (ùëî
‚Ä≤
, ùúè :: eval(frv(ùë£)) :: ùëã
‚Ä≤ f nil) otherwise
value(ùë£, (ùëî, ùúè)) = let (ùë£
‚Ä≤
, ùëî‚Ä≤
) = value(ùë£, ùëî) in ùë£
‚Ä≤
, (ùëî
‚Ä≤
, ùúè :: eval(frv(ùë£)))
observe(ùëã, ùë£, (ùëî, ùúè)) = observe(ùëã, ùë£, ùëî), (ùúè :: obs(ùëã))
Fig. 10. Tracing semantics of delayed sampling operators.
A trace is a list of primitive operations, where each primitive is one of:
‚Ä¢ Assumption, written ùëã f ùëã
‚Ä≤ when ùëã is assumed from another random variable ùëã
‚Ä≤ or
ùëã f nil when it is assumed without a parent.
‚Ä¢ Evaluation using the eval keyword, which refers to evaluating a set of random variables X.
‚Ä¢ Observation using the obs keyword, which refers to observing a random variable ùëã.
We define an augmented semantics that operates on a pair of a delayed sampling graph and a trace.
Figure 10 defines augmented versions of the assume, value, and observe operations, and the full
semantics (written ‚ü¶¬∑‚üß and {[¬∑]}) is defined by replacing these operators in Figure 9 with their traced
counterparts from Figure 10.
The ùëö-consumed Property. The ùëö-consumed property is used to enforce that every variable
introduced with assume is eventually consumed either by directly being passed to a value or observe
or transitively by being passed to a assume that introduces a variable that is also ùëö-consumed.
Definition 5.2 (ùëö-consumed). A variable ùëã is ùëö-consumed in a trace ùúè under the following
circumstances:
‚Ä¢ ùëã is 0-consumed if it is observed or evaluated (i.e., ùúè has eval(ùëã) where ùëã ‚àà X or obs(ùëã)).
‚Ä¢ ùëã is 0-consumed if it is never used (i.e., there is no ùëã
‚Ä≤ f ùëã, eval(ùëã), or obs(ùëã) in ùúè).
‚Ä¢ ùëã is ùëö-consumed if it is passed to the assume statement that introduces another variable ùëã
‚Ä≤
(i.e., ùëã
‚Ä≤ f ùëã is in ùúè), and ùëã
‚Ä≤
is (ùëö ‚àí 1)-consumed.
The Unseparated Paths Property. The unseparated paths property states the existence of a sequence
of variables, each assumed from the previous, with no variable in the sequence observed or evaluated.
Definition 5.3 (Unseparated Paths). An unseparated path inùúè is a sequence of variablesùëã0, ùëã1, . . . , ùëãùëõ
such that each ùëãùëñ+1 was assumed from ùëãùëñ
(i.e., ùëãùëñ+1 f ùëãùëñ
is in ùúè) and no ùëãùëñ
is directly observed or
evaluated (i.e., ùúè does not contain any eval or obs operations that reference ùëãùëñ
).
High-level Bounded Memory. We now present the high-level bounded memory property. This
property states that all variables must eventually be ùëö-consumed, and there must be a uniform
bound across iterations on the length of an unseparated path starting from a program state variable.
Definition 5.4 (High-level Bounded-Memory). A program execution (ùë†ùëõ, (ùëîùëõ, ùúèùëõ))ùëõ‚ààN is high-level
bounded-memory if and only if
‚Ä¢ There exists an ùëö such that for every iteration ùëõ and every variable introduced before ùëõ (i.e.,
ùëã such that ùëã f ùëã
‚Ä≤ or ùëã f nil is in ùúèùëõ), there exists a ùëõ
‚Ä≤ ‚â• ùëõ such that for all ùëõ
‚Ä≤‚Ä≤ ‚â• ùëõ
‚Ä≤
, ùëã
is ùëö-consumed in ùúèùëõ
‚Ä≤‚Ä≤.
‚Ä¢ There exists a ùëê such that for all ùëõ, no random variable referenced in ùë†ùëõ starts an unseparated
path in ùúèùëõ of length more than ùëê.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:16 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
5.3 Equivalence of Low-Level and High-Level Definitions
In this section, we show the equivalence of the low-level and high-level definitions. We do so
by showing that both properties are equivalent to the delayed sampling graph having a uniform
bound (i.e., a bound that holds across all iterations) on the length of initialized and marginalized
chains as defined in Section 4.3.
5.3.1 Low-Level Bounded Memory vs. Infinite Chains.
Lemma 5.5. If the delayed sampling graph is constructed using assume, observe, and value operations, then each random variable starts either an initialized chain, a marginalized chain, or an
initialized chain followed by a marginalized chain.
Proof. The assume, observe, and value operations can only make the following modifications to
a delayed sampling graph ùëî. (1) Add a independent Marginalized node which creates a marginalized
chain of length zero. (2) Attach a new Initialized node ùëã to a node ùëå with a conjugate distribution. It
means that ùëå is either Initialized or Marginalized and thus it creates either a longer initialized chain
or an initialized chain followed by a marginalized chain. (3) Perform a graft which ensures that
every ancestor of a node is marginalized and has a single marginalized child. Every non-ancestor
variable is either as it was before or becomes realized, so this operation preserves the structure of
the previous graph and cannot increase the length of the chains. (4) convert a Marginalized node
into a Realized node which can only break a chain. ‚ñ°
Theorem 5.6. A program is low-level bounded-memory iff there is a uniform boundùëö on the length
of an initialized chain and a uniform bound ùëê on the length of a marginalized chain.
Proof. Assuming a uniform bound, when the number of variables is bounded by ùëÅ, according
to Lemma 5.5, the number of reachable nodes in the graph is bounded by ùëÅ √ó (ùëê + ùëö).
Conversely, if no uniform bound exists (i.e., for every potential bounds ùëê and ùëö, there exists
a iteration ùëõ such that chains may exceed the bound at ùëõ), the execution cannot be low-level
bounded-memory, because even if the number of root variables is bounded by ùëÅ, the reachable
variables may exceed ùëÅ √ó (ùëê + ùëö). ‚ñ°
5.3.2 High-Level Bounded Memory vs. Infinite Chains.
Theorem 5.7 (High-level Soundness). In a program execution that is high-level bounded-memory,
no infinite chains can exist in any of the delayed sampling graphs.
Proof. All initialized chains must be shorter than ùëö, where ùëö is from the ùëö-consumed property
of high-level bounded-memory. This is because when a variable‚Äôs descendant is subject to observe
or value, the variable becomes marginalized. Such a descendant can be at most ùëö variables away
because of the definition of ùëö-consumed.
All marginalized chains must be shorter than ùëê +ùëö, where ùëê is from the unseparated path property
of high-level bounded-memory and ùëö is from the ùëö-consumed property. By Lemma 5.5, every
marginalized chain must start at either a root or an initialized chain. If it starts at a root, the
unseparated path property ensures that the path between the root and the end of the chain can
contain at most ùëê variables. This is because any observed or valued variables become realized and
become the end of the chain. If it starts at an initialized chain, by the above reasoning that chain
has length at most ùëö, and there was a previous iteration at which the marginalized chain started at
a root and had length at most ùëê, giving an overall length of at most ùëê + ùëö. ‚ñ°
Lemma 5.8. If there exists a variable that is not ùëö-consumed, then the program produces a graph at
some iteration with an initialized chain of length ùëö.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:17
Proof. If a variable is not ùëö-consumed, then by the definition of ùëö-consumed must start an
assume chain of length ùëö. All of the nodes in this chain must be initialized, and therefore form an
initialized chain of length ùëö. ‚ñ°
Lemma 5.9. If every variable is ùëö-consumed, and there exists a variable that starts an unseparated
path of length ùëê where ùëê > ùëö, then there exists an iteration with a marginalized chain that has length
at least ùëê ‚àí ùëö.
Proof. Note that the firstùëê ‚àíùëö variables in the unseparated path must be either marginalized or
realized. Otherwise, there would be more than ùëö initialized variables in the tail of the unseparated
path that are initialized, which would violate soundness of ùëö-consumed. Let ùëã be the variable that
starts the unseparated path and ùëã
‚Ä≤ be the last marginalized or realized variable in the unseparated
path, and consider the iteration ùëõ
‚Ä≤ when ùëã
‚Ä≤ was first marginalized. It must be true that (1) ùëã is in
the program state at iteration ùëõ
‚Ä≤ because it is in the state at the current iteration ùëõ > ùëõ
‚Ä≤
, and (2) a
marginalized chain runs from ùëã to ùëã
‚Ä≤
. Thus, at ùëõ
‚Ä≤
, the marginalized chain had length ùëê ‚àí ùëö. ‚ñ°
Theorem 5.10 (High-level Completeness). If a program execution is not high-level boundedmemory, the delayed sampling graph has either unbounded initialized chains or marginalized chains.
Proof. If the execution is not high-level bounded-memory, it either fails the ùëö-consumed
property or the unseparated path property. If it fails the ùëö-consumed property, apply Lemma 5.8.
Otherwise, apply Lemma 5.9. ‚ñ°
Theorem 5.11. A program execution is high-level bounded-memory if and only if it is low-level
bounded-memory.
Proof. Apply Theorems 5.6, 5.7, and 5.10. ‚ñ°
6 ANALYSIS
In this section, we develop an analysis to check that a ùúáùêπ program executes in bounded memory. We
approach this problem by developing two independent analyses within a shared analysis framework.
One analysis checks the ùëö-consumed property of a program and the other checks the unseparated
paths property, which together ensure that the program executes in bounded memory (Section 5).
Our shared analysis framework abstracts the execution of a program as the execution of abstract
operations on an abstract graph. An abstract graph abstracts the dynamic state of a program‚Äôs
delayed sampling graph. We implement the analysis framework by means of a type system, such
that well-typed programs satisfy the ùëö-consumed and unseparated paths properties, given each
analysis‚Äôs respective instantiation of the abstract graph. The typing judgment
Œì, G ‚ä¢ùõº ùëí : ùë°, G
‚Ä≤
asserts that in a context Œì, and for an abstract graph G, that an expression ùëí accesses the random
variables denoted by the type ùë° and yields a new abstract graph G
‚Ä≤
. The parameter ùõº is either mc
to denote the ùëö-consumed analysis or up to denote the unseparated paths. We write Œì ‚ä¢ùõº ùëí : ùë° as
shorthand for Œì, G ‚ä¢ùõº ùëí : ùë°, G when ùëí has no effect on the graph.
6.1 Types and Contexts
A type ùë° captures the random variables the expression could refer to as well as its shape, as primitive
data, a product, a function, or a stream instance.
ùë° F ùëü | () | ùë°1 √ó ùë°2 | ùë°1 ‚Üí ùë°2 | stream(ùë°, ùë†) | bounded
ùë† F stepfn(ùëùstate, ùëùin, Œìùëí, ùëí)
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:18 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
The type of a primitive expression is a reference set, denoted ùëü, which specifies the random variables
to which the expression refers. We distinguish two types of stream instances, before and after
bounded-memory checking. The first is stream(ùë°, ùë†), where ùë° is the type of the current state and ùë† is
a step function representation to be described later. The second is bounded, representing instances
that have passed bounded memory analysis and hide their inner structure.
Reference Sets. A reference set of a ùúáùêπ expression, denoted ùëü, specifies the random variables that
are affected when the expression is observed or evaluated. In the presence of branches, we define ùëü
to be a pair of sets (lb, ub), where the lower bound lb contains all random variables which must
be affected and the upper bound ub all random variables which may be affected. For example,
a constant value in ùúáùêπ such as 1.5 has the reference set (‚àÖ, ‚àÖ) because it references no random
variables. If the program variables x and y correspond to random variables ùëã and ùëå respectively,
then the expression gaussian(x,y), specifying a distribution with two parameters, has reference
set ({ùëã, ùëå }, {ùëã, ùëå }), meaning that observing it will observe the random variables ùëã and ùëå.
Contexts. The context Œì, ùë• : ùë° maps variable ùë• to type ùë°. As ùúáùêπ syntactic patterns ùëù may be
variables or pairs, we use the shorthand Œì, ùëù : ùë° to define types for variables in ùëù by structural
correspondence with ùë°, as defined by the first rule below. We also define a judgment ‚ä¢ùëù ùëù : ùë° that
synthesizes a deterministic type ùë° from a pattern ùëù.
Œì, ùëù1 : ùë°1, ùëù2 : ùë°2 ‚ä¢ùõº ùëí : ùë°
Œì, (ùëù1, ùëù2) : ùë°1 √ó ùë°2 ‚ä¢ùõº ùëí : ùë° ‚ä¢ùëù ùë• : (‚àÖ, ‚àÖ)
‚ä¢ùëù ùëù1 : ùë°1 ‚ä¢ùëù ùëù2 : ùë°2
‚ä¢ùëù (ùëù1,ùëù2) : ùë°1 √ó ùë°2
6.2 Abstract Graphs
An abstract graph G is an abstraction of the delayed sampling graph that tracks which random
variables have been consumed and active paths between random variables, properties relevant to
the semantic properties. For each analysis ùõº there exists an abstract graph type, G, and a set of
operations that form its interface (Figure 11).
Specifically, in the ùëö-consumed analysis we define G to be a pair of sets in and con which
respectively represent an over-approximation of variables introduced into the graph and an underapproximation of the variables consumed by observation or sampling (Figure 12). In the unseparated
paths analysis, we define G to be a set sep of separators containing consumed random variables
and a partial path function ùëù mapping a pair of random variables to an upper bound on the length
of an unseparated path between them (Figure 13).
Operations on the abstract graph manipulate random variables, graphs, and reference sets. The
function assume returns a new graph with a random variable ùëã from a distribution with reference
set ùëü added to G, observe returns a graph where ùëã is observed with a value with reference set ùëü,
and value returns a graph where an expression with reference set ùëü is evaluated. The join operator
‚äîùõº represents a conservative choice between two graphs.
ùëö-consumed Graph Operations. In Figure 12, assumemc(ùëã, ùëü, G) marks the random variable ùëã as
introduced. In all cases, the lower bound of random variables in the input is marked consumed. To
join two states, we union the introduced variables and intersect the consumed variables.
Unseparated Paths Graph Operations. In Figure 13, observeup and valueup mark input variables as
separators. In assumeup, we set the length of the path from the new variable ùëã to itself to zero. For
a parent ùëãùëù that is not a separator, we set the length of the path from any variable ùëãùëñ to ùëã to one
more than the length from ùëãùëñ to ùëãùëù . To join two states, we intersect the separators and take the
maximum length between the results of the two path functions (where defined, or 0 otherwise).
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:19
assumeùõº : RV ‚Üí ùëü ‚Üí G ‚Üí G
observeùõº : RV ‚Üí ùëü ‚Üí G ‚Üí G
valueùõº : ùëü ‚Üí G ‚Üí G
‚äîùõº : G ‚Üí G ‚Üí G
Fig. 11. Abstract graph interface.
G F {ùëñùëõ ‚äÜ RV;con ‚äÜ RV}
assumemc(ùëã, ùëü, G) = {G.ùëñùëõ ‚à™ {ùëã}; G.con ‚à™ ùëü .lb}
observemc(ùëã, ùëü, G) = {G.ùëñùëõ; G.con ‚à™ ùëü .lb ‚à™ {ùëã}}
valuemc(ùëü, G) = {G.ùëñùëõ; G.con ‚à™ ùëü .lb}
G1 ‚äîmc G2 = {G1.ùëñùëõ ‚à™ G2.ùëñùëõ; G1.con ‚à© G2.con}
Fig. 12. ùëö-consumed abstract graph operations.
G F {ùëù : RV √ó RV ‚Ü©‚Üí N;sep ‚äÜ RV}
assumeup (ùëã, ùëü, G) = {ùëù
‚Ä≤
; G.sep} where ùëù
‚Ä≤
(ùëã, ùëã) ‚Ü¶‚Üí 0,
ùëù
‚Ä≤
(ùëãùëñ
, ùëã) ‚Ü¶‚Üí G.ùëù(ùëãùëñ
, ùëãùëù ) + 1 for all ùëãùëù ‚àà ùëü .ub \ G.sep, ùëãùëñ ‚àà RV,
ùëù
‚Ä≤
(ùëã, ùëå) ‚Ü¶‚Üí G.ùëù(ùëã, ùëå) otherwise
observeup (ùëã, ùëü, G) = {G.ùëù; G.sep ‚à™ ùëü .lb ‚à™ {ùëã}}
valueup (ùëü, G) = {G.ùëù; G.sep ‚à™ ùëü .lb}
G1 ‚äîup G2 = {ùëù
‚Ä≤
; G1.sep ‚à© G2.sep} where ùëù
‚Ä≤
(ùë£1, ùë£2) ‚Ü¶‚Üí max(G1.ùëù(ùë£1, ùë£2), G2.ùëù(ùë£1, ùë£2))
Fig. 13. Unseparated paths abstract graph operations.
6.3 Typing Rules
In Figure 14 we present the typing rules that are relevant to analyzing probabilistic streams, with
the full definition in Appendix D. Constants reference no random variables. sample introduces a
fresh random variable sampled from its argument and adds it to the graph. observe introduces
an intermediate random variable for its first argument by the same mechanism as sample, and
observes it to be the evaluation of its second argument.
Operators and Scalar Folding. We use ùúáùêπ operators ùëúùëù to describe probability distributions and
other operations over scalars and assume them to have scalar return values. The auxiliary judgment
‚Üò folds products and stream instances into scalars by taking unions of variable sets.
() ‚Üò ( ‚àÖ, ‚àÖ) ùëü ‚Üò ùëü
ùë°1 ‚Üò (lb, ub) ùë°2 ‚Üò (lb‚Ä≤
, ub‚Ä≤
)
ùë°1 √ó ùë°2 ‚Üò (lb ‚à™ lb‚Ä≤
, ub ‚à™ ub‚Ä≤
)
ùë° ‚Üò (lb, ub)
stream(ùë°, ùë†) ‚Üò (lb, ub) bounded ‚Üò ( ‚àÖ, ‚àÖ)
Sequencing. Sequencing using the let-expression follows the standard typing rule for let, and
also threads the output graph of evaluating ùëí into the evaluation of ùëí
‚Ä≤
.
() ‚äî () = ()
(ùë°1 √ó ùë°2) ‚äî (ùë°
‚Ä≤
1 √ó ùë°
‚Ä≤
2
) = (ùë°1 ‚äî ùë°
‚Ä≤
1
) √ó (ùë°2 ‚äî ùë°
‚Ä≤
2
)
(lb, ub) ‚äî (lb‚Ä≤
, ub‚Ä≤
) = (lb ‚à© lb‚Ä≤
, ub ‚à™ ub‚Ä≤
)
Fig. 15. Join operator for types.
Conditionals and Join. if-expressions evaluate
the condition, check both branches in parallel, and
join the resulting reference set and graphs. The join
operator ‚äî (Figure 15), representing the conservative union of two types, unions the upper bounds
and intersects the lower bounds. We disallow ifbranching over functions and stream instances.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:20 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Œì ‚ä¢ùõº ùëê : (‚àÖ, ‚àÖ)
Œì ‚ä¢ùõº ùë£ : ùëü ùëã = fresh(G)
Œì, G ‚ä¢ùõº sample(ùë£) : ({ùëã}, {ùëã}), assumeùõº (ùëã, ùëü, G)
Œì, G ‚ä¢ùõº sample(ùë£1) : ({ùëã}, {ùëã}), G
‚Ä≤
Œì ‚ä¢ùõº ùë£2 : ùëü2
Œì, G ‚ä¢ùõº observe(ùë£1,ùë£2) : (), observeùõº (ùëã, ùëü2, valueùõº (ùëü2, G
‚Ä≤
))
Œì ‚ä¢ùõº ùë£ : ùë° ùë° ‚Üò ùëü
Œì ‚ä¢ùõº op(ùë£) : ùëü
Œì, G ‚ä¢ùõº ùëí : ùë°, G
‚Ä≤
Œì, ùëù : ùë°, G
‚Ä≤
‚ä¢ùõº ùëí
‚Ä≤
: ùë°
‚Ä≤
, G
‚Ä≤‚Ä≤
Œì, G ‚ä¢ùõº let ùëù = ùëí in ùëí
‚Ä≤
: ùë°
‚Ä≤
, G
‚Ä≤‚Ä≤
Œì ‚ä¢ùõº ùë£ : ùëü G
‚Ä≤ = valueùõº (ùëü, G) Œì, G
‚Ä≤
‚ä¢ùõº ùëí1 : ùë°1, G1 Œì, G
‚Ä≤
‚ä¢ùõº ùëí2 : ùë°2, G2
Œì, G ‚ä¢ùõº if ùë£ then ùëí1 else ùëí2 : ùë°1 ‚äî ùë°2, G1 ‚äîùõº G2
Œì ‚ä¢ùõº ùëö : (ùë°, ùë†)
Œì ‚ä¢ùõº init(ùëö) : stream(ùë°, ùë†)
Œì ‚ä¢ùõº ùë• : stream(ùë°, stepfn(ùëùstate, ùëùin, Œìùëí, ùëí)) Œì ‚ä¢ùõº ùë£ : ùë°in Œìùëí, ùëùstate : ùë°, ùëùin : ùë°in, G ‚ä¢ùõº ùëí : ùë°
‚Ä≤ √ó ùë°out, G
‚Ä≤
Œì, G ‚ä¢ùõº unfold(ùë•,ùë£) : ùë°out √ó stream(ùë°
‚Ä≤
, stepfn(ùëùstate, ùëùin, Œìùëí, ùëí)), G
‚Ä≤
Œì ‚ä¢mc ùëö bounded Œì ‚ä¢up ùëö bounded
Œì ‚ä¢ùõº infer(ùëö) : bounded
Œì ‚ä¢ùõº ùë• : bounded Œì ‚ä¢ùõº ùë£ : ùë° ùë° ‚Üò (‚àÖ, ‚àÖ)
Œì ‚ä¢ùõº unfold(ùë•,ùë£) : (‚àÖ, ‚àÖ) √ó bounded
Fig. 14. Delayed sampling type system.
Streams and Inference. To facilitate typing of stream functions, we define the following auxiliary
judgment, which computes, for a stream function, the type of its initial state and the syntactic
fragment for its step function.
Œì ‚ä¢ùõº ùëí
‚Ä≤
: ùë°init ùë°init ‚Üò (‚àÖ, ‚àÖ)
Œì ‚ä¢ùõº stream { init = ùëí
‚Ä≤
; step(ùëùstate,ùëùin) = ùëí } : (ùë°init, stepfn(ùëùstate, ùëùin, Œì, ùëí))
Correspondingly, we define the context Œì,ùëö : (ùë°init, stepfn(ùëùstate, ùëùin, Œìùëí, ùëí)) to map the stream
function name ùëö to its initial state type and step function.
Instances that are created by init expose the type of their internal state and their step function.
The unfold rule applies the step function to the current state, yielding an output and an instance
with the new state. It ensures that the argument ùë£ is compatible with the type of the step function.
An infer expression marks the entry point of a new sub-analysis for its new delayed sampling
graph. The premises of the typing rule for infer are the success conditions for both analyses that
must hold regardless of ùõº. This judgment, Œì ‚ä¢ùõº ùëö bounded, states that the stream function ùëö can be
unfolded for an arbitrary number of iterations while satisfying property ùõº starting with an empty
delayed sampling graph.
Instances created by infer possess a newly instantiated delayed sampling graph. Their internal
state contains the delayed sampling graph and bookkeeping information for the inference algorithm.
Thus, the state is hidden to the exterior and the instance is assigned the opaque type bounded.
unfold on a bounded type only requires that the input and output are purely deterministic.
m-consumed Success Condition. We conclude a stream function passes the ùëö-consumed analysis
when all variables that are introduced are consumed by the program. Because an introduced variable
may take several stream iterations to be consumed, we repeatedly execute the analysis until we
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:21
consume all variables and succeed or reach a fixed point and fail. Define the iteration judgment
Œì ‚ä¢ùõº (ùëõ) ùëö : ùë°
‚Ä≤
, G, where ùõº is either ùëöùëê or ùë¢ùëù, as follows:
Œì ‚ä¢ùõº ùëö : (ùë°, stepfn(ùëùstate, ùëùin, Œìùëí, ùëí)) ‚ä¢ùëù ùëùin : ùë°in Œìùëí, ùëùin : ùë°in, ùëùstate : ùë°, ‚ä•ùõº ‚ä¢ùõº ùëí : ùë°out √ó ùë°
‚Ä≤
, G
Œì ‚ä¢ùõº (0) ùëö : ùë°
‚Ä≤
, G
Œì ‚ä¢ùõº ùëö : (ùë°, stepfn(ùëùstate, ùëùin, Œìùëí, ùëí)) Œì ‚ä¢ùõº (ùëõ‚àí1) ùëö : ùë°
‚Ä≤
, G
‚ä¢ùëù ùëùin : ùë°in Œìùëí, ùëùin : ùë°in, ùëùstate : ùë°
‚Ä≤
, G ‚ä¢ùõº ùëí : ùë°out √ó ùë°
‚Ä≤‚Ä≤
, G
‚Ä≤
Œì ‚ä¢ùõº (ùëõ) ùëö : ùë°
‚Ä≤‚Ä≤
, G
‚Ä≤
On each iteration, this judgment applies the appropriate type rule for the step function and returns
the result, using the abstract graph from the previous iteration as the context for the step function
rule. The initial iteration uses an empty abstract graph as the context, represented by ‚ä•ùõº . For the
ùëö-consumed analysis, we specialize the judgment to Œì ‚ä¢ùëöùëê (ùëõ) ùëö : ùë°
‚Ä≤
, G, and define ‚ä•mc to be (‚àÖ, ‚àÖ).
The rule continues iterating until it reaches the success condition. The success condition states
that every variable introduced that is kept in the program state must be used with in a bounded
number of time steps. We formalize this as the following type rule:
Œì ‚ä¢ùëöùëê (0) ùëö : ùë°, G ùë° ‚Üò (ùëôùëè, ùë¢ùëè) Œì ‚ä¢ùëöùëê (ùëõ) ùëö : ùë°
‚Ä≤‚Ä≤
, G
‚Ä≤
(G.ùëñùëõ \ G‚Ä≤
.con) ‚à© ub = ‚àÖ
Œì ‚ä¢mc ùëö bounded
Alternatively, if evaluating one more iteration does not consume any more variables, we reach a
fixed point and return failure. Since every iteration we either consume a variable or reach a fixed
point, the analysis is guaranteed to terminate.
Unseparated Paths Success Condition. Like the ùëö-consumed analysis, the unseparated paths
analysis is iterative, and we may need to repeat it for some number of iterations. We specialize the
iteration judgment defined in the previous section to Œì ‚ä¢ùë¢ùëù (ùëõ) ùëö : ùë°, G and define ‚ä•up to be a pair
of an empty map and an empty set. Define path(ùë°, G) where ùë° ‚Üò (lb, ub) to be the length of the
longest path from any random variable in ub to any other variable in G.ùëù. Then we conclude the
program passes the unseparated path analysis when the length of the longest path converges after
some finite number of iterations:
Œì ‚ä¢ùë¢ùëù (ùëõ) ùëö : ùë°, G Œì ‚ä¢ùë¢ùëù (ùëõ+(path(ùë°,G)‚àósize(ùë°))+1) ùëö : ùë°
‚Ä≤‚Ä≤
, G
‚Ä≤
path(ùë°, G) = path(ùë°
‚Ä≤‚Ä≤
, G
‚Ä≤
)
Œì ‚ä¢up ùëö bounded
The implementation of this rule repeatedly computes a new abstract graph starting from the
previous iteration‚Äôs output. It exits when the longest path length at the current iteration is equal to
the longest path after (path(ùë°, G) ‚àó size(ùë°)) + 1 additional iterations. The function size determines,
for a given type ùë°, how many values of base type are contained in ùë°.
size(ùëü) = 1 size(ùë°1 √ó ùë°2) = size(ùë°1) + size(ùë°2)
The extra iterations ensure that the path length has stabilized and the analysis can safely conclude
that there is a bound on the length of the longest unseparated path.
If the path length check fails, the implementation keeps iterating until a pre-specified bound is
reached. Upon reaching this bound, the implementation outputs an analysis failure. Note that the
analysis may be imprecise and reject correct programs if the bound is not sufficiently high.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:22 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
6.4 Example Type Derivation
This section presents an example type derivation used by the analysis to confirm that the program
in Figure 1 satisfies the ùëö-consumed property. In particular, we confirm that the stream function
kalman passes the ùëö-consumed analysis.
Using ùëí as shorthand for the body of its step function, we derive the following ùëö-consumed
success condition for kalman:
Œì ‚ä¢mc(0) kalman : ( ({ùëã}, {ùëã}) √ó ({ùëã}, {ùëã})) ( ({ùëã}, {ùëã}) √ó ({ùëã}, {ùëã})) ‚Üò ({ùëã}, {ùëã})
Œì ‚ä¢mc(0) kalman : ( ({ùëã}, {ùëã}) √ó ({ùëã}, {ùëã})) ({ùëã} \ {ùëã}) ‚à© {ùëã} = ‚àÖ
Œì ‚ä¢mc kalman bounded
The second and fourth premises follow immediately from definitions and set operations. The
derivation of the first and third premises is as follows:
Œì ‚ä¢mc kalman : ( (‚àÖ, ‚àÖ), stepfn(pre_x, obs, Œìùëí, ùëí)) ‚ä¢ùëù obs : (‚àÖ, ‚àÖ)
Œìùëí, obs : (‚àÖ, ‚àÖ), pre_x : (‚àÖ, ‚àÖ), (‚àÖ, ‚àÖ) ‚ä¢mc ùëí : ( ({ùëã}, {ùëã}) √ó ({ùëã}, {ùëã})), ({ùëã}, {ùëã})
Œì ‚ä¢mc(0) kalman : ( ({ùëã}, {ùëã}) √ó ({ùëã}, {ùëã}))
The second premise follows from definitions. The derivation of the first premise is as follows:
Œìùëí ‚ä¢mc 0.0 : (‚àÖ, ‚àÖ) (‚àÖ, ‚àÖ) ‚Üò (‚àÖ, ‚àÖ)
Œì ‚ä¢mc stream { init = 0.0 ; step(pre_x,obs) = ùëí } : ( (‚àÖ, ‚àÖ), stepfn(pre_x, obs, Œìùëí, ùëí))
where the premises follow immediately. Finally, let Œì
‚Ä≤ be the context Œìùëí, obs : (‚àÖ, ‚àÖ), pre_x : (‚àÖ, ‚àÖ)
and ùë° be the type (‚àÖ, ‚àÖ). The derivation of the third premise is as follows.
Œì
‚Ä≤
, (‚àÖ, ‚àÖ) ‚ä¢ùëöùëê sample(gaussian(pre_x, 1.0)) : ùë°, ({ùëã}, ‚àÖ)
Œì
‚Ä≤
, x : ùë°, ({ùëã}, ‚àÖ) ‚ä¢ùëöùëê let () = observe(gaussian(x, 1.0),obs) in (x, x) : (ùë° √ó ùë°), ({ùëã}, {ùëã})
Œì
‚Ä≤
, (‚àÖ, ‚àÖ) ‚ä¢ùëöùëê ùëí : (ùë° √ó ùë°), ({ùëã}, {ùëã})
The first premise follows from the typing rule for sample. The second premise follows from the
typing rule for let as follows:
Œì
‚Ä≤
, x : ùë°, ({ùëã}, ‚àÖ) ‚ä¢ùëöùëê observe(gaussian(x, 1.0),obs) : (), ({ùëã}, {ùëã})
Œì
‚Ä≤
, x : ùë°, ({ùëã}, {ùëã}) ‚ä¢ùëöùëê (x, x) : (ùë° √ó ùë°), ({ùëã}, {ùëã})
Œì
‚Ä≤
, x : ùë°, ({ùëã}, ‚àÖ) ‚ä¢ùëöùëê let () = observe(gaussian(x, 1.0),obs) in (x, x) : (ùë° √ó ùë°), ({ùëã}, {ùëã})
where the first premise follows from the rule for observe and the second from the rule for pairs.
6.5 Soundness
Here, we outline how we show the type system is sound. We give a high-level overview of the
approach; the details are in Appendix E.
Entailment Relations. In Appendix E.2, we establish several entailment relations that relate semantic objects to their type-level counterparts. These relations are parameterized by ùõº which is
either ùëöùëê for the ùëö-consumed relation or ùë¢ùëù for the unseparated path relation. We write ùë£ ‚ä®ùõº ùë°
to mean a value entails a type. We write ùõæ ‚ä®ùõº Œì to mean an environment entails a type context.
We write ùë£, (ùëî, ùúè) ‚ä®ùõº ùë°, G to mean a value and traced graph (see Section 5.2 for the definition of a
traced graph) entail a type and abstract graph. We write ùõæ, (ùëî, ùúè) ‚ä®ùõº Œì, G to mean an environment
and traced graph entail a type context and abstract graph.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:23
Soundness. The following theorems establish the soundness of the type system. The first theorem
states that the type system soundly ascribes types to values and soundly updates the abstract
delayed sampling graph:
Theorem 6.1 (ùëö-consumed and Unseparated Path Soundness). If ùõæ, (ùëî, ùúè) ‚ä®ùõº Œì, G and Œì, G ‚ä¢ùõº
ùëí : ùë°, G
‚Ä≤ and {[ùëí]}ùõæ
(ùëî, ùúè),ùë§ = ùë£, (ùëî
‚Ä≤
, ùúè ‚Ä≤
),ùë§‚Ä≤
, then ùë£, (ùëî
‚Ä≤
, ùúè ‚Ä≤
) ‚ä®ùõº ùë°, G
‚Ä≤
.
Next, the type system soundly ensures a stream function maintains bounded memory.
Theorem 6.2 (Analysis Soundness). If ùõæ ‚ä®ùõº Œì and Œì ‚ä¢ùõº ùëö : bounded, then ‚ü¶ùëö‚üßùõæ
‚ä®ùõº bounded.
We prove these theorems in Appendix E.2.
6.6 Implementation
We implemented our analysis framework and the ùëö-consumed and unseparated paths analyses in
OCaml. Our implementation takes as input a ùúáùêπ program and outputs either true or false for each
analysis. It also accepts a parameter for the iteration bound for the unseparated paths analysis.
The implementation goes beyond the type system laid out in the paper by supporting functions
that have probabilistic effects as well as interfaces for list and array operations. ùúáùêπ programs can
further be compiled to OCaml and executed using the ProbZelus delayed sampling runtime. The
code is available at https://github.com/psg-mit/probzelus-analysis-impl.
7 EVALUATION
To evaluate the ability of the analysis to accept only ùúáùêπ programs that can execute in bounded
memory, we executed it on several benchmarks reflective of real-world inference tasks.
Research Questions. We used our implementation to answer two research questions. For realistic
probabilistic programs, (1) does the type system precisely verify the properties required for boundedmemory execution, and (2) is a small iteration bound sufficient for the unseparated paths analysis?
7.1 Methodology
We executed the analysis on example programs from Baudart et al. [2020] originally written in
ProbZelus, a probabilistic programming language featuring probabilistic data streams and delayed
sampling. We manually translated them to ùúáùêπ , and they reflect a range of realistic control problems
with different memory usage characteristics. For the unseparated paths analysis, we set an iteration
count bound of 10, which was sufficient for these programs. We compared the outputs of the
analysis to our manual logical reasoning about the ability of each of the following programs to
execute in bounded memory. We provide source code for all benchmarks in Appendix F.
Kalman is the simplified core model of Figure 1 and models an agent that estimates position
from noisy observations. Applying delayed sampling on this model is equivalent to a Kalman filter
[Kalman 1960] where each particle returns the exact solution.
Kalman Hold-First is the example from Figure 3 with a reference to the output of the first iteration.
Gaussian Random Walk is a simplification of Kalman that does not observe of the true position,
effectively expressing a Gaussian random walk.
Robot is the full example from Figure 1 that includes the Kalman core model as well as a main
stream function that invokes a controller based on the inferred position.
Coin models an agent that estimates the bias of a coin. The model chooses the probability of
the coin from a uniform distribution, and thereafter chooses the observations by flipping a coin
with that probability. Applying delayed sampling to this model is equivalent to exact inference in a
Beta-Bernoulli conjugate model [Fink 1997] where each particle returns the exact solution.
Gaussian-Gaussian estimates the mean and variance of a Gaussian distribution.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:24 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Table 1. Bounded memory analysis on benchmark programs.
ùëö-consumed unsep. paths bounded mem.
output actual output actual output actual
Kalman ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì
Kalman Hold-First ‚úì ‚úì ‚úó ‚úó ‚úó ‚úó
Gaussian Random Walk ‚úó ‚úó ‚úì ‚úì ‚úó ‚úó
Robot ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì
Coin ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì
Gaussian-Gaussian ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì
Outlier ‚úó ‚úó ‚úì ‚úì ‚úó ‚úó
MTT ‚úó ‚úó ‚úì ‚úì ‚úó ‚úó
SLAM ‚úó ‚úì ‚úì ‚úì ‚úó ‚úì
Outlier, adapted from Section 2 of [Minka 2001], models the same situation as the Kalman
benchmark, but with a sensor that can occasionally produce invalid readings. The model chooses
the probability of an invalid reading from a beta(100,1000) distribution, so that invalid readings
occur approximately 10% of the time. At each time step, with the previously chosen probability,
the model chooses the observation from either the invalid distribution gaussian(0,100) or the
Kalman model. Applying delayed sampling to this model is equivalent to a Rao-Blackwellized
particle filter [Doucet et al. 2000b] combining exact inference with approximate particle filtering.
MTT (Multi-Target Tracker) is adapted from [Murray and Sch√∂n 2018] and involves a variable
number of targets with linear-Gaussian 2D position/velocity motion models that produce measurements of position at each time step. The model randomly introduces targets as a Poisson process
and deletes them with fixed probability at each step.
SLAM (Simultaneous Localization and Mapping) is adapted from [Doucet et al. 2000a] and models
an agent that estimates its position on a one-dimensional grid and also a map of its environment
associating each cell with black or white. The robot uses inference to decide its next move, but its
motion commands are noisy with some probability that its wheels may slip, and its observations
may also be incorrectly reported.
7.2 Analysis Results
Table 1 displays the analysis outputs for each of the benchmark programs. For each analysis, the
‚Äúoutput‚Äù column is the result of the implementation, and the ‚Äúactual‚Äù column is the ground truth, i.e.,
whether the program satisfies the semantic property according to manual analysis. The ‚Äúbounded
memory‚Äù columns are the logical conjunction of the two semantic properties.
For the first six benchmarks, the analysis implementation yielded the same answer as manual
analysis for whether the program satisfies both semantic properties and thus permits execution in
bounded memory. In every case, the output of the implementation is sound with respect to the
ground truth. Furthermore, all unseparated-path analyses converged within 10 iterations.
Kalman. For this program, every variable is ùëö-consumed for ùëö ‚â§ 1 and starts an unseparated
path of length at most 1, and thus it can execute in bounded memory.
Kalman Hold-First. For this program, every variable is ùëö-consumed for ùëö ‚â§ 1. However, the
analysis detects that unseparated paths starting from the initial value for x grow without bound
and fail to converge after 10 iterations, so this program cannot execute in bounded memory.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:25
Gaussian Random Walk. Here, every unseparated path has length at most 1. However, the analysis
detects that there is noùëö such that any variable isùëö-consumed because no variable is ever observed
or evaluated, so this program cannot execute in bounded memory.
Robot. Every variable is 1-consumed and every separated path has length at most 1. The analysis
succeeds and indicates this program can execute in bounded memory.
Coin. Every variable is 1-consumed and every unseparated path has length at most 1. The analysis
succeeds and indicates this program can execute in bounded memory.
Gaussian-Gaussian. Every variable is 1-consumed and every separated path has length at most 1.
The analysis succeeds and indicates this program can execute in bounded memory.
Outlier. Every unseparated path has length at most 1. However, in the event that samples
are indefinitely considered outliers, no observation will occur that causes the variable xt to be
consumed, so this program cannot execute in bounded memory.
MTT. Every unseparated path has length at most 1. However, not all random variables are
guaranteed to be consumed, as the final observe operation is only executed based on a dynamic
condition on the lengths of two list data structures. Because this condition is not guaranteed to be
met, this program cannot execute in bounded memory.
SLAM. Every unseparated path has length at most 1. The analysis concludes that the environment
map array is not consumed because the model makes random choices that are not guaranteed to
cover all the entries of the map. However, manual examination shows that an entry of the map
that is never covered by a random choice is 0-consumed by virtue of being never used. Thus, the
analysis soundly but imprecisely determines that the ùëö-consumed condition fails.
7.3 Discussion
For the Outlier and MTT benchmarks, even though both fail the ùëö-consumed semantic property
and therefore are not guaranteed to execute in bounded memory, they will almost certainly execute
in bounded memory. For example, in Outlier, the only way that the memory consumption of the
model will increase indefinitely is if a particular random choice always takes one branch, which is
a probability-zero event. In general, our semantic properties and analysis implementation reason
about the absence of any program execution that yields unbounded memory. However, in practice,
almost certain bounded-memory execution may also be a useful property of programs.
In general, the analysis can provide a sound guarantee that a program executes with bounded
memory. However, as we saw with SLAM, it is not always precise enough such that if it rejects a
program, then the program must have unbounded memory consumption. For example, it is possible
to deliberately construct pathological programs requiring a large number of iterations for the
unseparated paths analysis. Remaining limitations on precision include common static analysis
challenges such as path sensitivity due to if statements and aliasing due to complex data structures.
When facing conditional branches, the analysis takes a conservative approach that may not
utilize all statically available knowledge. Specifically, it cannot determine that certain branches
are taken at most once over the entire input stream or that only certain program paths are valid
over multiple sequential branches. The analysis also cannot accurately track variables that are
stored into complex data structures, meaning it cannot mark them as consumed. We discuss these
challenges in greater detail and provide specific examples in Appendix G.
8 RELATED WORK
Resource Analysis for Probabilistic Programs. Static resource analysis is capable of automatically
determining upper bounds for resources such as time or memory required to execute a probabilistic
program. Ngo et al. [2018] proposed a weakest-precondition approach to determine the expected
memory usage of a probabilistic program, which bounds the number of loop iterations executed
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:26 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
and number of explicit memory allocation ticks encountered. Our analysis, on the other hand,
extends static reasoning to the inherent memory usage of the inference algorithm itself.
Reactive Probabilistic Programming. Gupta et al. [1997] first introduced the idea of reactive
probabilistic programming. They extend a concurrent constraint language with random variables.
In contrast, our language is based on a synchronous dataflow model and focus on resource analysis.
Baudart et al. [2020] developed ProbZelus, a reactive probabilistic programming language which
operates over streams of data and supports inference at each stream iteration. It uses an implementation of delayed sampling designed to provide bounded-memory inference for a class of reactive
probabilistic programs. However, ProbZelus provides no static guarantee of bounded-memory
inference. In this work, we define a language that can be used as a target for the compilation
of ProbZelus and identify the semantic conditions and a static analysis that makes it possible to
provide a static guarantee.
Delayed Sampling and Bounded-Memory Inference. The mechanism of delayed sampling in probabilistic programs was introduced by Murray et al. [2018] and implemented in the Anglican and
Birch programming languages, neither of which supports inference over streams. Delayed sampling,
a form of Sequential Monte Carlo [Liu and Chen 1998], can execute in bounded memory because it
automates the construction of Rao-Blackwellized particle filters [Doucet et al. 2000b], a particularly
efficient variant of SMC. By comparison, Markov chain Monte Carlo techniques generally cannot
execute in bounded memory because they maintain a sample of the full history of program execution, the size of which can grow without bound for a probabilistic stream. Variational inference has
extensions that make it amenable to streaming [Broderick et al. 2013], but we are not aware of any
probabilistic programming system that makes use of them.
Other programming languages such as Hakaru [Narayanan et al. 2016] use static program
transformations to accomplish the same goal of deferring approximate inference as much as possible.
It is unclear if these transformations apply to a streaming context, where dynamic information is
necessary to reflect the evolution of the underlying model over many iterations.
9 CONCLUSION
Probabilistic programming has been augmented by constructs that perform inference over unbounded iterations on streams of data. Underlying this programming model is delayed sampling,
which combines the benefits of exact inference and the flexibility of sampling.
In our paper, we introduce the ùëö-consumed and unseparated path semantic properties, which
show that delayed sampling can execute in bounded memory for reactive probabilistic programs. We
present a sound static analysis that verifies these two properties with a type system and an abstract
delayed sampling graph. To the best of our knowledge, our work is the first to develop a resource
analysis for a probabilistic program in relation to its probabilistic programming system‚Äôs underlying inference algorithm. We hope this work will enable automatic inference mechanisms whose
performance is better understood by model developers in probabilistic programming languages.
ACKNOWLEDGMENTS
We would like to thank Cambridge Yang, Alex Renda, Jesse Michel, and Ben Sherman, who all
provided feedback on drafts of this paper. This work was supported in part by the MIT-IBM Watson
AI Lab and the Office of Naval Research (ONR N00014-17-1-2699). Any opinions, findings, and
conclusions or recommendations expressed in this material are those of the authors and do not
necessarily reflect the views of the Office of Naval Research.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:27
REFERENCES
Eric Atkinson, Cambridge Yang, and Michael Carbin. 2018. Verifying Handcoded Probabilistic Inference Procedures. In
arXiv e-prints.
Guillaume Baudart, Louis Mandel, Eric Atkinson, Benjamin Sherman, Marc Pouzet, and Michael Carbin. 2020. Reactive
Probabilistic Programming. In Conference on Programming Language Design and Implementation.
Leonard E. Baum and Ted Petrie. 1966. Statistical Inference for Probabilistic Functions of Finite State Markov Chains. The
Annals of Mathematical Statistics 37, 6 (1966).
Atilim G√ºne≈ü Baydin, Lei Shao, Wahid Bhimji, Lukas Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid
Naderiparizi, Bradley Gram-Hansen, Gilles Louppe, Mingfei Ma, Xiaohui Zhao, Philip Torr, Victor Lee, Kyle Cranmer,
Prabhat, and Frank Wood. 2019. Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale. In
Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC ‚Äô19).
Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh,
Paul Szerlip, Paul Horsfall, and Noah D. Goodman. 2019. Pyro: Deep Universal Probabilistic Programming. Journal of
Machine Learning Research 20, 28 (2019).
Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. 2013. Streaming Variational
Bayes. In International Conference on Neural Information Processing Systems.
Jean-Louis Cola√ßo, Bruno Pagano, and Marc Pouzet. 2017. SCADE 6: A formal language for embedded critical software
development (invited paper). In TASE. IEEE Computer Society, 1‚Äì11.
Marco F Cusumano-Towner, Feras A Saad, Alexander K Lew, and Vikash K Mansinghka. 2019. Gen: a General-purpose
Probabilistic Programming System with Programmable Inference. In Conference on Programming Language Design and
Implementation.
Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. 2006. Sequential Monte Carlo samplers. J. Royal Statistical Society: Series
B (Statistical Methodology) 68, 3 (2006), 411‚Äì436.
Arnaud Doucet, Nando de Freitas, Kevin P. Murphy, and Stuart J. Russell. 2000a. Rao-Blackwellised Particle Filtering for
Dynamic Bayesian Networks. In UAI.
Arnaud Doucet, Nando de Freitas, Kevin P. Murphy, and Stuart J. Russell. 2000b. Rao-Blackwellised Particle Filtering for
Dynamic Bayesian Networks. In Conference on Uncertainty in Artificial Intelligence.
Daniel Fink. 1997. A Compendium of Conjugate Priors. (1997).
Hong Ge, Kai Xu, and Zoubin Ghahramani. 2018. Turing: Composable inference for probabilistic programming. In
International Conference on Artificial Intelligence and Statistics.
Andrew Gelman, Daniel Lee, and Jiqiang Guo. 2015. Stan: A probabilistic programming language for Bayesian inference
and optimization. Journal of Educational and Behavioral Statistics 40, 5 (2015), 530‚Äì543.
Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B. Tenenbaum. 2008. Church: A
language for generative models. In Conference on Uncertainty in Artificial Intelligence.
Noah D Goodman and Andreas Stuhlm√ºller. 2014. The Design and Implementation of Probabilistic Programming Languages.
http://dippl.org. Accessed: 2020-10-30.
Andrew D. Gordon, Thore Graepel, Nicolas Rolland, Claudio Russo, Johannes Borgstrom, and John Guiver. 2014. Tabular: a
schema-driven probabilistic programming language. In Symposium on Principles of Programming Languages.
Vineet Gupta, Radha Jagadeesan, and Vijay A. Saraswat. 1997. Probabilistic Concurrent Constraint Programming. In
CONCUR (Lecture Notes in Computer Science, Vol. 1243). Springer, 243‚Äì257.
Daniel Huang, Jean-Baptiste Tristan, and Greg Morisett. 2017. Compiling Markov Chain Monte Carlo Algorithms for
Probabilistic Modeling. In Conference on Programming Language Design and Implementation.
R. E. Kalman. 1960. A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering 82, 1 (1960).
Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models - Principles and Techniques. MIT Press.
Jun S. Liu and Rong Chen. 1998. Sequential Monte Carlo Methods for Dynamic Systems. J. Amer. Statist. Assoc. 93, 443
(1998), 1032‚Äì1044.
Daniel Lund√©n. 2017. Delayed sampling in the probabilistic programming language Anglican. Master‚Äôs thesis. KTH Royal
Institute of Technology.
Vikash Mansingkha, Ulrich Schaechtle, Shivam Handa, Alexey Radul, Yutian Chen, and Martin Rinard. 2018. Probabilistic
Programming with Programmable Inference. In Conference on Programming Language Design and Implementation.
George H. Mealy. 1955. A method for synthesizing sequential circuits. The Bell System Technical Journal 34, 5 (1955),
1045‚Äì1079.
Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag, Daniel L. Ong, and Andrey Kolobov. 2007. BLOG: Probabilistic
models with unknown objects. Statistical relational learning (2007).
Thomas P. Minka. 2001. Expectation Propagation for Approximate Bayesian Inference. In Conference in Uncertainty in
Artificial Intelligence.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:28 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Lawrence M. Murray, Daniel Lund√©n, Jan Kudlicka, David Broman, and Thomas B. Sch√∂n. 2018. Delayed Sampling and
Automatic Rao-Blackwellization of Probabilistic Programs. In International Conference on Artificial Intelligence and
Statistics.
Lawrence M. Murray and Thomas B. Sch√∂n. 2018. Automated learning with a probabilistic programming language: Birch.
Annual Reviews in Control 46 (2018).
Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh Shan, and Robert Zinkov. 2016. Probabilistic inference by
program transformation in Hakaru (system description). In International Symposium on Functional and Logic Programming.
Van Chan Ngo, Quentin Carbonneaux, and Jan Hoffmann. 2018. Bounded Expectations: Resource Analysis for Probabilistic
Programs. In Conference on Programming Language Design and Implementation.
Aditya V. Nori, Sherjil Ozair, Sriram K. Rajamani, and Deepak Vijaykeerthy. 2015. Efficient Synthesis of Probabilistic
Programs. In Conference on Programming Language Design and Implementation.
Avi Pfeffer. 2009. Figaro: An object-oriented probabilistic programming language. Vol. 137. 96.
Eduardo D Sontag. 2013. Mathematical control theory: deterministic finite dimensional systems. Vol. 6. Springer Science &
Business Media.
Sam Staton. 2017. Commutative Semantics for Probabilistic Programming. In European Symposium on Programming.
Dustin Tran, Matthew D Hoffman, Rif A Saurous, Eugene Brevdo, Kevin Murphy, and David M Blei. 2017. Deep probabilistic
programming. In International Conference on Learning Representations.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:29
A IDEAL SEMANTICS
In this section we present the complete semantics of the deterministic part of ùúáùêπ in Figure 16 and
the ideal semantics of the probabilistic part in Figure 17.
The probabilistic semantics of Figure 17 is a measure-based semantics similar to one presented
in [Staton 2017]. Given an environment ùõæ, an expression is interpreted as a measure {[ùëí]}ùõæ : Œ£ùê∑ ‚Üí
[0, 1), that is, a function which associates a positive number to each measurable set ùëà ‚àà Œ£ùê∑ , where
Œ£ùê∑ denotes the Œ£-algebra of the domain of the expression ùê∑, i.e., the set of measurable sets of
possible values. sample(ùë£) returns the distribution ‚ü¶ùë£‚üßùõæ . observe(ùë£1,ùë£2) weights execution paths
using the likelihood of the observation ‚ü¶ùë£2‚üßùõæ w.r.t. the distribution ‚ü¶ùë£1‚üßùõæ (for a distribution ùúá we
note ùúápdf its probability density function). Local definitions are interpreted as integration, and we
use the Dirac delta measure to interpret deterministic expressions.
B CORE TYPES IN ùúáùêπ
This section describes a type system for ùúáùêπ programs. All programs we consider in this work
must type check according to this system. The type system ensures that if an expression ùëí is
given a probabilistic typing judgment Œì ‚ä¢prob ùëí : ùëá (which means that ùëí will be evaluated using
its probabilistic semantics {[ùëí]} rather than its deterministic semantics ‚ü¶ùëí‚üß), then its type ùëá is a
measurable space that does not include nonmeasurable objects such as functions. The type system
also prohibits nested inference.
The types of ùúáùêπ are unit, Booleans, reals, functions, and pairs, as well as probability distributions,
and deterministic and probabilistic stream functions and stream instances.
ùëá ::= unit | bool | real | ùëá ‚Üí ùëá | ùëá √óùëá | distr ùëá
| dstreamfn(ùëá ,ùëá ) | dstream(ùëá ,ùëá ) | pstreamfn(ùëá ,ùëá ) | pstream(ùëá ,ùëá )
Only a subset of these types may act as the support of probability distributions, denoted by the
judgment measurable(ùëá ). These exclude function and stream types:
measurable(unit) measurable(bool) measurable(real)
measurable(ùëá1) measurable(ùëá2)
measurable(ùëá1 √óùëá2)
measurable(ùëá )
measurable(distr ùëá )
We present the full type system of ùúáùêπ in Figures 18 and 19.
C DEFINITION OF graft
In this section, we review the definition of graft from Murray et al. [2018].
C.1 Preliminaries
This definition makes use of an alternative type of marginalized node that maintains its own
marginal distribution as well as a conditional distribution that relates the marginalized node to
its unique marginalized child. We use the notation Marginalized(ùúámarg, ùúácond ) to refer to such a
marginalized node with marginal distribution ùúámarg and conditional distribution ùúácond . We use the
notation ùë† = Marginalized(_) to mean that the node state ùë† is a marginalized node of any type.
The two types of marginalized nodes only differ in the distributions they store, and have the same
reachability and memory consumption properties.
Murray et al. [2018] defines invariants of delayed sampling runtimes. Namely, it specifies that
delayed sampling maintains that (1) all nodes in the delayed sampling graph have at most one
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:30 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
‚ü¶val ùë• = ùëí‚üßùõæ = ùõæ [ùë• ‚Üê ‚ü¶ùëí‚üßùõæ ]
‚ü¶val ùëì = fun ùëù -> ùëí‚üßùõæ = ùõæ [ùëì ‚Üê (ùúÜùë£. ‚ü¶ùëí‚üßùõæ+ [ùë£/ùëù ])]
‚ü¶ùëë1 ùëë2‚üßùõæ = let ùõæ1 = ‚ü¶ùëë1‚üßùõæ in ‚ü¶ùëë2‚üßùõæ1
‚ü¶val ùëö = stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }‚üßùõæ
= ùõæ [ùëö ‚Üê stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
]
‚ü¶ùëê‚üßùõæ = ùëê
‚ü¶ùë•‚üßùõæ = ùõæ (ùë•)
‚ü¶(ùë£1,ùë£2)‚üßùõæ = (‚ü¶ùë£1‚üßùõæ,‚ü¶ùë£2‚üßùõæ)
‚ü¶op(ùë£)‚üßùõæ = op(‚ü¶ùë£‚üßùõæ )
‚ü¶ùëì (ùë£)‚üßùõæ = ùõæ (ùëì ) (‚ü¶ùë£‚üßùõæ )
‚ü¶let ùëù = ùëí1 in ùëí2‚üßùõæ = let ùë£ = ‚ü¶ùëí1‚üßùõæ in ‚ü¶ùëí2‚üßùõæ+ [ùë£/ùëù ]
‚ü¶if ùë£ then ùëí1 else ùëí2‚üßùõæ = if ‚ü¶ùë£‚üßùõæ then ‚ü¶ùëí1‚üßùõæ else ‚ü¶ùëí2‚üßùõæ
‚ü¶init(ùëö)‚üßùõæ = let stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
‚Ä≤ = ‚ü¶ùëö‚üßùõæ in
let ùë†init = ‚ü¶ùëíinit‚üßùõæ
‚Ä≤ in
(ùë†init, ùúÜ(ùë†, ùë£). ‚ü¶ùëí‚üßùõæ
‚Ä≤+ [ùë†/ùëùstate,ùë£/ùëùinput ]) if ùëí is deterministic
‚ü¶init(ùëö)‚üßùõæ = let stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
‚Ä≤ = ‚ü¶ùëö‚üßùõæ in
let ùë†init = ‚ü¶ùëíinit‚üßùõæ
‚Ä≤ in
(ùë†init, ùúÜ(ùë†, ùë£). {[ùëí]}ùõæ
‚Ä≤+ [ùë†/ùëùstate,ùë£/ùëùinput ]) if ùëí is probabilistic
‚ü¶unfold(ùë•,ùë£)‚üßùõæ = let ùë£state, ùëì = ‚ü¶ùë•‚üßùõæ in
let ùë£output, ùë£‚Ä≤
state = ùëì (ùë£state, ‚ü¶ùë£‚üßùõæ ) in
(ùë£output, (ùë£
‚Ä≤
state, ùëì ))
‚ü¶infer(ùëö)‚üßùõæ = let stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
‚Ä≤ = ‚ü¶ùëö‚üßùõæ in
let ùë†init = ‚ü¶ùëíinit‚üßùõæ
‚Ä≤ in (ùõøùë†init , infer(ùúÜ(ùë†, ùë£). {[ùëí]}ùõæ
‚Ä≤+ [ùë†/ùëùstate,ùë£/ùëùinput ]))
where infer(ùëì ) = ùúÜ(ùúé, ùë£). let ùúá = ùúÜùëà . ‚à´
ùëÜ
ùúé(ùëëùë†)ùëì (ùë†, ùë£) (ùëà ) in
let ùúà = ùúÜùëà . ùúá(ùëà )/ùúá(‚ä§) in
(ùúã1‚àó (ùúà), ùúã2‚àó (ùúà))
Fig. 16. Deterministic semantics of ùúáùêπ .
parent, and (2) all marginalized nodes in the graph have at most one marginalized or realized child.
In the following definitions, we use the notation parent(ùëã, ùê∏) to mean a function that returns the
unique parent of ùëã in the edge set ùê∏. We also use the notation child(ùëã, ùê∏) to mean a function that
returns the unique realized or marginalized child of the marginalized node ùëã in the edge set ùê∏.
C.2 Definitions
We define the graft function as follows. When called on an initialized node, graft recursively
marginalizes every initialized ancestor of the given node. This means that it performs integration
to incorporate parent information into the distributions of each node in the initialized chain. When
called on a marginalized node, graft calls the prune function.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:31
{[ùë£]}ùõæ = ùúÜùëà . ùõø‚ü¶ùë£‚üßùõæ
(ùëà )
{[op(ùë£)]}ùõæ = ùúÜùëà . ùõøop(‚ü¶ùë£‚üßùõæ )
(ùëà )
{[ùëì (ùë£)]}ùõæ = ùúÜùëà . ùõøùõæ (ùëì ) (‚ü¶ùë£‚üßùõæ )
(ùëà )
{[let ùëù = ùëí1 in ùëí2]}ùõæ = ùúÜùëà . ‚à´
ùëá
{[ùëí1]}ùõæ (ùëëùë¢){[ùëí2]}ùõæ+ [ùë¢/ùëù ] (ùëà )
{[if ùë£ then ùëí1 else ùëí2]}ùõæ = ùúÜùëà . if ‚ü¶ùë£‚üßùõæ then {[ùëí1]}ùõæ (ùëà ) else {[ùëí2]}ùõæ (ùëà )
{[unfold(ùë•,ùë£)]}ùõæ = ùúÜùëà . let ùë£state, ùëì = ‚ü¶ùë•‚üßùõæ in
let ùúá = ùëì (ùë£state, ‚ü¶ùë£‚üßùõæ ) in
‚à´
ùúá(ùëëùë£output, ùëëùë£‚Ä≤
state)ùõø(ùë£output,(ùë£
‚Ä≤
state,ùëì )) (ùëà )
{[sample(ùë£)]}ùõæ = ùúÜùëà . ‚ü¶ùë£‚üßùõæ (ùëà )
{[observe(ùë£1,ùë£2)]}ùõæ = ùúÜùëà . let ùúá = ‚ü¶ùë£1‚üßùõæ in ùúápdf(‚ü¶ùë£2‚üßùõæ ) ‚àó ùõø() (ùëà )
Fig. 17. Probabilistic semantics of ùúáùêπ .
Œì ‚ä¢det ùëí : ùëá
Œì ‚ä¢decl val ùëù = ùëí : Œì, ùëù : ùëá
Œì, ùëù : ùëá ‚ä¢det ùëí : ùëá
‚Ä≤
Œì ‚ä¢decl val ùëì = fun ùëù -> ùëí : Œì, ùëù : ùëá ‚Üí ùëá
‚Ä≤
Œì ‚ä¢decl ùëë1 : Œì
‚Ä≤
Œì
‚Ä≤
‚ä¢decl ùëë2 : Œì
‚Ä≤‚Ä≤
Œì ‚ä¢decl ùëë1 ùëë2 : Œì
‚Ä≤‚Ä≤
Œì ‚ä¢det ùëíinit : ùëástate Œì, (ùëùstate, ùëùinput) : ùëástate √óùëáinput ‚ä¢det ùëístep : ùëástate √óùëáout
Œì ‚ä¢decl val ùëö = stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëístep } : Œì,ùëö : dstreamfn(ùëáinput,ùëáout)
Œì ‚ä¢prob ùëíinit : ùëástate Œì, (ùëùstate, ùëùinput) : ùëástate √óùëáinput ‚ä¢prob ùëístep : ùëástate √óùëáout
Œì ‚ä¢decl val ùëö = stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëístep } : Œì,ùëö : pstreamfn(ùëáinput,ùëáout)
Fig. 18. Typing rules for programs in ùúáùêπ . The judgment Œì ‚ä¢decl ùëë : Œì
‚Ä≤ means that the ùúáùêπ declaration ùëë, when
typed under the typing context Œì, produces the typing context Œì
‚Ä≤
.
graft(ùëã, ùëî) = let (ùëâ , ùê∏, ùëû) = ùëî in
if ùëû(ùëã) = Initialized(ùúá) then
let ùëãpar = parent(ùëã, ùê∏) in
let ùúáprior, ùëî‚Ä≤ =
if ùëû(ùëãpar) = Marginalized(ùúápar) or
ùëû(ùëãpar) = Initialized(ùúápar)
then let (ùëâ
‚Ä≤‚Ä≤, ùê∏‚Ä≤‚Ä≤, ùëû‚Ä≤‚Ä≤) = graft(ùëãpar, ùëî) in
let Marginalized(ùúápar) = ùëû
‚Ä≤‚Ä≤(ùëãpar) in
ùúápar, (ùëâ
‚Ä≤‚Ä≤, ùê∏‚Ä≤‚Ä≤, ùëû‚Ä≤‚Ä≤[ùëãpar ‚Üê Marginalized(ùúápar, ùúá)])
else if ùëû(ùëãpar) = Realized(ùë£) then ùõø (ùë£), (ùëâ , ùê∏ ‚àí (ùëã, ùëãpar), ùëû)
in
let ùúá
‚Ä≤
, (ùëâ
‚Ä≤‚Ä≤, ùê∏‚Ä≤‚Ä≤, ùëû‚Ä≤‚Ä≤) =
‚à´
ùúá dùúáprior, ùëî‚Ä≤
in
(ùëâ
‚Ä≤‚Ä≤, ùê∏‚Ä≤‚Ä≤, ùëû‚Ä≤‚Ä≤[ùëã ‚Üê Marginalized(ùúá
‚Ä≤
)])
else if ùëû(ùëã) = Marginalized(ùúá, ùúáchild ) then prune(ùëã, ùëî)
else ùëî Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:32 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Œì ‚ä¢det () : unit
ùëê ‚àà {true, false}
Œì ‚ä¢det ùëê : bool
ùëê ‚àà R
Œì ‚ä¢det ùëê : real
Œì ‚ä¢det ùëí1 : ùëá1 Œì ‚ä¢det ùëí2 : ùëá2
Œì ‚ä¢det (ùëí1,ùëí2) : ùëá1 √óùëá2
Œì, ùëù1 : ùëá1, ùëù2 : ùëá2 ‚ä¢k ùëí : ùëá
‚Ä≤
Œì, (ùëù1, ùëù2) : ùëá1 √óùëá2 ‚ä¢k ùëí : ùëá
‚Ä≤
Œì, ùë• : ùëá ‚ä¢k ùë• : ùëá
Œì ‚ä¢det ùëí : ùëá Œì ‚ä¢k ùëì : ùëá ‚Üí ùëá
‚Ä≤
Œì ‚ä¢k ùëì (ùëí) : ùëá
‚Ä≤
Œì ‚ä¢det ùëí1 : bool Œì ‚ä¢k ùëí2 : ùëá Œì ‚ä¢k ùëí3 : ùëá
Œì ‚ä¢k if ùëí1 then ùëí2 else ùëí3 : ùëá
Œì ‚ä¢k ùëí1 : ùëá1 Œì, ùëù : ùëá1 ‚ä¢k ùëí2 : ùëá2
Œì ‚ä¢k let ùëù = ùëí1 in ùëí2 : ùëá2
Œì ‚ä¢det ùëö : dstreamfn(ùëáinput,ùëáout)
Œì ‚ä¢det init(ùëö) : dstream(ùëáinput,ùëáout)
Œì ‚ä¢det ùëö : pstreamfn(ùëáinput,ùëáout)
Œì ‚ä¢det infer(ùëö) : pstream(ùëáinput,ùëáout)
Œì ‚ä¢det ùëí1 : dstream(ùëáinput,ùëáout) Œì ‚ä¢det ùëí2 : ùëáinput
Œì ‚ä¢k unfold(ùëí1,ùëí2) : ùëáout √ó dstream(ùëáinput,ùëáout)
Œì ‚ä¢det ùëí1 : pstream(ùëáinput,ùëáout) Œì ‚ä¢det ùëí2 : ùëáinput
Œì ‚ä¢k unfold(ùëí1,ùëí2) : distr ùëáout √ó pstream(ùëáinput,ùëáout)
Œì ‚ä¢det ùëí : ùëá measurable(ùëá )
Œì ‚ä¢prob ùëí : ùëá
Œì ‚ä¢det ùëí : distr ùëá
Œì ‚ä¢prob sample(ùëí) : ùëá
Œì ‚ä¢det ùëí1 : distr ùëá Œì ‚ä¢det ùëí2 : ùëá
Œì ‚ä¢prob observe(ùëí1,ùëí2) : unit
Fig. 19. Deterministic and probabilistic type systems for ùúáùêπ . The typing judgment Œì ‚ä¢det ùëí : ùëá means that the
ùúáùêπ expression ùëí under the context Œì has the deterministic type ùëá . The judgment Œì ‚ä¢prob ùëí : ùëá means that the
ùúáùêπ expression ùëí under context Œì has the probabilistic type ùëá . The judgment Œì ‚ä¢k
ùëí : ùëá stands for either the
deterministic or the probabilistic judgment, where ùëò is instantiated to be ùëëùëíùë° or ùëùùëüùëúùëè. These rules state that
sample and observe can only be used inside the body of a probabilistic stream.
We define the prune function as follows. When called on a marginalized node with a marginalized
or realized child, the function first recursively prunes that child if the child itself is marginalized. If
the node is marginalized, it samples a value for that node and then conditions the current node on
the child taking on that value. If the child node is realized, the function proceeds to immediately
condition the current node on the child node‚Äôs value.
In either case, the conditioning proceeds as follows. The prune function first extracts probability density functions from the relevant measures using the pdf function. It then follows Bayes‚Äô
rule, multiplying the prior and conditional density functions and normalizing the result with the
normalize function. It finally updates the marginal distribution of the given node and removes the
edge connecting the node to its child.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:33
prune(ùëã, ùëî) = let (ùëâ , ùê∏, ùëû) = ùëî in
if ùëû(ùëã) = Marginalized(ùúáùëã, ùúá) then
let ùëãchild = child(ùëã, ùê∏) in
let ùëî
‚Ä≤ = prune(ùëãchild, ùëî) in
if ùëû(ùëãchild ) = Marginalized(ùúáchild ) then
let ùë£, (ùëâ
‚Ä≤‚Ä≤, ùê∏‚Ä≤‚Ä≤, ùëû‚Ä≤‚Ä≤) = value(ùëãchild, ùëî‚Ä≤
) in
let ùëùùëã, ùëùchild |ùëã = pdf (ùúáùëã ), pdf (ùúá) in
let ùúá
‚Ä≤
ùëã
= normalize(ùúÜùë•.ùëùùëã (ùë•) ‚àó ùëùchild |ùëã (ùë£|ùë•)) in
let ùëû
‚Ä≤‚Ä≤‚Ä≤ = ùëû
‚Ä≤‚Ä≤[ùëãchild ‚Üê Realized(ùë£), ùëã ‚Üê Marginalized(ùúá
‚Ä≤
ùëã
)] in
(ùëâ
‚Ä≤‚Ä≤, ùê∏‚Ä≤‚Ä≤ ‚àí (ùëã, ùëãchild ), ùëû‚Ä≤‚Ä≤‚Ä≤)
else if ùëû(ùëãchild ) = Realized(ùë£) then
let ùë£, (ùëâ
‚Ä≤‚Ä≤, ùê∏‚Ä≤‚Ä≤, ùëû‚Ä≤‚Ä≤) = ùëî
‚Ä≤
in
let ùëùùëã, ùëùchild |ùëã = pdf (ùúáùëã ), pdf (ùúá) in
let ùúá
‚Ä≤
ùëã
= normalize(ùúÜùë•.ùëùùëã (ùë•) ‚àó ùëùchild |ùëã (ùë£|ùë•)) in
let ùëû
‚Ä≤‚Ä≤‚Ä≤ = ùëû
‚Ä≤‚Ä≤[ùëã ‚Üê Marginalized(ùúá
‚Ä≤
ùëã
)] in
(ùëâ
‚Ä≤‚Ä≤, ùê∏‚Ä≤‚Ä≤ ‚àí (ùëãchild, ùëã), ùëû‚Ä≤‚Ä≤‚Ä≤)
else ùëî
else ùëî
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:34 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
D COMPLETE ANALYSIS TYPE SYSTEM
The following is the complete definition of the typing judgment Œì, G ‚ä¢ùõº ùëí : ùë°, G
‚Ä≤ describing the
types and abstract graph transitions of expressions.
Œì ‚ä¢ùõº ùëê : (‚àÖ, ‚àÖ) Œì, ùë• : ùë° ‚ä¢ùõº ùë• : ùë°
Œì,ùëö : (ùë°init, stepfn(ùëùstate, ùëùin, Œìùëí, ùëí)) ‚ä¢ùõº ùëö : (ùë°init, stepfn(ùëùstate, ùëùin, Œìùëí, ùëí))
Œì ‚ä¢ùõº ùë£ : ùëü ùëã = fresh(G)
Œì, G ‚ä¢ùõº sample(ùë£) : ({ùëã}, {ùëã}), assumeùõº (ùëã, ùëü, G)
Œì, G ‚ä¢ùõº sample(ùë£1) : ({ùëã}, {ùëã}), G
‚Ä≤
Œì ‚ä¢ùõº ùë£2 : ùëü2
Œì, G ‚ä¢ùõº observe(ùë£1,ùë£2) : (), observeùõº (ùëã, ùëü2, valueùõº (ùëü2, G
‚Ä≤
))
Œì ‚ä¢ùõº ùë£ : ùë° ùë° ‚Üò ùëü
Œì ‚ä¢ùõº op(ùë£) : ùëü
Œì, G ‚ä¢ùõº ùëí : ùë°, G
‚Ä≤
Œì, ùëù : ùë°, G
‚Ä≤
‚ä¢ùõº ùëí
‚Ä≤
: ùë°
‚Ä≤
, G
‚Ä≤‚Ä≤
Œì, G ‚ä¢ùõº let ùëù = ùëí in ùëí
‚Ä≤
: ùë°
‚Ä≤
, G
‚Ä≤‚Ä≤
Œì ‚ä¢ùõº ùë£1 : ùë°1 Œì ‚ä¢ùõº ùë£2 : ùë°2
Œì ‚ä¢ùõº (ùë£1,ùë£2) : ùë°1 √ó ùë°2
Œì ‚ä¢ùõº ùë£ : ùëü G
‚Ä≤ = valueùõº (ùëü, G) Œì, G
‚Ä≤
‚ä¢ùõº ùëí1 : ùë°1, G1 Œì, G
‚Ä≤
‚ä¢ùõº ùëí2 : ùë°2, G2
Œì, G ‚ä¢ùõº if ùë£ then ùëí1 else ùëí2 : ùë°1 ‚äî ùë°2, G1 ‚äîùõº G2
‚ä¢ùëù ùëù : ùë° Œì, ùëù : ùë° ‚ä¢ùõº ùëí : ùë°
‚Ä≤
Œì ‚ä¢ùõº fun ùëù -> ùëí : ùë° ‚Üí ùë°
‚Ä≤
Œì ‚ä¢ùõº ùëì : ùë° ‚Üí ùë°
‚Ä≤
Œì ‚ä¢ùõº ùë£ : ùë°
Œì ‚ä¢ùõº ùëì (ùë£) : ùë°
‚Ä≤
Œì ‚ä¢ùõº ùëö : (ùë°, ùë†)
Œì ‚ä¢ùõº init(ùëö) : stream(ùë°, ùë†)
Œì ‚ä¢ùëöùëê ùëö bounded Œì ‚ä¢ùë¢ùëù ùëö bounded
Œì ‚ä¢ùõº infer(ùëö) : bounded
Œì ‚ä¢ùõº ùë• : stream(ùë°, stepfn(ùëùstate, ùëùin, Œìùëí, ùëí))
Œì ‚ä¢ùõº ùë£ : ùë°in Œìùëí, ùëùstate : ùë°, ùëùin : ùë°in, G ‚ä¢ùõº ùëí : ùë°
‚Ä≤ √ó ùë°out, G
‚Ä≤
Œì, G ‚ä¢ùõº unfold(ùë•,ùë£) : ùë°out √ó stream(ùë°
‚Ä≤
, stepfn(ùëùstate, ùëùin, Œìùëí, ùëí)), G
‚Ä≤
Œì ‚ä¢ùõº ùë• : bounded Œì ‚ä¢ùõº ùë£ : ùë° ùë° ‚Üò (‚àÖ, ‚àÖ)
Œì ‚ä¢ùõº unfold(ùë•,ùë£) : (‚àÖ, ‚àÖ) √ó bounded
ùúáùêπ programs consist of a series of value, function, and stream function declarations. Thus, we also
define a top-level judgment Œì ‚ä¢ùõº ùëë :: program that states that a ùúáùêπ program ùëë contains declarations
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:35
that are all well-formed. This judgment is defined as follows:
Œì ‚ä¢ùõº ùúñ :: program
Œì, G ‚ä¢ùõº ùëí : ùë°, G
‚Ä≤
Œì, ùëù : ùë° ‚ä¢ùõº ùëë :: program
Œì ‚ä¢ùõº val ùëù = ùëí;ùëë :: program
Œì, G ‚ä¢ùõº fun ùëù -> ùëí : ùë°, G
‚Ä≤
Œì, ùëì : ùë° ‚ä¢ùõº ùëë :: program
Œì ‚ä¢ùõº val ùëì = fun ùëù -> ùëí;ùëë :: program
Œì ‚ä¢ùõº stream { init = ùëí
‚Ä≤
; step(ùëùstate,ùëùin) = ùëí } : (ùë°init, stepfn(ùëùstate, ùëùin, Œì, ùëí))
Œì,ùëö : (ùë°init, stepfn(ùëùstate, ùëùin, Œì, ùëí)) ‚ä¢ùõº ùëë :: program
Œì ‚ä¢ùõº val ùëö = stream { init = ùëí
‚Ä≤
; step(ùëùstate,ùëùin) = ùëí };ùëë :: program
where if ùëë is empty (i.e. all streams including main are valid) the judgment holds trivially.
E SOUNDNESS
E.1 Executions
During the execution of a program, the only constructs that can dynamically allocate memory
are sample and observe which add a new node to the delayed sampling graph using the assume
operation. These two probabilistic constructs can only be used in a model, i.e., the argument of the
infer operator. We thus focus on the memory footprint of infer‚Äôs transition function.
The execution of the transition function infer of infer comprises three steps (see Section 4.1):
(1) draw a set of particles, i.e., pairs (state, graph), (2) execute the model for each particle, (3) extract
the distributions of state and outputs. The only operation that can dynamically allocate memory is
the second one, where the delayed sampling graph can be altered.
At iteration ùëõ, for each particle, the current pair (state, graph) is obtained from a succession of
application of the model transition function from the initial state ùë†0 and an empty graph ùëî0 = ‚àÖ
(step (1) in the definition of infer can only drop some execution paths). We call this sequence
(ùë†0, ùëî0), (ùë†1, ùëî1), . . . an execution of the model. The following properties states that if only boundedmemory execution are possible, then the infer function executes in bounded-memory.
Lemma E.1 (Execution Sufficiency). For all stream functions ùëö and environments ùõæ, let
stream { init = ùëíinit ; step(ùëùstate,ùëùinput) = ùëí }ùõæ
‚Ä≤ = ‚ü¶ùëö‚üßùõæ and let ùë†ùëö = ‚ü¶ùëíinit‚üßùõæ
‚Ä≤ and let ùëìùëö =
ùúÜ(ùë†, ùë£). {[ùëí]}ùõæ+ [ùë†/ùëùstate,ùë£/ùëùinput ] and let ùë†ùëñ
, ùëìùëñ = ‚ü¶infer(ùëö)‚üßùõæ . We say that ùëìùëñ produces a sequence
of distributions (ùúáùëõ)ùëõ‚ààN given an input sequence (ùëñùëõ)ùëõ‚ààN if ùëìùëñ(ùúáùëõ,ùëñùëõ) = (ùúîùëõ, ùúáùëõ+1) for some sequence of output distributions (ùúîùëõ)ùëõ‚ààN. Similarly, we say ùëìùëö produces the execution (ùëîùëõ, ùë†ùëõ)ùëõ‚ààN if
ùëìùëö (ùë†ùëõ,ùëñùëõ) (ùëîùëõ, 1) = ( (ùëúùëõ, ùë†ùëõ+1),ùë§ùëõ, ùëîùëõ+1) for some sequences of outputs (ùëúùëõ)ùëõ‚ààN and weights (ùë§ùëõ)ùëõ‚ààN.
The lemma states that for all input sequences (ùëñùëõ)ùëõ‚ààN, if ùëìùëñ produces the sequence (ùúáùëõ)ùëõ‚ààN, then for
all ùëõ and (ùëîùëõ, ùë†ùëõ) ‚àà support(ùúáùëõ), there exists an execution (ùëî
‚Ä≤
ùëõ
, ùë†‚Ä≤
ùëõ
)ùëõ‚ààN such that ùëî
‚Ä≤
ùëõ = ùëîùëõ, ùë†
‚Ä≤
ùëõ = ùë†ùëõ, and
ùëìùëö produces (ùëî
‚Ä≤
ùëõ
, ùë†‚Ä≤
ùëõ
)ùëõ‚ààN.
Proof. Proceed by induction on ùëõ. If ùëõ = 0, the distribution ùúá0 is obtained by the execution
of ùëìùëö by each particle on the initial state and the empty graph. So the support of ùúá0 is obtained
by the execution of ùëìùëö. If ùëõ > 0, by definition of infer, each pair (ùëîùëõ, ùë†ùëõ) from the support of the
distribution ùúáùëõ is obtained by the application of ùëìùëö on (ùëîùëõ‚àí1, ùë†ùëõ‚àí1) drawn from the distribution
ùúáùëõ‚àí1. By application of the induction hypothesis, (ùëîùëñ
, ùë†ùëñ)0‚â§ùëñ<ùëõ is an execution produced by ùëìùëö, and
therefore (ùëîùëñ
, ùë†ùëñ)0‚â§ùëñ ‚â§ùëõ is also produced by ùëìùëö. ‚ñ°
Corollary E.2. If all executions of ùëìùëö are bounded-memory, then for any input sequence (ùëñùëõ)ùëõ‚ààN
and any execution (ùëîùëõ, ùë†ùëõ)ùëõ‚ààN such that for all ùëõ, every (ùëîùëõ, ùë†ùëõ) ‚àà support(ùúáùëõ), (ùëîùëõ, ùë†ùëõ)ùëõ‚ààN has bounded
memory when ùúáùëõ is produced by ùëìùëñ
.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:36 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
E.2 Type Soundness
In this section, we show that the type system is sound. We first define the ‚ä® relations referenced in
Section 6.5. We then prove the soundness theorems stated in Section 6.5.
Variable Mappings. Both delayed sampling and the type system use a set of fresh variable names
to label random variables. Because the type system and the delayed sampling execution may each
use a different name for conceptually the same random variable, we define an association that maps
between these namespaces. We use the notation ‚Ñì to refer to a function that maps a delayed sampling
variable to a type system variable, and ‚Ñì‚àó to the extension of ‚Ñì to sets: ‚Ñì‚àó (ùëãÀÜ) = {‚Ñì(ùëã) | ùëã ‚àà ùëãÀÜ }.
E.2.1 Entailment. Here we establish what it means for a value to entail a type. A value entails a
type if the type accurately captures the random variables the value could refer to, as well as the
shape of the value (i.e. whether the value is a scalar, a pair, or a stream function). Because step
function types include type contexts, we also establish what it means for an environment to entail
a type context.
A stream value entails bounded if it produces a sequence of states in which every delayed
sampling graph is bounded. We formalize this as follows. Given a sequence of inputs (ùëñùëõ)ùëõ‚ààN and
an initial state ùë†0, we say a stream function ùëì produces the sequence of state (ùë†ùëõ)ùëõ‚ààN on (ùë†0,ùëñ), if
ùëì (ùëñùëõ, ùë†ùëõ) = (ùëúùëõ, ùë†ùëõ+1) for some output sequence (ùëúùëõ)ùëõ‚ààN. We say ùë†, is bounded if every sequence
delayed sampling graphs contained in ùë† is low-level bounded-memory.
Definition E.3 (Type Entailment). A value ùë£ entails a type ùë°, written ùë£ ‚ä®
‚Ñì
ùë°, under the following
circumstances:
ùëê ‚ä®
‚Ñì
(‚àÖ, ub)
ùëã ‚ä®
‚Ñì
(lb, ub) ‚áê‚áí lb ‚äÜ {‚Ñì(ùëã)} ‚äÜ ub
app(op, ùë£) ‚ä®
‚Ñì
(lb, ub) ‚áê‚áí lb ‚äÜ ‚Ñì‚àó (frv(ùë£)) ‚äÜ ub
(ùë£1,ùë£2) ‚ä®
‚Ñì
ùë°1 √ó ùë°2 ‚áê‚áí ùë£1 ‚ä®
‚Ñì
ùë°1 and ùë£2 ‚ä®
‚Ñì
ùë°2
stream { init = ùëíinit ; step(ùëùin,ùëùstate) = ùëístate }ùõæùëí
‚ä®
‚Ñì
(ùë°init, stepfn(ùëùin, ùëùstate, Œìùëí, ùëístate)) ‚áê‚áí ùëíinit ‚ä®
‚Ñì
ùë°init ‚àßùõæùëí ‚ä®
‚Ñì
Œìùëí
(ùë†0, ùëì ) ‚ä®
‚Ñì
stream(ùë°init, ùëÜ) ‚áê‚áí ùë†0 ‚ä®
‚Ñì
ùë°init and ùëì ‚ä®
‚Ñì
ùëÜ
ùëì ‚ä®
‚Ñì
stepfn(ùëùin, ùëùstate, Œìùëí, ùëí) ‚áê‚áí ‚àÉùõæ . ùõæ ‚ä®
‚Ñì
Œìùëí and ùëì = ùúÜ(ùë†, ùë£). {[ùëí]}ùõæ+ [ùë†/ùëùstate,ùë£/ùëùin ]
ùõæ ‚ä®
‚Ñì
Œì ‚áê‚áí ‚àÄùë• : ùë° ‚àà Œì. ùõæ (ùë•) = ùë£ s.t. ùë£ ‚ä®
‚Ñì
ùë°
(ùë†0, ùëì ) ‚ä® bounded ‚áê‚áí ‚àÄi. ùëì produces ùë† on (ùë†0,ùëñ)
‚áí ùë† is bounded
We further define a version of type entailment that only applies to a restricted set of variables.
Definition E.4 (Restricted Type Entailment). A value ùë£ entails a type ùë° ‚Äì restricted to the variable
set ùëãÀÜ, written ùë£ ‚ä®
‚Ñì
ùëãÀÜ
ùë°, under the following circumstances:
ùëê ‚ä®
‚Ñì
ùëãÀÜ
(‚àÖ, ub)
ùëã ‚ä®
‚Ñì
ùëãÀÜ
(lb, ub) ‚áê‚áí ùëã ‚àà ùëãÀÜ ‚áí lb ‚äÜ {‚Ñì(ùëã)} ‚äÜ ub
app(op, ùë£) ‚ä®
‚Ñì
ùëãÀÜ
(lb, ub) ‚áê‚áí ùëã ‚àà ùëãÀÜ ‚áí lb ‚äÜ ‚Ñì‚àó (frv(ùë£)) ‚äÜ ub
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:37
The rules for any other values are similar to those in Definition E.3, but pass the set ùëãÀÜ through
unchanged for recursive definitions.
The fold operation ‚Üò is designed to generate a scalar type that encapsulates the free variables
of a value while disregarding its shape.
Lemma E.5 (Fold Entailment). If ùë£ ‚ä®
‚Ñì
ùë° and ùë° ‚Üò (lb, ub), then lb ‚äÜ ‚Ñì‚àó (frv(ùë£)) ‚äÜ ub.
A traced graph entails an ùëö-consumed abstract graph if the abstract graph soundly approximates
the variables that are not used.
Definition E.6 (ùëö-consumed Graph Entailment). A traced graph (ùëî, ùúè) entails an ùëö-consumed
abstract graph G, written (ùëî, ùúè) ‚ä®
‚Ñì
mc G, if for every variable ùëã not in G.in \ G.con and for every ùëã0
such that ‚Ñì(ùëã0) = ùëã, ùëã0 is used in ùúè.
A traced graph entails an unseparated-path abstract graph if the path function soundly approximates the unseparated paths in the traced graph and the separator set soundly approximates the
set of variables that are observed or valued.
Definition E.7 (Unseparated Path Graph Entailment). A graph (ùëî, ùúè) entails an unseparated-path
abstract graph G, written (ùëî, ùúè) ‚ä®
‚Ñì
up G if for every ùëã1, ùëã2 that are referenced in ùúè, G.ùëù(‚Ñì(ùëã1), ‚Ñì(ùëã2))
is at least the length of the unseparated path between ùëã1 and ùëã2 in ùúè, and, for all ùëã referenced in ùúè,
G.ùë†ùëíùëù(‚Ñì(ùëã)) is only true if ùëã is a separator in ùúè.
Entailment from Section 6.5. Here, we define the entailment relations that are referenced in
Section 6.5. These definitions are defined in terms of the relevant definitions in this section with
the variable map ‚Ñì existentially quantified:
ùë£ ‚ä®ùõº ùë° ‚áê‚áí ‚àÉ‚Ñì. ùë£ ‚ä®
‚Ñì
ùë°
ùõæ ‚ä®ùõº Œì ‚áê‚áí ‚àÉ‚Ñì. ùõæ ‚ä®
‚Ñì
Œì
ùë£, (ùëî, ùúè) ‚ä®ùõº ùë°, G ‚áê‚áí ‚àÉ‚Ñì. ùë£ ‚ä®
‚Ñì
ùë° ‚àß (ùëî, ùúè) ‚ä®
‚Ñì
ùõº G
ùõæ, (ùëî, ùúè) ‚ä®ùõº Œì, G ‚áê‚áí ‚àÉ‚Ñì. ùõæ ‚ä®
‚Ñì
Œì ‚àß (ùëî, ùúè) ‚ä®
‚Ñì
ùõº G
We further extend these definitions to incorporate a restricted variable set ùëãÀÜ.
ùë£ ‚ä®ùõº,ùëãÀÜ ùë° ‚áê‚áí ‚àÉ‚Ñì. ùë£ ‚ä®
‚Ñì
ùëãÀÜ
ùë°
ùõæ ‚ä®ùõº,ùëãÀÜ Œì ‚áê‚áí ‚àÉ‚Ñì. ùõæ ‚ä®
‚Ñì
ùëãÀÜ
Œì
ùë£, (ùëî, ùúè) ‚ä®ùõº,ùëãÀÜ ùë°, G ‚áê‚áí ‚àÉ‚Ñì. ùë£ ‚ä®
‚Ñì
ùëãÀÜ
ùë° ‚àß (ùëî, ùúè) ‚ä®
‚Ñì
ùõº G
ùõæ, (ùëî, ùúè) ‚ä®ùõº,ùëãÀÜ Œì, G ‚áê‚áí ‚àÉ‚Ñì. ùõæ ‚ä®
‚Ñì
ùëãÀÜ
Œì ‚àß (ùëî, ùúè) ‚ä®
‚Ñì
ùõº G
E.2.2 Soundness Theorems. Anùëö-consumed type judgment is sound if it abstracts theùëö-consumed
property of the semantics according to the entailment relations.
Special Case of Theorem 6.1 (ùëö-consumed Type Soundness). If ùõæ, (ùëî, ùúè) ‚ä®mc Œì, G and Œì, G ‚ä¢mc
ùëí : ùë°, G
‚Ä≤ and {[ùëí]}ùõæ
(ùëî, ùúè),ùë§ = ùë£, (ùëî
‚Ä≤
, ùúè ‚Ä≤
),ùë§‚Ä≤
, then ùë£, (ùëî
‚Ä≤
, ùúè ‚Ä≤
) ‚ä®mc ùë°, G
‚Ä≤
Proof. By structural induction on derivations of ‚ä¢mc. ‚ñ°
Proving the soundness of the ùëö-consumed judgment producing the bounded type requires
strengthening this theorem to work with partial traces, meaning the abstract graph applies only to
the tail end of the trace rather than the whole trace. Using the notation ùúè1 ‚äï ùúè2 to mean the trace ùúè1
appended with ùúè2, we formalize this as follows:
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:38 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Lemma E.8. ùëö-consumed Soundness on Partial Traces If ùõæ, (ùëî, ùúè2) ‚ä®mc Œì, G and Œì, G ‚ä¢mc ùëí : ùë°, G
‚Ä≤
and {[ùëí]}ùõæ
(ùëî, ùúè1 ‚äï ùúè2),ùë§ = ùë£, (ùëî
‚Ä≤
, ùúè ‚Ä≤
),ùë§‚Ä≤
, then ùúè
‚Ä≤ = ùúè1 ‚äï ùúè
‚Ä≤
2
and ùë£, (ùëî
‚Ä≤
, ùúè ‚Ä≤
2
) ‚ä®mc ùë°, G
‚Ä≤
Proof. By structural induction on derivations of ‚ä¢mc. The individual steps are the same as the
previous theorem, except that they also use the associativity of ‚äï. ‚ñ°
An unseparated-path type judgment is sound if it abstracts the unseparated path property of the
semantics according to the entailment relations.
Special Case of Theorem 6.1 (Unseparated Path Type Soundness). If ùõæ, (ùëî, ùúè) ‚ä®up Œì, G and
Œì, G ‚ä¢up ùëí : ùë°, G
‚Ä≤ and {[ùëí]}ùõæ
(ùëî, ùúè),ùë§ = ùë£, (ùëî
‚Ä≤
, ùúè ‚Ä≤
),ùë§‚Ä≤
, then ùë£, (ùëî
‚Ä≤
, ùúè ‚Ä≤
) ‚ä®up ùë°, G
‚Ä≤
We also strengthen this theorem to aid in proving the soundness of the bounded judgment.
Lemma E.9. Unseparated Paths Soundness on Partial Traces If ùõæ, (ùëî, ùúè1 ‚äï ùúè3) ‚ä®up,frv (ùúè1‚äïùúè3) Œì, G
and Œì, G ‚ä¢up ùëí : ùë°, G
‚Ä≤ and {[ùëí]}ùõæ
(ùëî, ùúè1 ‚äï ùúè2 ‚äï ùúè3),ùë§ = ùë£, (ùëî
‚Ä≤
, ùúè ‚Ä≤
),ùë§‚Ä≤
, then ùúè
‚Ä≤ = ùúè1 ‚äï ùúè2 ‚äï ùúè
‚Ä≤
3
and
ùë£, (ùëî
‚Ä≤
, ùúè1 ‚äï ùúè
‚Ä≤
3
) ‚ä®up,frv (ùúè1‚äïùúè
‚Ä≤
3
)
ùë°, G
‚Ä≤
Proof. By structural induction on derivations of ‚ä¢up. The individual steps are the same as the
previous theorem, except that they also use the associativity of ‚äï. ‚ñ°
Theorem E.10 (Analysis Soundness). If ùõæ ‚ä®ùõº Œì and Œì ‚ä¢ùõº ùëö : bounded, then ‚ü¶ùëö‚üßùõæ
‚ä®ùõº bounded.
Proof. We first show that any execution of a stream function ùëö satisfying Œì ‚ä¢mc ùëö bounded
satisfies the high-level ùëö-consumed semantic property. We then show that any execution of a
stream function ùëö satisfying Œì ‚ä¢up ùëö bounded satisfies the high-level unseparated paths semantic
property. Then, by Theorem 5.11 and Lemma E.1, if ùëö satisfies both these properties, then calling
infer on ùëö must be bounded.
ùëö-consumed. Here, we show that any execution of a stream functionùëö satisfying Œì ‚ä¢mc ùëö bounded
satisfies the ùëö-consumed semantic property. We show this using the definition of ‚ä¢mc. Let (ùëîùëñ
, ùúèùëñ)
be the ùëñth step of the execution. By Lemma E.8, G captures all variables introduced at time ùëñ. Also
by Lemma E.8, G
‚Ä≤
captures the variables that are guaranteed to be consumed between ùëñ and ùëñ + ùëõ.
Thus, any variable introduced at time step ùëñ must either be consumed within ùëõ steps (where ùëõ is
a static bound) or must is not stored in the program state. If it is consumed, the variable will be
ùëö-consumed at all future time steps where ùëö is at most ùëõ times a constant bound based on the
number of sample statements in the stream function. If it is not stored in the program state, it can
never be used and is therefore always 0-consumed.
Unseparated Paths. We proceed by contradiction. Assume that ùë†ùëñ
, (ùëîùëñ
, ùúèùëñ)ùëñ ‚ààN is an execution that
violates the unseparated paths semantic property. At some time step ùëó, the execution must a) add
a variable to the delayed sampling graph in such a way that it increases the unseparated path
starting from some variable in the graph, and b) store the variable starting the increased path in ùë†ùëó+1.
Otherwise, the execution would easily satisfy the property. According to Lemma E.9, we must have
that after each iteration the abstract graph also has a variable with starting an increased path and
that some reference ùëü
‚àó
contained in the type ùë°
‚Ä≤
references this variable. Letting ùëÖÀÜ be the set of all
possible references ùëü
‚àó
, by the pidgeonhole principle, after ùëò ‚â• size(ùë°) = size(ùë°
‚Ä≤
) ‚â• |ùëÖÀÜ| iterations,2
the longest path in the abstract graph starting from a variable referenced by an element of ùëÖÀÜ must
have increased by at least 1. Similarly, after ùëõ ‚â• path(ùë°, G) instances of this pattern, the longest
path in the abstract graph starting from a variable referenced in ùëÖÀÜ must have increased by at least
path(G, ùë°) and thus be the longest such graph starting from a state variable. This contradicts the
termination condition that path(ùë°, G) = path(ùë°
‚Ä≤‚Ä≤
, G
‚Ä≤
). ‚ñ°
2
the equality of size(ùë°) and size(ùë°
‚Ä≤
) is enforced by the type rules in Figure 18 of Appendix B
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:39
F BENCHMARKS
Each of these benchmarks are followed by a main stream that serves as the entry point of the
program:
val main = stream {
init = infer f;
step (f, args) = unfold (f, args)
}
F.1 Kalman
val f = stream {
init = 0.;
step (pre_x, obs) =
let x = sample (gaussian (pre_x, 1.0)) in
let () = observe (gaussian (x, 1.0), obs) in
(x, x)
}
F.2 Kalman Hold-First
val kalman = stream {
init = (true, 0., 0.);
step ((first, i, pre_x), obs) =
let (i, pre_x) =
if first then (let i = sample (gaussian(0., 1.)) in (i, i))
else (i, pre_x) in
let x = sample (gaussian (pre_x, 1.)) in
let () = observe (gaussian (x, 1.), obs) in
(x, (false, i, x))
}
F.3 Gaussian Random Walk
val f = stream {
init = (true, 0.);
step ((first, x), ()) =
let x = if first then sample (gaussian (0., 1.)) else sample (gaussian (x, 1.)) in
(x, (false, x))
}
F.4 Coin
val f = stream {
init = (true, 0.);
step ((first, xt), yobs) =
let xt = if first then sample (beta (1., 1.)) else xt in
let () = observe (bernoulli (xt), yobs) in
(xt, (false, xt))
}
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:40 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
F.5 Outlier
val f = stream {
init = (true, 0., 0.);
step ((first, xt, outlier_prob), yobs) =
let (xt, outlier_prob) =
if first then
(sample (gaussian (0., 100.)), sample (beta (100., 1000.)))
else (sample (gaussian (xt, 1.)), outlier_prob) in
let is_outlier = sample (bernoulli (outlier_prob)) in
let () =
if is_outlier then (observe (gaussian (0., 100.), yobs))
else (observe (gaussian (xt, 1.), yobs)) in
(xt, (false, xt, outlier_prob))
}
F.6 MTT
val f = stream {
init = (true, List.nil);
step ((first, t), (inp, cmd)) =
let last_t = t in
let t_survived =
List.filter (fun (_, _) -> eval (sample (bernoulli (0.5))), last_t) in
let n_new = sample (poisson (1.0)) in
let t_new = List.init (n_new, fun _ -> (0, sample (bernoulli (0.5)))) in
let t_tot = List.append (t_survived, t_new) in
let t = List.map (fun (tr_num, tr) -> (tr_num, sample (bernoulli (tr))), t_tot) in
let obs = List.map (fun (_, tr) -> bernoulli (tr), t) in
let n_clutter = sub (List.length (inp), List.length (obs)) in
let () = observe (poisson (0.5), n_clutter) in
let clutter = List.init (n_clutter, fun _ -> bernoulli (tr)) in
let obs_shuffled = sample (shuffle (List.append (obs, clutter))) in
let () =
if (not (lt (n_clutter, 0))) then
List.iter2 (fun (var, value) ->
observe (gaussian (0.5, var), value), obs_shuffled, inp)
else () in
(t, (false, t))
}
F.7 SLAM
val f = stream {
init = (true, 0., Array.empty);
step ((first, x, map), (obs, cmd)) =
let map =
if first then Array.init (100, fun _ -> sample (bernoulli (0.5))) else map in
let wheel_slip = sample (bernoulli (0.5)) in
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:41
let x = if first then 0. else if wheel_slip then x else plus (x, cmd) in
let o = Array.get (map, x) in
let _ = observe (bernoulli (ite (o, 0.9, 0.1)), obs) in
((x, map), (false, x, map))
}
G PRECISION LIMITATIONS
The precision of the analysis is limited by path and complex sensitivity, two common challenges
for static analysis. The analysis can be overly conservative when facing conditional branches, for
example in the following snippet:
let x = sample bernoulli(0.5) in
let y = sample gaussian (0., 1.) in
let () = if x then observe (gaussian (y, 1.), 1.) else () in
let () = if x then () else observe (gaussian (y, 1.), -1.) in
y
According to the analysis, y is not consumed because each branch is separately and conservatively
judged to not consume y, even though there is no path where y is unobserved. A more sophisticated
analysis that reasons about actual values, not just affected variables, would be more precise here.
Similarly, the analysis can be imprecise in the presence of complex data such as tuples. Consider
the following snippet:
let x = sample (gaussian(0., 1.)) in
let y = sample (gaussian(0., 1.)) in
let (a, b) =
if (sample (bernoulli(0.5))) then
(gaussian (x, 1.), gaussian (y, 1.))
else (gaussian (y, 1.), gaussian (x, 1.)) in
let () = observe (a, 1.) in
let () = observe (b, 2.) in
(x, y)
Like the previous example, x, y are not considered consumed even though there is no path that
does not observe both. The analysis can determine that both a and b may reference x and y but
neither alone must do so. Knowledge about a and b taken as a pair is lost when they are stored into
the tuple. In this case, some kind of alias or shape analysis might recover the relationship between
the fields of a tuple.
Without executing for multiple iterations, the ùëö-consumed analysis would be occasionally too
conservative due to requiring that all variables be used before the end of the current iteration of
the step function. Consider:
stream {
init = 0.;
step (x_prev, obs) =
let _ = observe (gaussian (x_prev, 1.), obs) in
let x = sample (gaussian (x_prev, 1.)) in
(x, x)
}
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:42 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
In this example, every sample is eventually consumed but only on the subsequent iteration of
the step function. If the ùëö-consumed analysis only considered one iteration, it would reject this
example. Allow introduced variables to be consumed over multiple iterations as we do allows this
example to pass the analysis.
Most examples do not require a significant number of iterations for the unseparated paths
analysis to converge. However, the analysis may fail to detect convergence in programs with many
variables if the iteration bound parameter is too low, as in the following program which requires
four iterations:
stream {
init = (0., 0., 0., 0.);
step ((x_p, x_pp, x_ppp, x_pppp), obs) =
let x = sample (gaussian (x_p, 1.)) in
let _ = observe (gaussian (x, 1.), 1.0) in
(x_pppp, (x, x_p, x_pp, x_ppp))
}
In this program, the longest unseparated path increases over four iterations, after which variables
start being dropped from the state and the maximum length converges. We suggest that the
parameter should be set to be comfortably larger than the number of variables or statements in the
program to avoid this issue. Since each iteration is fast to run, it should not cause performance
degradation.
Finally, the analysis could incorporate higher-order functions, though they would be hard to
analyze statically, and the storage of chains of closures built over many iterations could itself violate
a bound on memory usage.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
