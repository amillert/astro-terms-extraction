115
Statically Bounded-Memory Delayed Sampling for
Probabilistic Streams
ERIC ATKINSON, MIT, USA
GUILLAUME BAUDART, INRIA, Ã‰cole normale supÃ©rieure â€“ PSL University, France
LOUIS MANDEL, MIT-IBM Watson AI Lab, IBM Research, USA
CHARLES YUAN, MIT, USA
MICHAEL CARBIN, MIT, USA
Probabilistic programming languages aid developers performing Bayesian inference. These languages provide
programming constructs and tools for probabilistic modeling and automated inference. Prior work introduced
a probabilistic programming language, ProbZelus, to extend probabilistic programming functionality to
unbounded streams of data. This work demonstrated that the delayed sampling inference algorithm could be
extended to work in a streaming context. ProbZelus showed that while delayed sampling could be effectively
deployed on some programs, depending on the probabilistic model under consideration, delayed sampling is
not guaranteed to use a bounded amount of memory over the course of the execution of the program.
In this paper, we the present conditions on a probabilistic programâ€™s execution under which delayed sampling
will execute in bounded memory. The two conditions are dataflow properties of the core operations of delayed
sampling: the ğ‘š-consumed property and the unseparated paths property. A program executes in bounded
memory under delayed sampling if, and only if, it satisfies the ğ‘š-consumed and unseparated paths properties.
We propose a static analysis that abstracts over these properties to soundly ensure that any program that
passes the analysis satisfies these properties, and thus executes in bounded memory under delayed sampling.
CCS Concepts: â€¢ Theory of computation â†’ Program analysis; Streaming models; â€¢ Software and its
engineering â†’ Data flow languages.
Additional Key Words and Phrases: Probabilistic programming, reactive programming, streaming inference,
semantics, program analysis
ACM Reference Format:
Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin. 2021. Statically BoundedMemory Delayed Sampling for Probabilistic Streams. Proc. ACM Program. Lang. 5, OOPSLA, Article 115
(October 2021), 42 pages. https://doi.org/10.1145/3485492
1 INTRODUCTION
Probabilistic programming languages aid developers performing Bayesian inference [Atkinson et al.
2018; Bingham et al. 2019; Cusumano-Towner et al. 2019; Ge et al. 2018; Gelman et al. 2015; Goodman
et al. 2008; Goodman and StuhlmÃ¼ller 2014; Gordon et al. 2014; Huang et al. 2017; Mansingkha et al.
2018; Milch et al. 2007; Narayanan et al. 2016; Nori et al. 2015; Pfeffer 2009; Tran et al. 2017]. These
languages provide programming constructs and tools for probabilistic modeling and automated
inference. Researchers have developed probabilistic programming languages for several domains,
Authorsâ€™ addresses: Eric Atkinson, MIT, USA; Guillaume Baudart, INRIA, Ã‰cole normale supÃ©rieure â€“ PSL University,
France; Louis Mandel, MIT-IBM Watson AI Lab, IBM Research, USA; Charles Yuan, MIT, USA; Michael Carbin, MIT, USA.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,
contact the owner/author(s).
Â© 2021 Copyright held by the owner/author(s).
2475-1421/2021/10-ART115
https://doi.org/10.1145/3485492
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
arXiv:2109.12473v1 [cs.PL] 26 Sep 2021
115:2 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
including data science [Gelman et al. 2015], machine learning [Bingham et al. 2019; Tran et al.
2017], scientific simulation [Baydin et al. 2019], and real-time control [Baudart et al. 2020].
Probabilistic Programming with Streams. In this paper, we consider programs that accept inputs
and compute outputs at discrete time steps, with the outputs of each step flowing into the environment to affect future inputs to the program. Mathematically, one can model these programs as
computations that operate on and produce infinite streams. Computing with streams is a common
computational model for applications in real-time control, such as robotics and avionics [ColaÃ§o
et al. 2017]. For example, control for an airplane fly-by-wire system can be implemented as a
program transforming a stream of altitude measurements into a stream of commands to the engine.
Baudart et al. [2020] introduced a probabilistic programming language, ProbZelus, to enable probabilistic programming in this domain of computations on streams. A key innovation of ProbZelus
was to demonstrate that delayed sampling [Murray et al. 2018] could be extended to work with
streams to provide high-quality inference procedures. Delayed sampling is an inference algorithm
that combines both exact and approximate inference; it takes advantage of exact inference when
efficient known closed-formed solutions exist and falls back on sampling-based, approximate inference when required. Specifically, delayed sampling combines Bayesian networks â€“ graphs that
encode exact distributions of probabilistic models â€“ with particle filtering [Del Moral et al. 2006] â€“
an approximate inference algorithm.
The challenge in adapting delayed sampling to computations on streams is that such computations
run for indefinite periods of time and are often subject to stringent limits on resources, such as
memory. Baudart et al. [2020] showed that, in many cases, only a finite number of nodes in
delayed samplingâ€™s graph data structures were reachable at any given time, and the rest could not
influence the computation in the future and could be removed from memory. However, this behavior
depends on the probabilistic model under consideration; delayed sampling is not guaranteed to
maintain a bounded amount of memory for all programs. The result is then that though probabilistic
programming languages are designed to hide the complexities of developing probabilistic inference
algorithms, certain combinations of a model and the inference algorithm will result in undesirable
behaviors that the developer did not anticipate. Moreover, the developer has no means to reason
about these behaviors except by inspecting the implementation of the inference algorithm.
Bounded-Memory Delayed Sampling. In this paper, we formalize semantic conditions under which
applying delayed sampling to probabilistic programs with streams will execute in bounded memory.
The two conditions are dataflow properties of the core operations of delayed sampling: assume,
observe, and value, which respectively add a new random variable to the delayed sampling graph,
observe a random variable, and evaluate a random variable to produce a sampled value. The ğ‘š-
consumed property states that all variables introduced with assume are eventually consumed by an
observe or a value, or are passed to other assumes resulting in new variables that are themselves
(ğ‘šâˆ’1)-consumed. An unseparated path is a sequence of random variables, each passed as parameter
to the assume operation of the next, where no variable is passed to an observe or value operation.
The unseparated paths property states that no variable maintained in the program state starts an
unseparated longer than some fixed bound ğ‘›. A program executes in bounded memory under
delayed sampling if, and only if, it satisfies the ğ‘š-consumed and unseparated paths properties.
Static Analysis. We propose a static analysis that checks the ğ‘š-consumed and unseparated paths
properties to soundly ensure that any program that passes the analysis satisfies these properties,
and thus executes in bounded memory under delayed sampling.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:3
Contributions. In this paper, we present the following contributions:
â€¢ We introduce and formalize the ğ‘š-consumed and unseparated paths properties, and show these
are necessary and sufficient for a program to have bounded-memory execution.
â€¢ We present a static analysis to check these properties, and prove that the analysis is sound.
â€¢ We implement the analysis and evaluate it against several probabilistic inference benchmarks.
Our results show that for eight of nine benchmarks, the analysis determines whether the semantic
properties necessary for bounded-memory execution are satisfied, and we identify the precision
limitation of conservative static analysis on the remaining benchmark.
This work brings probabilistic programming to control settings with the new benefit of static
guarantees on the systemâ€™s resource consumption. To the best of our knowledge, our work is
the first to develop a resource analysis for a probabilistic program in relation to its probabilistic
programming systemâ€™s underlying inference algorithm.
The remainder of the paper is structured as follows. In Section 2, we give an example program
to illustrate the concepts in the paper. In Section 3, we present the syntax and semantics of a
language for probabilistic programming with streams, adapted from the ğœ‡ğ¹ language from Baudart
et al. [2020]. In Section 4, we review background on delayed sampling, based on the contributions
from Murray et al. [2018] and Baudart et al. [2020]. In Section 5, we present the ğ‘š-consumed and
unseparated paths semantic properties. In Sections 6 and 7, we present and evaluate the static
analysis. Sections 8 and 9 summarize related work and present conclusions.
2 EXAMPLE
Figure 1 presents the example of a robot designed to navigate to a desired position target using
measurements obs from a noisy position sensor. The robot issues a command u that indicates the
acceleration to apply to change its position. The robot (1) estimates its current position with a
probabilistic model kalman and (2) uses this estimate to compute the command u with a deterministic
controller (e.g., a Linear-Quadratic Regulator [Sontag 2013], the implementation of which we
have elided for simplicity). We present the example in ğœ‡ğ¹ , a purely functional core calculus for
probabilistic programming with streams.
1 val kalman = stream {
2 init = 0.0;
3 step (pre_x, obs) =
4 let x = sample (gaussian (pre_x, 1.0)) in
5 let () = observe (gaussian (x, 1.0), obs) in
6 (x, x)
7 }
8 val robot = stream {
9 init = (0.0, init controller, infer kalman);
10 step ((c, k), (obs, target)) =
11 let x_dist, k' = unfold (k, obs) in
12 let u, c' = unfold (c, (target, mean (x_dist))) in
13 (u, (c', k'))
14 }
Fig. 1. ğœ‡ğ¹ program with main stream function robot.
The program is a set of stream function definitions that each consist of (1)
an initializer, and (2) a step function
that given the previous state and an input value produces an output value and
a new state [Mealy 1955]. The operators init and infer instantiate a stream
function by creating an internal state. A
stream function can be applied to an input stream to generate an output stream
with the operator unfold, which applies
the step function using the internal state
and the input values. Unlike init, the
step function of an instance created using infer performs probabilistic inference and thus returns at each iteration
a distribution of outputs and a distribution of states.
The main stream function, robot, has a state composed of two stream function instances: c the
deterministic controller, and k the kalman probabilistic model. The robot initializer creates these
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:4 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
two instances (L.9). The transition function of instance k performs probabilistic inference to infer
a distribution of the robotâ€™s state x_dist and an updated instance k' (L.11). Then the transition
function of instance c computes a command u to go toward the destination target using statistics
of the position distribution and an updated instance c' (L.12). The transition function of robot
returns the command u and the updated state (L.13).
2.1 Probabilistic Model
The stream function kalman specifies a hidden Markov model [Baum and Petrie 1966], a common
probabilistic model for tracking applications in which the goal is to estimate the trajectory of an
object given noisy measurements of the objectâ€™s position.
The stream functionâ€™s state consists of a latent random variable, pre_x, that denotes the position
of the robot at the previous iteration. The robotâ€™s state is latent in that the robot is unable to directly
observe its position; instead it must leverage a noisy measurement or observation of its position to
infer a probability distribution over its potential states.
Inside the definition of kalman, the program models the latent nature of x by sampling the
current position from a Gaussian distribution centered around its previous position pre_x (L.4).
The program models the observation by taking the observed sensor value as input, obs, and
supplying it as an input to the observe operator. In this example, the observe specifies that obs is
an observation from a Gaussian distribution centered around the position x. The observe operator
conditions the programâ€™s execution on the observed value (L.5) in that it adjusts the distribution
that will be inferred for x.
The sequence of diagrams in Figure 2 illustrates the evolution of a representation of the hidden
Markov model over the first four iterations of the program. Each light grey node denotes a latent
random variable for pre_x or x at a given iteration. Each dark grey node denotes an observation
at the given iteration. Each solid black arrow signifies a dependence between random variables
as in a traditional Bayesian network representation of a probabilistic graphical model [Koller and
Friedman 2009]. Of note, each observation at each iteration depends on the current position and
the robotâ€™s state at a given iteration depends only on its position at the previous iteration.
2.2 Inference with Delayed Sampling
The kalman probabilistic model is not sufficient for the robot to reason about its position. Instead, the
robot must perform inference on the model to compute a posterior distribution of x conditioned on
its observations. As mentioned, the infer operator in the robot stream function applies inference
to the probabilistic model it receives as input. In this paper, we study delayed sampling [Baudart
et al. 2020; Murray et al. 2018] as the algorithmic implementation of the infer operator.
Delayed sampling is an extension of a particle filtering algorithm that leverages symbolic execution to reason about the relationship between random values and perform exact inference if
possible. A particle filter estimates the posterior distribution from a set of particles, i.e., independent
executions of the model. For each particle, delayed sampling operates by dynamically maintaining
a graph â€” i.e., a Bayesian network â€“ that records the dependence relationships between the random
variables in the program (Figure 2). The key idea is that rather than sample a concrete value for each
random variable in the program (e.g., x), delayed sampling instead returns a reference to a node
in the graph. This node contains a closed-form representation of the distribution that the sample
operator sampled from, along with the distributionâ€™s dependence on other random variables in
the program. If a symbolic computation fails, delayed sampling can fall back to a particle filter by
drawing concrete values for the random variables.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:5
x
obs
(a) iteration 1
pre_x
âœ—
âœ—
x
obs
(b) iteration 2
âœ—
âœ—
pre_x
âœ—
âœ—
x
obs
(c) iteration 3
âœ—
âœ—
âœ—
âœ—
pre_x
âœ—
âœ—
x
obs
(d) iteration 4
Fig. 2. The evolution of the delayed sampling graph for the hidden Markov model in Figure 1 (kalman) as
implemented by Baudart et al. [2020]. Each node denotes either a value (dark gray) or a distribution (light
gray). A plain arrow denotes a dependency in the underlying Bayesian network. A dotted arrow denotes a
pointer in the implementation of the delayed sampling graph. Each label indicates the program variable that
corresponds to a node. An âœ— on a node denotes that the node is not reachable from the program state.
2.3 Bounded-Memory Delayed Sampling
A key concern when applying delayed sampling to streams, which may execute for an indefinite
number of iterations, is if the size of the delayed sampling graph is bounded from above by a fixed
constant for all iterations of the program. If not, then the delayed sampling graph may not consume
bounded memory and the program may exhaust its resources if permitted to execute indefinitely.
In general, bounding memory use is challenging because the underlying Bayesian network can
in fact be unbounded. Nevertheless, a delayed sampling implementation can maintain bounded
memory for some programs, depending on the operation of said programs. In this subsection, we
review the delayed sampling implementation presented by Baudart et al. [2020] which can execute
in bounded memory for some programs.
Bounded-Memory Example. Figure 2 shows how delayed sampling maintains bounded memory
for the program in Figure 1. For each particle, the delayed sampling implementation must keep in
memory all the nodes that are reachable from any node referenced in the program state. The dashed
lines in Figure 2 visualize the reachability relation, where the node each line points to is reachable
from the node the line points from. As the program evolves its state and changes the variables the
state contains, nodes in the delayed sampling graph may become unreachable, marked âœ—.
Figure 2a shows the delayed sampling graph after the first iteration. The graph consists of two
nodes: one introduced by sampling the variable x, and one introduced by the observation of obs.
At the end of the step, both are in the program state and reachable.
Figure 2b shows the delayed sampling graph after the second iteration. The program has added
two nodes to the graph for sampling x and observing obs. The nodes left over from the first iteration
are still in the graph, but are no longer reachable.
Figures 2c and 2d show the delayed sampling graph at iterations 3 and 4 respectively. In each
case, the most recently introduced nodes for x and obs are reachable, and the nodes from the
previous iterations are unreachable. In general, the program ensures that at any iteration, the most
recently introduced nodes are reachable, and the rest are unreachable. Because there are at most
two reachable nodes for all iterations, inference executes in bounded memory.
Unbounded-Memory Example. Figure 3 presents an example of a program that does not execute
in bounded memory. This is a modified version of kalman from Figure 1 that samples an initial
latent position i from a Gaussian distribution and keeps a reference to this random variable in the
state. Figure 4 shows how the program in Figure 3 fails to maintain bounded memory.
Figure 4a shows the delayed sampling graph after the first iteration. The graph consists of three
reachable nodes introduced by sampling the variables i and x and by the observation of obs.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:6 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
val kalman_first = stream {
init = (true, 0.0, 0.0);
step ((first, i, pre_x), obs) =
let (i, pre_x) =
if first then (let i = sample (gaussian (0.0, 1.0)) in (i, i))
else (i, pre_x) in
let x = sample (gaussian (pre_x, 1.0)) in
let () = observe (gaussian (x, 1.0), obs) in
(x, (false, i, x))
}
Fig. 3. Model with unbounded memory consumption.
i x
obs
(a) iteration 1
i
âœ—
x
obs
(b) iteration 2
i
âœ— âœ—
x
obs
(c) iteration 3
i
âœ— âœ— âœ—
x
obs
(d) iteration 4
Fig. 4. The evolution of the delayed sampling graph for the variant of a Kalman probabilistic model in Figure 3.
Nodes and edges have the same meaning as in Figure 2.
ğœ2 = x1 f nil :: y1 f x1 :: obs y1 :: x2 f x1 :: y2 f x2 :: obs y2
iteration 1 iteration 2
Fig. 5. A depiction of a trace of the program in Figure 1. The figure depicts the trace ğœ2 at the end of iteration 2.
The trace is a ::-separated list of primitive operations, where each primitive operation is a sampling operation
f or an observation operation obs. In this diagram, we use xğ‘› and yğ‘› to refer to the random variables
introduced at iteration ğ‘› by, respectively, sampling x and observing obs in Figure 1.
Figure 4b shows the delayed sampling graph after the second iteration. The program has added
two nodes to the graph for sampling x and observing obs. Since the variable i is in the program
state, the node between i and x is reachable.
Figures 4c and 4d show that in the next iterations two new nodes are introduced at each step and
one remains reachable. The primary observation to note is that the number of introduced nodes
increases at every iteration. Therefore, there is no bound on the size of the delayed sampling graph
and, hence, the program does not execute in bounded memory.
2.4 Analyzing Delayed Sampling
In this paper, we present an analysis that can show that the program in Figure 1 maintains bounded
memory while the program in Figure 3 does not. For that, we define two dataflow properties that
encode whether a program executes in bounded memory: the unseparated paths property and the
ğ‘š-consumed property. We then show how these properties can be verified using a static analysis.
Traces. We formalize the dataflow properties as properties of traces. A trace is a recording of the
important features of a program execution. In our case, a trace records all sampling and observation
operations that the program has executed, as well as the variables that were involved in these
operations. Figure 5 illustrates a trace of the execution of the program in Figure 1.
Unseparated Paths. An unseparated path in a trace is a sequence of variables {ğ‘¥ğ‘– }, where the
trace specifies that each variable ğ‘¥ğ‘– was sampled from its predecessor ğ‘¥ğ‘–âˆ’1 and no ğ‘¥ğ‘–
is observed.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:7
ğœ2 = i f nil :: x1 f i :: y1 f x1 :: obs y1 :: x2 f x1 :: y2 f x2 :: obs y2
iteration 1 iteration 2
ğ‘£2 = (false, i, x2)
Fig. 6. A depiction of a trace of the program in Figure 3. The figure depicts the trace ğœ2 and the value of
the program state ğ‘£2 at the end of iteration 2. In this diagram, we use i, xğ‘›, yğ‘›, respectively, to refer to the
random variable introduced by sampling i, the variable introduced at iteration ğ‘› by sampling x, and the
variable introduced at iteration ğ‘› by observing obs in Figure 3. We have highlighted the elements of the
unseparated path between ğ‘– and x2 in green.
The unseparated paths property states that there is a uniform bound ğ‘ so that for all iterations no
variable in the program state starts an unseparated path with more than ğ‘ variables in it.
Figure 6 illustrates the trace for the program in Figure 3. This program carries the variable i in
the program state, and because the trace specifies that x1 was sampled from i, and x2 was sampled
from x1, the sequence i, x1, x2 is an unseparated path with 3 variables. In general, at iteration ğ‘›,
the program in Figure 3 maintains that i is in the program state and starts an unseparated path
with length ğ‘› + 1. Because no bound can exist on the length of this path for an arbitrary number of
iterations, this program fails the unseparated path property.
ğ‘š-consumed. A variable is ğ‘š-consumed if it is no more than ğ‘š sampling operations away from a
variable that is consumed by an observe statement. The ğ‘š-consumed property states that there is a
uniform bound ğ‘š such every variable introduced by a sampling operation is ğ‘š-consumed for some
ğ‘š â‰¤ ğ‘š. We note that the traces in Figures 5 and 6 satisfy the ğ‘š-consumed property, because every
variable is at most 2-consumed. For all ğ‘¡, yğ‘¡
is 0-consumed because it is directly observed, and xğ‘¡
is
1-consumed because yğ‘¡
is sampled from xğ‘¡ and yğ‘¡
is 0-consumed. The variable i is 2-consumed
because x1 is sampled from i, and x1 is 1-consumed.
The Outlier benchmark presented in Section 7 is an example of a program that fails the ğ‘š-
consumed property, and thus does not execute in bounded memory. This program sometimes
observes values close to the true latent state but otherwise observes values from an outlier distribution. When the program observes a value from the outlier distribution, it fails to observe any
dependencies of the latent state, and thus cannot guarantee that the latent state is ğ‘š-consumed.
Over time, if the program performs latent state updates that remain unobserved (due to the program
always observing from the outlier distribution), the lack of this guarantee results in there being no
uniform bound ğ‘š under which the latent state could be ğ‘š-consumed.
Analysis. Our goal is ultimately to analyze whether a given program executes in bounded memory.
As we show in Section 5, a program execution maintains bounded memory if and only if it satisfies
both the unseparated path and ğ‘š-consumed properties. This reduces the problem of analyzing
the bounded-memory behavior of a program to analyzing these dataflow properties. Our analysis
utilizes an abstract delayed sampling graph, formally defined in Section 6, with the key aspects of
these properties. For ğ‘š-consumed, the abstract graph maintains a set of variables that have been
introduced but not yet consumed, and for unseparated paths, it maintains an upper bound on their
length. For example, the abstract graphs for the trace in Figure 6 are given in Figure 7.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:8 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
ğœ2 = i f nil :: x1 f i :: y1 f x1 :: obs y1 :: x2 f x1 :: y2 f x2 :: obs y2
iteration 1 iteration 2
ğ‘š-consumed i x1 y1 x2 y2
unseparated paths (i, i), 0 (i, x1), 1 (i, y1), 2 (i, x1), 1 (i, x2), 2 (i, y2), 3 (i, x2), 2
Fig. 7. A depiction of the abstract graphs of the program in Figure 3, with the same trace as Figure 6. At
each operation, we depict the ğ‘š-consumed abstract graph, a set of nodes that have been introduced but
not consumed. Because this set is empty at the end of any iteration, the program satisfies the ğ‘š-consumed
semantic property. The unseparated paths abstract graph is a mapping, for each unseparated path in the
graph, from its endpoints to its length. We depict the longest path in the mapping. After each iteration, this
longest path continues to lengthen, so the program does not satisfy the unseparated paths semantic property.
3 LANGUAGE MODEL
In this section, we present a semantics for probabilistic programs with streams using the language
ğœ‡ğ¹ . We have adapted ğœ‡ğ¹ from Baudart et al. [2020]â€™s core calculus for probabilistic programs and
extended it with syntax for explicit streams.
3.1 Syntax
The syntax of the ğœ‡ğ¹ language is defined according to the following grammar:
program ::= ğ‘‘
âˆ— ğ‘š
ğ‘‘ ::= val ğ‘ = ğ‘’ | val ğ‘“ = fun ğ‘ -> ğ‘’ | val ğ‘š = stream { init = ğ‘’ ; step(ğ‘,ğ‘) = ğ‘’ }
ğ‘’ ::= ğ‘£ | op(ğ‘£) | ğ‘“ (ğ‘£) | if ğ‘£ then ğ‘’ else ğ‘’ | let ğ‘ = ğ‘’ in ğ‘’
| init(ğ‘š) | unfold(ğ‘¥,ğ‘£) | sample(ğ‘£) | observe(ğ‘£,ğ‘£) | infer(ğ‘š)
ğ‘£ ::= ğ‘ | ğ‘¥ | (ğ‘£,ğ‘£)
ğ‘ ::= ğ‘¥ | (ğ‘,ğ‘)
A program is a set of value, function, and stream function definitions followed by the name of the
main stream function. A stream function ğ‘š is composed of an initial state (init) and a transition
function (step). Given a state and an input, the transition function returns an output and a new
state. An expression is either a value (constant, variable, or pair), the application of a primitive
operator (arithmetic operator, distribution, etc.), a function call, a conditional, or a local definition.
The expression init(ğ‘š) creates an instance of a stream function, and unfold(ğ‘¥,ğ‘£) applies the
instance ğ‘¥ of a stream function on an input and returns the next element and the updated instance.
Finally, the set of expressions comprises the probabilistic operators sample, observe, and infer.
Nested inference and higher-order functions on streams are not allowed in the language. We require
that arguments for all syntactic operators are values to simplify the presentation of the semantics.
Since new variables can always be introduced to capture the value of any expression, this choice
does not reduce the expressiveness of the language.
3.2 Semantics
The execution of a program ğ‘ = ğ‘‘
âˆ—ğ‘š comprises three steps. First, declarations ğ‘‘
âˆ—
are evaluated to
produce an environment ğ›¾ which contains the definition of the main stream function ğ‘š. Second,
an instance of the stream function ğ‘š is created.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:9
âŸ¦val ğ‘¥ = ğ‘’âŸ§ğ›¾ = ğ›¾ [ğ‘¥ â† âŸ¦ğ‘’âŸ§ğ›¾ ]
âŸ¦val ğ‘“ = fun ğ‘ -> ğ‘’âŸ§ğ›¾ = ğ›¾ [ğ‘“ â† (ğœ†ğ‘£. âŸ¦ğ‘’âŸ§ğ›¾+ [ğ‘£/ğ‘ ])]
âŸ¦val ğ‘š = stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }âŸ§ğ›¾
= ğ›¾ [ğ‘š â† stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
]
âŸ¦init(ğ‘š)âŸ§ğ›¾ = let stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
â€² = âŸ¦ğ‘šâŸ§ğ›¾ in
let ğ‘ init = âŸ¦ğ‘’initâŸ§ğ›¾
â€² in (ğ‘ init, ğœ†(ğ‘ , ğ‘£). âŸ¦ğ‘’âŸ§ğ›¾
â€²+ [ğ‘ /ğ‘state,ğ‘£/ğ‘input ])
if ğ‘’ is deterministic
âŸ¦init(ğ‘š)âŸ§ğ›¾ = let stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
â€² = âŸ¦ğ‘šâŸ§ğ›¾ in
let ğ‘ init = âŸ¦ğ‘’initâŸ§ğ›¾
â€² in (ğ‘ init, ğœ†(ğ‘ , ğ‘£). {[ğ‘’]}ğ›¾
â€²+ [ğ‘ /ğ‘state,ğ‘£/ğ‘input ])
if ğ‘’ is probabilistic
âŸ¦unfold(ğ‘¥,ğ‘£)âŸ§ğ›¾ = let ğ‘£state, ğ‘“ = âŸ¦ğ‘¥âŸ§ğ›¾ in
let ğ‘£output, ğ‘£â€²
state = ğ‘“ (ğ‘£state, âŸ¦ğ‘£âŸ§ğ›¾ ) in
(ğ‘£output, (ğ‘£
â€²
state, ğ‘“ ))
âŸ¦infer(ğ‘š)âŸ§ğ›¾ = let stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
â€² = âŸ¦ğ‘šâŸ§ğ›¾ in
let ğ‘ init = âŸ¦ğ‘’initâŸ§ğ›¾
â€² in (ğ›¿ğ‘ init , infer(ğœ†(ğ‘ , ğ‘£). {[ğ‘’]}ğ›¾
â€²+ [ğ‘ /ğ‘state,ğ‘£/ğ‘input ]))
where infer(ğ‘“ ) = ğœ†(ğœ, ğ‘£). let ğœ‡ = ğœ†ğ‘ˆ . âˆ«
ğ‘†
ğœ(ğ‘‘ğ‘ )ğ‘“ (ğ‘ , ğ‘£) (ğ‘ˆ ) in
let ğœˆ = ğœ†ğ‘ˆ . ğœ‡(ğ‘ˆ )/ğœ‡(âŠ¤) in
(ğœ‹1âˆ— (ğœˆ), ğœ‹2âˆ— (ğœˆ))
Fig. 8. Deterministic semantics of ğœ‡ğ¹ (complete definition in Figure 16).
Third, the instance is iteratively applied on an input stream (ğ‘–ğ‘›)ğ‘›âˆˆN to produce an output
stream (ğ‘œğ‘›)ğ‘›âˆˆN, defined in the following way:
âŸ¦ğ‘âŸ§(ğ‘–)ğ‘› = ğ‘œğ‘› where ğ‘ = ğ‘‘
âˆ—ğ‘š ğ›¾ = âŸ¦ğ‘‘
âˆ—âŸ§âˆ…
ğ‘ 0 = âŸ¦init(ğ‘š)âŸ§ğ›¾ ğ‘œğ‘›, ğ‘ ğ‘›+1 = âŸ¦unfold(ğ‘ ğ‘›,ğ‘–ğ‘›)âŸ§ğ›¾ âˆ€ğ‘› â‰¥ 0
Figure 8 defines the semantics of declarations and deterministic expressions âŸ¦Â·âŸ§. The declarations
build the evaluation environment ğ›¾ which maps names to values, functions, and stream functions.
The semantics of deterministic expressions corresponds to a first order functional language with
new constructs to handle streams and the infer(Â·) operator (the complete definition is given in
Figure 16 of Appendix A). The expression init(ğ‘š) creates an instance of the stream function ğ‘š: a
pair corresponding to the current state, and the transition function. The current state is initialized
with the value of the init field. The expression unfold(ğ‘¥,ğ‘£) executes the transition function of
the instance ğ‘¥ on its current state and the input ğ‘£. This expression produces a pair composed of the
transformed value and the updated instance.
The ideal semantics of ğœ‡ğ¹ probabilistic expressions {[Â·]} is a measure-based semantics similar
to the one presented by Staton [2017] (the complete definition is given in Appendix A). Given an
environment ğ›¾, an expression is interpreted as a measure {[ğ‘’]}ğ›¾ : Î£ğ· â†’ [0, âˆ), that is, a function
which associates a positive number to each measurable setğ‘ˆ âˆˆ Î£ğ· , where Î£ğ· denotes the Î£-algebra
of the domain of the expression ğ· (i.e., the set of measurable sets of possible values). sample(ğ‘£)
returns the distribution âŸ¦ğ‘£âŸ§ğ›¾ . observe(ğ‘£1,ğ‘£2) weights execution paths using the likelihood of
the observation âŸ¦ğ‘£2âŸ§ğ›¾ w.r.t. the distribution âŸ¦ğ‘£1âŸ§ğ›¾ (for a distribution ğœ‡ we denote its probability
density function as ğœ‡pdf). Local definitions are interpreted as integration, and we use the Dirac
delta measure to interpret deterministic expressions.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:10 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
{[ğ‘£]}ğ›¾ = ğœ†ğ‘”,ğ‘¤ . (âŸ¦ğ‘£âŸ§ğ›¾ , ğ‘”,ğ‘¤)
{[op(ğ‘£)]}ğ›¾ = ğœ†ğ‘”,ğ‘¤ . (app(op, âŸ¦ğ‘£âŸ§ğ›¾ ), ğ‘”,ğ‘¤)
{[ğ‘“ (ğ‘£)]}ğ›¾ = ğœ†ğ‘”,ğ‘¤ . ğ›¾ (ğ‘“ ) (âŸ¦ğ‘£âŸ§ğ›¾ ) (ğ‘”,ğ‘¤)
{[let ğ‘ = ğ‘’1 in ğ‘’2]}ğ›¾ = ğœ†ğ‘”,ğ‘¤ . let ğ‘£1, ğ‘”1,ğ‘¤1 = {[ğ‘’1]}ğ›¾ (ğ‘”,ğ‘¤) in {[ğ‘’2]}ğ›¾+ [ğ‘£1/ğ‘ ] (ğ‘”1,ğ‘¤1)
{[if ğ‘£ then ğ‘’1 else ğ‘’2]}ğ›¾ = ğœ†ğ‘”,ğ‘¤ . let ğ‘, ğ‘”ğ‘ = value(âŸ¦ğ‘£âŸ§ğ›¾ , ğ‘”) in
if ğ‘ then {[ğ‘’1]}ğ›¾ (ğ‘”ğ‘,ğ‘¤) else {[ğ‘’2]}ğ›¾ (ğ‘”ğ‘,ğ‘¤)
{[unfold(ğ‘¥,ğ‘£)]}ğ›¾ = ğœ†ğ‘”,ğ‘¤ . let ğ‘£state, ğ‘“ = âŸ¦ğ‘¥âŸ§ğ›¾ in
let (ğ‘£output, ğ‘£â€²
state), ğ‘”â€²ğ‘¤
â€² = ğ‘“ (ğ‘£state, âŸ¦ğ‘£âŸ§ğ›¾ ) (ğ‘”,ğ‘¤) in
( (ğ‘£output, (ğ‘£
â€²
state, ğ‘“ )), ğ‘”â€²ğ‘¤
â€²
)
{[sample(ğ‘£)]}ğ›¾ = ğœ†ğ‘”,ğ‘¤ . let ğ‘‹, ğ‘”â€² = assume(âŸ¦ğ‘£âŸ§ğ›¾ , ğ‘”) in (ğ‘‹, ğ‘”â€²
,ğ‘¤)
{[observe(ğ‘£1,ğ‘£2)]}ğ›¾ = ğœ†ğ‘”,ğ‘¤ . let ğ‘‹, ğ‘”ğ‘¥ = assume(âŸ¦ğ‘£1âŸ§, ğ‘”) in
let ğ‘£, ğ‘”ğ‘£ = value(âŸ¦ğ‘£2âŸ§, ğ‘”ğ‘¥ ) in
let ğ‘”
â€² = observe(ğ‘‹, ğ‘£, ğ‘”ğ‘£ ) in ((), ğ‘”â€²
,ğ‘¤ âˆ— ğœ‡pdf(ğ‘£))
Fig. 9. Delayed sampling semantics. Probabilistic expressions are functions from a graph and a weight to a
triplet (value, graph, weight).
The infer(ğ‘š) operator creates an instance of a probabilistic stream: the initial state is a Dirac
delta distribution on the initial state of ğ‘š, and the transition function is infer(ğ‘“ ) where ğ‘“ is the
transition function of ğ‘š. The body of ğ‘“ (the expression ğ‘’) is interpreted with the probabilistic
semantics which defines a measure over pairs of output values and states. The function infer(ğ‘“ )
takes as arguments a distribution of states ğœ and an input ğ‘£ and returns a distribution of outputs
and a distribution of new states. These two distributions are obtained by integrating the transition
function ğ‘“ along the distribution ğœ of possible states (domain ğ‘†) to build a measure ğœ‡ which is then
normalized to build a distribution ğœˆ of pairs (outputs, states). The distribution ğœˆ is then split into a
pair of marginal distributions using the pushforward of ğœˆ across the projections ğœ‹1 and ğœ‹2.
4 DELAYED SAMPLING
In this section, we present the details of delayed sampling that underpin this work. This is a new
formalization of results that were presented by Murray et al. [2018] and Baudart et al. [2020].
Delayed sampling is a semi-symbolic algorithm combining exact inference and â€“ when exact
computation fails â€“ approximate inference with particle filtering [Del Moral et al. 2006]. A particle
filter launches multiple executions of the model. Each execution â€” or particle â€” is associated to a
weight. In the operational semantics, sample(ğ‘‘) statements draw samples from the corresponding
distributions, and observe(ğ‘¥,ğ‘‘) statements update the weight to reflect the quality of the samples.
At the end of the executions the results of all the particles are normalized according to their weights
to form a categorical distribution that approximates the posterior distribution of the model.
In delayed sampling, each particle contains a graph of random variables and their dependencies
that can be used to compute closed-form distributions. Observations can be incorporated by
analytically conditioning the network. If symbolic conditioning fails, inference falls back to a
particle filter, drawing concrete samples for required random variables.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:11
4.1 Operational Semantics
The definition of infer in Figure 8 makes use of an intractable integral. The delayed sampling
semantics replaces this integral by a discrete sum over the set of particles of the particle filter.
Compared to traditional particle filtering, delayed sampling performs exact computations when
possible. Thus, we extend valuesğ‘£ with symbolic terms. Symbolic terms include random variables (ğ‘‹)
â€” the nodes of the delayed sampling graph â€” and applications of operators.
ğ‘£ ::= ... | ğ‘‹ | app(op, ğ‘£)
The semantics in Figure 9 rely on the following high-level operations to update the graph ğ‘”.
ğ‘£
â€²
, ğ‘”â€² = value(ğ‘£, ğ‘”) samples all the random variables in ğ‘£ to produce a concrete value.
ğ‘”
â€² = observe(ğ‘‹, ğ‘£, ğ‘”) conditions the graph on the fact that the random variable ğ‘‹ takes the value ğ‘£.
ğ‘‹, ğ‘”â€² = assume(ğ‘‘, ğ‘”) adds and returns a new random variable ğ‘‹ with distribution ğ‘‘.
Probabilistic semantics. The semantics of a probabilistic expressions are defined in Figure 9. The
semantics of an expression {[ğ‘’]}ğ›¾,ğ‘”,ğ‘¤ takes two additional arguments: ğ‘”, the delayed sampling graph,
and ğ‘¤, the weight for the particle filter, and returns a symbolic value, an updated graph, and
an updated weight. Operator application op(ğ‘£) introduces a symbolic expression app(op, ğ‘£). if
uses the value operation to sample a concrete value for the condition. sample(ğ‘£) introduces a
new random variable in the graph with distribution ğ‘£. observe(ğ‘£1,ğ‘£2) introduces a fresh random
variable ğ‘‹ with distribution ğ‘£1, and conditions the graph on the fact that ğ‘‹ takes the value ğ‘£2.
Inference. Given a transition function ğ‘“ , a distribution over states ğœ from the previous iteration,
and inputs ğ‘£ğ‘–
, the infer operator computes a distribution of outputs and new distribution over
states for the next iteration. First, the inference draws ğ‘ states from ğœ. Each of theses states ğ‘ ğ‘› is
associated with a delayed sampling graph ğ‘”ğ‘›. Second, the transition function ğ‘“ returns a symbolic
output value ğ‘£ğ‘›, a new state ğ‘ 
â€²
ğ‘›
, the updated graph ğ‘”
â€²
ğ‘›
, and the importance weight ğ‘¤ğ‘›. Third, the
distribution(ğ‘œğ‘›, ğ‘”â€²
ğ‘›
) function returns a distribution of values without altering the graph, and the
new distribution over states is a Dirac delta distribution on the pair (ğ‘ 
â€²
ğ‘›
, ğ‘”â€²
ğ‘›
). Finally, results are
accumulated in a mixture distribution using the weights ğ‘¤ğ‘› and this distribution is split into a
distribution of values and a distribution of next states.1
âŸ¦infer(ğ‘š)âŸ§ğ›¾ = let stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
â€² = âŸ¦ğ‘šâŸ§ğ›¾ in
let ğ‘ init = âŸ¦ğ‘’initâŸ§ğ›¾
â€² in (ğ›¿(ğ‘ init,âˆ…), infer(ğœ†(ğ‘state, ğ‘input). {[ğ‘’]}ğ›¾
â€²))
where infer(ğ‘“ ) = ğœ†(ğœ, ğ‘£ğ‘–). let ğœ‡ = ğœ†ğ‘ˆ . Ã
ğ‘
ğ‘›=1
let ğ‘ ğ‘›, ğ‘”ğ‘› = draw(ğœ) in
let (ğ‘œğ‘›, ğ‘ â€²
ğ‘›
), ğ‘”â€²
ğ‘›
,ğ‘¤ğ‘› = ğ‘“ (ğ‘ ğ‘›,ğ‘£ğ‘–)(ğ‘”ğ‘›, 1) in
let ğ‘‘ğ‘› = distribution(ğ‘œğ‘›, ğ‘”â€²
ğ‘›
) in
ğ‘¤ğ‘› âˆ— ğ‘‘ğ‘› (ğœ‹1 (ğ‘ˆ )) âˆ— ğ›¿ğ‘ 
â€²
ğ‘›,ğ‘”â€²
ğ‘›
(ğœ‹2 (ğ‘ˆ ))
in (ğœ‹1âˆ— (ğœ‡), ğœ‹2âˆ— (ğœ‡))
4.2 Graph Manipulation
We now describe the graph manipulation functions that are required to define the high-level
operations value, assume, and observe used in the semantics of Figure 9. LundÃ©n [2017] and Murray
et al. [2018] provide detailed explanations of these operations.
Notation. In this section and those that follow, frv(ğ‘£) denotes the free random variables of a
program value ğ‘£, i.e., the set of variables used in the symbolic expression ğ‘£.
1we write ğ‘¤ğ‘– = ğ‘¤ğ‘– /
Ãğ‘
ğ‘–=1 ğ‘¤ğ‘– for the normalized weights.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:12 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Graph Data Structure. A delayed sampling graph ğ‘” is defined by a tuple (ğ‘‰ , ğ¸, ğ‘) where ğ‘‰ is a set
of vertices â€“ the random variables, ğ¸ is a set of directed edges â€“ the dependencies between random
variables, and ğ‘ is a relation mapping each node to a state: Initialized, Marginalized, or Realized.
A node Initialized(ğ‘ğ‘‹ |ğ‘Œ ) represents a random variable ğ‘‹ with a conditional distribution ğ‘ğ‘‹ |ğ‘Œ
where ğ‘Œ is the unique parent of ğ‘‹. A node Marginalized(ğ‘ğ‘‹ ) represents a random variable ğ‘‹ with a
marginal distribution ğ‘ğ‘‹ . A Marginalized node has at most one parent. If there is a parent node, the
distribution ğ‘ğ‘‹ incorporates its distribution. A node Realized(ğ‘£) represents a random variable ğ‘‹
associated to a concrete value ğ‘£. By construction, a delayed sampling graph is a forest â€“ a set of
trees (each node has at most one parent).
value. The operation value(ğ‘£, ğ‘”) converts the symbolic expression ğ‘£ into a concrete value by
sampling all the random variables in ğ‘£. All these random variables become Realized nodes in the
graph, and the distributions depending on these variables are updated.
value(ğ‘£, ğ‘”) = (ğ‘£, ğ‘”) if ğ‘£ is a concrete value
value(app(op, ğ‘£), ğ‘”) = let ğ‘£
â€²
, ğ‘”â€² = value(ğ‘£, ğ‘”) in (op(ğ‘£
â€²
), ğ‘”â€²
)
value(ğ‘‹, ğ‘”) = let ğ‘‰ , ğ¸, ğ‘ = ğ‘” in
if ğ‘(ğ‘‹) = Realized(ğ‘£) then (ğ‘£, ğ‘”)
else let ğ‘‰
â€²
, ğ¸â€²
, ğ‘â€²
[ğ‘‹ â† Marginalized(ğœ‡)] = graft(ğ‘‹, ğ‘”) in
let ğ‘£ = draw(ğœ‡) in
(ğ‘£, (ğ‘‰
â€²
, ğ¸â€²
, ğ‘â€²
[ğ‘‹ â† Realized(ğ‘£)]))
If ğ‘£ is already a concrete value, there is nothing to do. If ğ‘£ is the application of an operator, value
recursively samples a concrete value for the argument and applies the operator to this value. Ifğ‘£ is a
random variable ğ‘‹ that is already realized, value returns the corresponding value. Otherwise, value
(1) calls the graft function defined in Appendix C to marginalize ğ‘‹ and all its ancestors, (2) draws a
sample from the marginalized distribution, and (3) returns this value and turns ğ‘‹ into a Realized
node. Note that graft might have to realize some nodes since it marginalizes all its ancestors and a
marginal node has a single marginalized child. During marginalization, graft also removes edges
between Marginalized nodes and their Realized child if any.
assume. The operation assume(ğ‘£, ğ‘”) adds a new random variable ğ‘‹ with distribution ğ‘£ in graph ğ‘”.
assume(ğ‘£, ğ‘”) = let (ğ‘‰ , ğ¸, ğ‘) = ğ‘” in
let ğ‘‹ = fresh(ğ‘‰ ) in
if frv(ğ‘£) = âˆ… then (ğ‘‹, (ğ‘‰ âˆª {ğ‘‹}, ğ¸, ğ‘[ğ‘‹ â† Marginalized(ğ‘£)]))
else if frv(ğ‘£) = {ğ‘Œ } âˆ§ conj(ğ‘£, ğ‘Œ, ğ‘”) then
(ğ‘‹, (ğ‘‰ âˆª {ğ‘‹}, ğ¸ âˆª {(ğ‘‹, ğ‘Œ)}, ğ‘[ğ‘‹ â† Initialized(ğ‘£)]))
else
let ğ‘£
â€²
, (ğ‘‰ ,â€² ğ¸
â€²
, ğ‘â€²
) = value(ğ‘£, (ğ‘‰ âˆª {ğ‘‹}, ğ¸, ğ‘)) in
(ğ‘‹, (ğ‘‰
â€²
, ğ¸â€²
, ğ‘â€²
[ğ‘‹ â† Marginalized(ğ‘£
â€²
)]))
The distribution ğ‘£ is a symbolic expression which can be a marginal distribution that does not
depends on other random variables â€” e.g., app(bernoulli, 0.5)â€” or a conditional distribution â€” e.g.,
app(bernoulli, ğ‘Œ) where ğ‘Œ is a random variable. If ğ‘£ is a marginal distribution, assume just adds a
new marginalized node in the graph. If ğ‘£ is a conditional distribution, assume tries to keep track of
the dependency between ğ‘‹ and a random variable used in ğ‘£ (the delayed sampling graph is a forest
where each node has at most one parent).
The value ğ‘£ thus represents a distribution ğ‘ğ‘‹ |ğ‘Œ where ğ‘‹ depends on a unique random variable ğ‘Œ.
If the distribution ğ‘ğ‘‹ |ğ‘Œ and ğ‘ğ‘Œ are conjugate (conj(ğ‘£, ğ‘Œ, ğ‘”)) â€” e.g., app(bernoulli, ğ‘Œ) with ğ‘Œ âˆ¼
beta(ğ›¼, ğ›½) â€” marginalization and conditioning are tractable operations, and assume adds an edge
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:13
between ğ‘Œ and a new initialized node ğ‘‹ to the graph. Otherwise, symbolic computation is not
possible; assume calls value to sample a concrete value, thus breaking the dependency, and adds a
new independent Marginalized node to the graph.
observe. The operation observe(ğ‘‹, ğ‘£, ğ‘”) assigns the concrete value ğ‘£ to ğ‘‹ and updates the distributions depending on ğ‘‹ accordingly.
observe(ğ‘‹, ğ‘£, ğ‘”) = let (ğ‘‰ , ğ¸, ğ‘) = graft(ğ‘‹, ğ‘”) in (ğ‘‰ , ğ¸, ğ‘[ğ‘‹ â† Realized(ğ‘£)])
Similarly to value, the observe operation uses the function graft to marginalize the variable ğ‘‹ and
then turns ğ‘‹ to a Realized node associated with the value ğ‘£.
4.3 Memory Usage
Baudart et al. [2020] proposed an implementation of delayed sampling where an Initialized node
only has a pointer to its parent, a Marginalized node only has a pointer to its unique Marginalized
or Realized child, if any, and a Realized node has no pointers to its parent or any of its children.
Garbage Collection. A node in the delayed sampling graph can be safely removed if none of the
program variables depend on its value. We assume the existence of a garbage collection routine
that deallocates the nodes of the graph that are not reachable as soon as possible.
Definition 4.1 (Reachability). Given a set of root variables ğ‘Ÿ and a delayed sampling graph
ğ‘” = (ğ‘‰ , ğ¸, ğ‘), the set of reachable variables â€“ written reachable(ğ‘”, ğ‘Ÿ) â€“ is defined as follows:
ğ‘… = {(ğ‘‹, ğ‘Œ) |
(ğ‘‹, ğ‘Œ) âˆˆ ğ¸ âˆ§ ğ‘(ğ‘‹) = Initialized
âˆ¨

(ğ‘Œ, ğ‘‹) âˆˆ ğ¸
âˆ§ ğ‘(ğ‘‹) = Marginalized âˆ§ (ğ‘(ğ‘Œ) = Marginalized âˆ¨ ğ‘(ğ‘Œ) = Realized)

reachable(ğ‘”, ğ‘Ÿ) = {ğ‘Œ | (ğ‘…
âˆ—
(ğ‘‹, ğ‘Œ)) âˆ§ ğ‘‹ âˆˆ ğ‘Ÿ âˆ§ ğ‘Œ âˆˆ ğ‘‰ }
where ğ‘…
âˆ— denotes the reflexive transitive closure of the relation ğ‘….
If we consider the graph in Figure 2b, reachable(ğ‘”, {x}) = {x}. In the example of Figure 4b, we
have reachable(ğ‘”, {i, x}) = {i, pre_x, x}, where pre_x is the gray node in between the nodes for i
and x. Reachability is the core property used in Definition 5.1 to define what it means for a program
to run in bounded memory.
Graph Expansion. The only operation that increases the size of the graph is assume which
introduces new nodes. The operations value and observe can only marginalize and realize nodes.
If ğ‘”
â€²
is the graph resulting from the application of value or observe on a graph ğ‘”, ğ‘” and ğ‘”
â€² have
the same structure but Initialized nodes can be Marginalized or Realized, and Marginalized nodes
can be Realized. The reachability relation of the graph implies that value and observe reduce the
number of dependencies in the delayed sampling graph, that is, reachable(ğ‘”
â€²
, ğ‘Ÿ) âŠ† reachable(ğ‘”, ğ‘Ÿ).
Initialized and Marginalized Chains. Two patterns can yield unbounded memory consumption.
First, it is possible to keep adding nodes without realizing them (via observation or sampling),
thus forming initialized chains. An initialized chain is a sequence of initialized nodes, each of
which holds a pointer to its parent and thereby expands the number of random variables that are
reachable. Second, it is possible that nodes are only indirectly used to realize one of their children.
These marginalized nodes can form marginalized chains. A marginalized chain is a sequence of
marginalized nodes, each of which holds a pointer to its child and thus expands the number of
random variables that are reachable. The last node of a marginalized chain may be realized.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 202
115:14 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
5 SEMANTIC PROPERTIES
In this section, we define conditions under which delayed sampling executes in bounded memory.
We define these conditions as properties of executions. An execution is a sequence of pairs of a state
and a delayed sampling graph (ğ‘ ğ‘›, ğ‘”ğ‘›)ğ‘›âˆˆN, where each state is a semi-symbolic value as defined in
Section 4.1. An execution defines the sequence of states and graphs a model â€” i.e., an argument of
an infer â€” goes through.
The inference step function infer(ğ‘“ ) in âŸ¦infer(ğ‘š)âŸ§ may operate over multiple executions
of ğ‘“ (see Section 4.1). However, infer(ğ‘“ ) executes in bounded memory if every execution of ğ‘“ is
bounded-memory. This is because infer(ğ‘“ ) always updates its state by mapping ğ‘“ over states and
graphs from the distribution at the previous iteration. Thus, any state and graph in the distribution
at the next iteration must have come from some execution of ğ‘“ , and if all executions of ğ‘“ are
bounded-memory, all states and graphs in the distribution must have bounded memory. We have
formalized this in more details in Appendix E.1.
Based on this notion of execution, we introduce two notions of bounded-memory executions
of delayed sampling, and semantic properties which are necessary and sufficient for boundedmemory execution. In Section 5.1 we present a low-level definition of bounded memory that directly
corresponds to how the delayed sampling runtime executes. In Section 5.2 we present an alternative
high-level definition in terms of dataflow properties of the high-level delayed sampling operators:
the ğ‘š-consumed and unseparated paths properties. In Section 5.3 we show that the high-level and
low-level formulations are equivalent. In particular, Section 5.3 shows a correspondence between the
ğ‘š-consumed property and a bound on the length of initialized chains, as well as a correspondence
between the unseparated paths property and a bound on the length of marginalized chains.
5.1 Low-Level Bounded Memory
A program executes in bounded memory if the delayed sampling graph maintains a bounded
number of reachable variables over time. We formalize this as follows:
Definition 5.1 (Low-level Bounded-Memory). An execution (ğ‘ ğ‘›, ğ‘”ğ‘›)ğ‘›âˆˆN of a model is low-level
bounded-memory if
âˆƒğ‘˜. âˆ€ğ‘› â‰¥ 0 |reachable(ğ‘”ğ‘›, ğ‘ ğ‘›)| â‰¤ ğ‘˜ âˆ— |frv(ğ‘ ğ‘›)|
This definition states that at each iteration, the size of the set of reachable nodes in the delayed
sampling graph may be at most a constant multiple of the number of free random variables in
the state. We do not consider the runtime to violate bounded memory in the trivial case that the
program state is intrinsically unbounded, i.e., when |frv(ğ‘ ğ‘›)|ğ‘›âˆˆN is unbounded. Such a program
would not execute in bounded memory under any inference algorithm; even a particle filter would
require unbounded memory to store the program state.
5.2 High-Level Definitions
In this section, we present an alternative high-level definition of bounded memory that is easier
to reason about. The high-level definition is in terms of dataflow properties of delayed sampling
operations. We have formalized these dataflow properties by augmenting the delayed sampling
operations with tracing. A trace is defined as follows:
ğœ ::= ğœ :: ğœ1 | nil
ğœ1 ::= ğ‘‹ f ğ‘‹ | ğ‘‹ f nil | eval(X) | obs(ğ‘‹)
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:15
assume(ğ‘£, (ğ‘”, ğœ)) = let ğ‘‹
â€²
, ğ‘”â€² = assume(ğ‘£, ğ‘”) in
ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³
ğ‘‹
â€²
, (ğ‘”
â€²
, ğœ :: ğ‘‹
â€² f nil) frv(ğ‘£) = âˆ…
ğ‘‹
â€²
, (ğ‘”
â€²
, ğœ :: ğ‘‹
â€² f ğ‘‹) {ğ‘‹} = frv(ğ‘£) âˆ§ conj(ğ‘£, ğ‘‹, ğ‘”)
ğ‘‹
â€²
, (ğ‘”
â€²
, ğœ :: eval(frv(ğ‘£)) :: ğ‘‹
â€² f nil) otherwise
value(ğ‘£, (ğ‘”, ğœ)) = let (ğ‘£
â€²
, ğ‘”â€²
) = value(ğ‘£, ğ‘”) in ğ‘£
â€²
, (ğ‘”
â€²
, ğœ :: eval(frv(ğ‘£)))
observe(ğ‘‹, ğ‘£, (ğ‘”, ğœ)) = observe(ğ‘‹, ğ‘£, ğ‘”), (ğœ :: obs(ğ‘‹))
Fig. 10. Tracing semantics of delayed sampling operators.
A trace is a list of primitive operations, where each primitive is one of:
â€¢ Assumption, written ğ‘‹ f ğ‘‹
â€² when ğ‘‹ is assumed from another random variable ğ‘‹
â€² or
ğ‘‹ f nil when it is assumed without a parent.
â€¢ Evaluation using the eval keyword, which refers to evaluating a set of random variables X.
â€¢ Observation using the obs keyword, which refers to observing a random variable ğ‘‹.
We define an augmented semantics that operates on a pair of a delayed sampling graph and a trace.
Figure 10 defines augmented versions of the assume, value, and observe operations, and the full
semantics (written âŸ¦Â·âŸ§ and {[Â·]}) is defined by replacing these operators in Figure 9 with their traced
counterparts from Figure 10.
The ğ‘š-consumed Property. The ğ‘š-consumed property is used to enforce that every variable
introduced with assume is eventually consumed either by directly being passed to a value or observe
or transitively by being passed to a assume that introduces a variable that is also ğ‘š-consumed.
Definition 5.2 (ğ‘š-consumed). A variable ğ‘‹ is ğ‘š-consumed in a trace ğœ under the following
circumstances:
â€¢ ğ‘‹ is 0-consumed if it is observed or evaluated (i.e., ğœ has eval(ğ‘‹) where ğ‘‹ âˆˆ X or obs(ğ‘‹)).
â€¢ ğ‘‹ is 0-consumed if it is never used (i.e., there is no ğ‘‹
â€² f ğ‘‹, eval(ğ‘‹), or obs(ğ‘‹) in ğœ).
â€¢ ğ‘‹ is ğ‘š-consumed if it is passed to the assume statement that introduces another variable ğ‘‹
â€²
(i.e., ğ‘‹
â€² f ğ‘‹ is in ğœ), and ğ‘‹
â€²
is (ğ‘š âˆ’ 1)-consumed.
The Unseparated Paths Property. The unseparated paths property states the existence of a sequence
of variables, each assumed from the previous, with no variable in the sequence observed or evaluated.
Definition 5.3 (Unseparated Paths). An unseparated path inğœ is a sequence of variablesğ‘‹0, ğ‘‹1, . . . , ğ‘‹ğ‘›
such that each ğ‘‹ğ‘–+1 was assumed from ğ‘‹ğ‘–
(i.e., ğ‘‹ğ‘–+1 f ğ‘‹ğ‘–
is in ğœ) and no ğ‘‹ğ‘–
is directly observed or
evaluated (i.e., ğœ does not contain any eval or obs operations that reference ğ‘‹ğ‘–
).
High-level Bounded Memory. We now present the high-level bounded memory property. This
property states that all variables must eventually be ğ‘š-consumed, and there must be a uniform
bound across iterations on the length of an unseparated path starting from a program state variable.
Definition 5.4 (High-level Bounded-Memory). A program execution (ğ‘ ğ‘›, (ğ‘”ğ‘›, ğœğ‘›))ğ‘›âˆˆN is high-level
bounded-memory if and only if
â€¢ There exists an ğ‘š such that for every iteration ğ‘› and every variable introduced before ğ‘› (i.e.,
ğ‘‹ such that ğ‘‹ f ğ‘‹
â€² or ğ‘‹ f nil is in ğœğ‘›), there exists a ğ‘›
â€² â‰¥ ğ‘› such that for all ğ‘›
â€²â€² â‰¥ ğ‘›
â€²
, ğ‘‹
is ğ‘š-consumed in ğœğ‘›
â€²â€².
â€¢ There exists a ğ‘ such that for all ğ‘›, no random variable referenced in ğ‘ ğ‘› starts an unseparated
path in ğœğ‘› of length more than ğ‘.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:16 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
5.3 Equivalence of Low-Level and High-Level Definitions
In this section, we show the equivalence of the low-level and high-level definitions. We do so
by showing that both properties are equivalent to the delayed sampling graph having a uniform
bound (i.e., a bound that holds across all iterations) on the length of initialized and marginalized
chains as defined in Section 4.3.
5.3.1 Low-Level Bounded Memory vs. Infinite Chains.
Lemma 5.5. If the delayed sampling graph is constructed using assume, observe, and value operations, then each random variable starts either an initialized chain, a marginalized chain, or an
initialized chain followed by a marginalized chain.
Proof. The assume, observe, and value operations can only make the following modifications to
a delayed sampling graph ğ‘”. (1) Add a independent Marginalized node which creates a marginalized
chain of length zero. (2) Attach a new Initialized node ğ‘‹ to a node ğ‘Œ with a conjugate distribution. It
means that ğ‘Œ is either Initialized or Marginalized and thus it creates either a longer initialized chain
or an initialized chain followed by a marginalized chain. (3) Perform a graft which ensures that
every ancestor of a node is marginalized and has a single marginalized child. Every non-ancestor
variable is either as it was before or becomes realized, so this operation preserves the structure of
the previous graph and cannot increase the length of the chains. (4) convert a Marginalized node
into a Realized node which can only break a chain. â–¡
Theorem 5.6. A program is low-level bounded-memory iff there is a uniform boundğ‘š on the length
of an initialized chain and a uniform bound ğ‘ on the length of a marginalized chain.
Proof. Assuming a uniform bound, when the number of variables is bounded by ğ‘, according
to Lemma 5.5, the number of reachable nodes in the graph is bounded by ğ‘ Ã— (ğ‘ + ğ‘š).
Conversely, if no uniform bound exists (i.e., for every potential bounds ğ‘ and ğ‘š, there exists
a iteration ğ‘› such that chains may exceed the bound at ğ‘›), the execution cannot be low-level
bounded-memory, because even if the number of root variables is bounded by ğ‘, the reachable
variables may exceed ğ‘ Ã— (ğ‘ + ğ‘š). â–¡
5.3.2 High-Level Bounded Memory vs. Infinite Chains.
Theorem 5.7 (High-level Soundness). In a program execution that is high-level bounded-memory,
no infinite chains can exist in any of the delayed sampling graphs.
Proof. All initialized chains must be shorter than ğ‘š, where ğ‘š is from the ğ‘š-consumed property
of high-level bounded-memory. This is because when a variableâ€™s descendant is subject to observe
or value, the variable becomes marginalized. Such a descendant can be at most ğ‘š variables away
because of the definition of ğ‘š-consumed.
All marginalized chains must be shorter than ğ‘ +ğ‘š, where ğ‘ is from the unseparated path property
of high-level bounded-memory and ğ‘š is from the ğ‘š-consumed property. By Lemma 5.5, every
marginalized chain must start at either a root or an initialized chain. If it starts at a root, the
unseparated path property ensures that the path between the root and the end of the chain can
contain at most ğ‘ variables. This is because any observed or valued variables become realized and
become the end of the chain. If it starts at an initialized chain, by the above reasoning that chain
has length at most ğ‘š, and there was a previous iteration at which the marginalized chain started at
a root and had length at most ğ‘, giving an overall length of at most ğ‘ + ğ‘š. â–¡
Lemma 5.8. If there exists a variable that is not ğ‘š-consumed, then the program produces a graph at
some iteration with an initialized chain of length ğ‘š.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:17
Proof. If a variable is not ğ‘š-consumed, then by the definition of ğ‘š-consumed must start an
assume chain of length ğ‘š. All of the nodes in this chain must be initialized, and therefore form an
initialized chain of length ğ‘š. â–¡
Lemma 5.9. If every variable is ğ‘š-consumed, and there exists a variable that starts an unseparated
path of length ğ‘ where ğ‘ > ğ‘š, then there exists an iteration with a marginalized chain that has length
at least ğ‘ âˆ’ ğ‘š.
Proof. Note that the firstğ‘ âˆ’ğ‘š variables in the unseparated path must be either marginalized or
realized. Otherwise, there would be more than ğ‘š initialized variables in the tail of the unseparated
path that are initialized, which would violate soundness of ğ‘š-consumed. Let ğ‘‹ be the variable that
starts the unseparated path and ğ‘‹
â€² be the last marginalized or realized variable in the unseparated
path, and consider the iteration ğ‘›
â€² when ğ‘‹
â€² was first marginalized. It must be true that (1) ğ‘‹ is in
the program state at iteration ğ‘›
â€² because it is in the state at the current iteration ğ‘› > ğ‘›
â€²
, and (2) a
marginalized chain runs from ğ‘‹ to ğ‘‹
â€²
. Thus, at ğ‘›
â€²
, the marginalized chain had length ğ‘ âˆ’ ğ‘š. â–¡
Theorem 5.10 (High-level Completeness). If a program execution is not high-level boundedmemory, the delayed sampling graph has either unbounded initialized chains or marginalized chains.
Proof. If the execution is not high-level bounded-memory, it either fails the ğ‘š-consumed
property or the unseparated path property. If it fails the ğ‘š-consumed property, apply Lemma 5.8.
Otherwise, apply Lemma 5.9. â–¡
Theorem 5.11. A program execution is high-level bounded-memory if and only if it is low-level
bounded-memory.
Proof. Apply Theorems 5.6, 5.7, and 5.10. â–¡
6 ANALYSIS
In this section, we develop an analysis to check that a ğœ‡ğ¹ program executes in bounded memory. We
approach this problem by developing two independent analyses within a shared analysis framework.
One analysis checks the ğ‘š-consumed property of a program and the other checks the unseparated
paths property, which together ensure that the program executes in bounded memory (Section 5).
Our shared analysis framework abstracts the execution of a program as the execution of abstract
operations on an abstract graph. An abstract graph abstracts the dynamic state of a programâ€™s
delayed sampling graph. We implement the analysis framework by means of a type system, such
that well-typed programs satisfy the ğ‘š-consumed and unseparated paths properties, given each
analysisâ€™s respective instantiation of the abstract graph. The typing judgment
Î“, G âŠ¢ğ›¼ ğ‘’ : ğ‘¡, G
â€²
asserts that in a context Î“, and for an abstract graph G, that an expression ğ‘’ accesses the random
variables denoted by the type ğ‘¡ and yields a new abstract graph G
â€²
. The parameter ğ›¼ is either mc
to denote the ğ‘š-consumed analysis or up to denote the unseparated paths. We write Î“ âŠ¢ğ›¼ ğ‘’ : ğ‘¡ as
shorthand for Î“, G âŠ¢ğ›¼ ğ‘’ : ğ‘¡, G when ğ‘’ has no effect on the graph.
6.1 Types and Contexts
A type ğ‘¡ captures the random variables the expression could refer to as well as its shape, as primitive
data, a product, a function, or a stream instance.
ğ‘¡ F ğ‘Ÿ | () | ğ‘¡1 Ã— ğ‘¡2 | ğ‘¡1 â†’ ğ‘¡2 | stream(ğ‘¡, ğ‘ ) | bounded
ğ‘  F stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’)
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:18 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
The type of a primitive expression is a reference set, denoted ğ‘Ÿ, which specifies the random variables
to which the expression refers. We distinguish two types of stream instances, before and after
bounded-memory checking. The first is stream(ğ‘¡, ğ‘ ), where ğ‘¡ is the type of the current state and ğ‘  is
a step function representation to be described later. The second is bounded, representing instances
that have passed bounded memory analysis and hide their inner structure.
Reference Sets. A reference set of a ğœ‡ğ¹ expression, denoted ğ‘Ÿ, specifies the random variables that
are affected when the expression is observed or evaluated. In the presence of branches, we define ğ‘Ÿ
to be a pair of sets (lb, ub), where the lower bound lb contains all random variables which must
be affected and the upper bound ub all random variables which may be affected. For example,
a constant value in ğœ‡ğ¹ such as 1.5 has the reference set (âˆ…, âˆ…) because it references no random
variables. If the program variables x and y correspond to random variables ğ‘‹ and ğ‘Œ respectively,
then the expression gaussian(x,y), specifying a distribution with two parameters, has reference
set ({ğ‘‹, ğ‘Œ }, {ğ‘‹, ğ‘Œ }), meaning that observing it will observe the random variables ğ‘‹ and ğ‘Œ.
Contexts. The context Î“, ğ‘¥ : ğ‘¡ maps variable ğ‘¥ to type ğ‘¡. As ğœ‡ğ¹ syntactic patterns ğ‘ may be
variables or pairs, we use the shorthand Î“, ğ‘ : ğ‘¡ to define types for variables in ğ‘ by structural
correspondence with ğ‘¡, as defined by the first rule below. We also define a judgment âŠ¢ğ‘ ğ‘ : ğ‘¡ that
synthesizes a deterministic type ğ‘¡ from a pattern ğ‘.
Î“, ğ‘1 : ğ‘¡1, ğ‘2 : ğ‘¡2 âŠ¢ğ›¼ ğ‘’ : ğ‘¡
Î“, (ğ‘1, ğ‘2) : ğ‘¡1 Ã— ğ‘¡2 âŠ¢ğ›¼ ğ‘’ : ğ‘¡ âŠ¢ğ‘ ğ‘¥ : (âˆ…, âˆ…)
âŠ¢ğ‘ ğ‘1 : ğ‘¡1 âŠ¢ğ‘ ğ‘2 : ğ‘¡2
âŠ¢ğ‘ (ğ‘1,ğ‘2) : ğ‘¡1 Ã— ğ‘¡2
6.2 Abstract Graphs
An abstract graph G is an abstraction of the delayed sampling graph that tracks which random
variables have been consumed and active paths between random variables, properties relevant to
the semantic properties. For each analysis ğ›¼ there exists an abstract graph type, G, and a set of
operations that form its interface (Figure 11).
Specifically, in the ğ‘š-consumed analysis we define G to be a pair of sets in and con which
respectively represent an over-approximation of variables introduced into the graph and an underapproximation of the variables consumed by observation or sampling (Figure 12). In the unseparated
paths analysis, we define G to be a set sep of separators containing consumed random variables
and a partial path function ğ‘ mapping a pair of random variables to an upper bound on the length
of an unseparated path between them (Figure 13).
Operations on the abstract graph manipulate random variables, graphs, and reference sets. The
function assume returns a new graph with a random variable ğ‘‹ from a distribution with reference
set ğ‘Ÿ added to G, observe returns a graph where ğ‘‹ is observed with a value with reference set ğ‘Ÿ,
and value returns a graph where an expression with reference set ğ‘Ÿ is evaluated. The join operator
âŠ”ğ›¼ represents a conservative choice between two graphs.
ğ‘š-consumed Graph Operations. In Figure 12, assumemc(ğ‘‹, ğ‘Ÿ, G) marks the random variable ğ‘‹ as
introduced. In all cases, the lower bound of random variables in the input is marked consumed. To
join two states, we union the introduced variables and intersect the consumed variables.
Unseparated Paths Graph Operations. In Figure 13, observeup and valueup mark input variables as
separators. In assumeup, we set the length of the path from the new variable ğ‘‹ to itself to zero. For
a parent ğ‘‹ğ‘ that is not a separator, we set the length of the path from any variable ğ‘‹ğ‘– to ğ‘‹ to one
more than the length from ğ‘‹ğ‘– to ğ‘‹ğ‘ . To join two states, we intersect the separators and take the
maximum length between the results of the two path functions (where defined, or 0 otherwise).
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:19
assumeğ›¼ : RV â†’ ğ‘Ÿ â†’ G â†’ G
observeğ›¼ : RV â†’ ğ‘Ÿ â†’ G â†’ G
valueğ›¼ : ğ‘Ÿ â†’ G â†’ G
âŠ”ğ›¼ : G â†’ G â†’ G
Fig. 11. Abstract graph interface.
G F {ğ‘–ğ‘› âŠ† RV;con âŠ† RV}
assumemc(ğ‘‹, ğ‘Ÿ, G) = {G.ğ‘–ğ‘› âˆª {ğ‘‹}; G.con âˆª ğ‘Ÿ .lb}
observemc(ğ‘‹, ğ‘Ÿ, G) = {G.ğ‘–ğ‘›; G.con âˆª ğ‘Ÿ .lb âˆª {ğ‘‹}}
valuemc(ğ‘Ÿ, G) = {G.ğ‘–ğ‘›; G.con âˆª ğ‘Ÿ .lb}
G1 âŠ”mc G2 = {G1.ğ‘–ğ‘› âˆª G2.ğ‘–ğ‘›; G1.con âˆ© G2.con}
Fig. 12. ğ‘š-consumed abstract graph operations.
G F {ğ‘ : RV Ã— RV â†©â†’ N;sep âŠ† RV}
assumeup (ğ‘‹, ğ‘Ÿ, G) = {ğ‘
â€²
; G.sep} where ğ‘
â€²
(ğ‘‹, ğ‘‹) â†¦â†’ 0,
ğ‘
â€²
(ğ‘‹ğ‘–
, ğ‘‹) â†¦â†’ G.ğ‘(ğ‘‹ğ‘–
, ğ‘‹ğ‘ ) + 1 for all ğ‘‹ğ‘ âˆˆ ğ‘Ÿ .ub \ G.sep, ğ‘‹ğ‘– âˆˆ RV,
ğ‘
â€²
(ğ‘‹, ğ‘Œ) â†¦â†’ G.ğ‘(ğ‘‹, ğ‘Œ) otherwise
observeup (ğ‘‹, ğ‘Ÿ, G) = {G.ğ‘; G.sep âˆª ğ‘Ÿ .lb âˆª {ğ‘‹}}
valueup (ğ‘Ÿ, G) = {G.ğ‘; G.sep âˆª ğ‘Ÿ .lb}
G1 âŠ”up G2 = {ğ‘
â€²
; G1.sep âˆ© G2.sep} where ğ‘
â€²
(ğ‘£1, ğ‘£2) â†¦â†’ max(G1.ğ‘(ğ‘£1, ğ‘£2), G2.ğ‘(ğ‘£1, ğ‘£2))
Fig. 13. Unseparated paths abstract graph operations.
6.3 Typing Rules
In Figure 14 we present the typing rules that are relevant to analyzing probabilistic streams, with
the full definition in Appendix D. Constants reference no random variables. sample introduces a
fresh random variable sampled from its argument and adds it to the graph. observe introduces
an intermediate random variable for its first argument by the same mechanism as sample, and
observes it to be the evaluation of its second argument.
Operators and Scalar Folding. We use ğœ‡ğ¹ operators ğ‘œğ‘ to describe probability distributions and
other operations over scalars and assume them to have scalar return values. The auxiliary judgment
â†˜ folds products and stream instances into scalars by taking unions of variable sets.
() â†˜ ( âˆ…, âˆ…) ğ‘Ÿ â†˜ ğ‘Ÿ
ğ‘¡1 â†˜ (lb, ub) ğ‘¡2 â†˜ (lbâ€²
, ubâ€²
)
ğ‘¡1 Ã— ğ‘¡2 â†˜ (lb âˆª lbâ€²
, ub âˆª ubâ€²
)
ğ‘¡ â†˜ (lb, ub)
stream(ğ‘¡, ğ‘ ) â†˜ (lb, ub) bounded â†˜ ( âˆ…, âˆ…)
Sequencing. Sequencing using the let-expression follows the standard typing rule for let, and
also threads the output graph of evaluating ğ‘’ into the evaluation of ğ‘’
â€²
.
() âŠ” () = ()
(ğ‘¡1 Ã— ğ‘¡2) âŠ” (ğ‘¡
â€²
1 Ã— ğ‘¡
â€²
2
) = (ğ‘¡1 âŠ” ğ‘¡
â€²
1
) Ã— (ğ‘¡2 âŠ” ğ‘¡
â€²
2
)
(lb, ub) âŠ” (lbâ€²
, ubâ€²
) = (lb âˆ© lbâ€²
, ub âˆª ubâ€²
)
Fig. 15. Join operator for types.
Conditionals and Join. if-expressions evaluate
the condition, check both branches in parallel, and
join the resulting reference set and graphs. The join
operator âŠ” (Figure 15), representing the conservative union of two types, unions the upper bounds
and intersects the lower bounds. We disallow ifbranching over functions and stream instances.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:20 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Î“ âŠ¢ğ›¼ ğ‘ : (âˆ…, âˆ…)
Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘Ÿ ğ‘‹ = fresh(G)
Î“, G âŠ¢ğ›¼ sample(ğ‘£) : ({ğ‘‹}, {ğ‘‹}), assumeğ›¼ (ğ‘‹, ğ‘Ÿ, G)
Î“, G âŠ¢ğ›¼ sample(ğ‘£1) : ({ğ‘‹}, {ğ‘‹}), G
â€²
Î“ âŠ¢ğ›¼ ğ‘£2 : ğ‘Ÿ2
Î“, G âŠ¢ğ›¼ observe(ğ‘£1,ğ‘£2) : (), observeğ›¼ (ğ‘‹, ğ‘Ÿ2, valueğ›¼ (ğ‘Ÿ2, G
â€²
))
Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘¡ ğ‘¡ â†˜ ğ‘Ÿ
Î“ âŠ¢ğ›¼ op(ğ‘£) : ğ‘Ÿ
Î“, G âŠ¢ğ›¼ ğ‘’ : ğ‘¡, G
â€²
Î“, ğ‘ : ğ‘¡, G
â€²
âŠ¢ğ›¼ ğ‘’
â€²
: ğ‘¡
â€²
, G
â€²â€²
Î“, G âŠ¢ğ›¼ let ğ‘ = ğ‘’ in ğ‘’
â€²
: ğ‘¡
â€²
, G
â€²â€²
Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘Ÿ G
â€² = valueğ›¼ (ğ‘Ÿ, G) Î“, G
â€²
âŠ¢ğ›¼ ğ‘’1 : ğ‘¡1, G1 Î“, G
â€²
âŠ¢ğ›¼ ğ‘’2 : ğ‘¡2, G2
Î“, G âŠ¢ğ›¼ if ğ‘£ then ğ‘’1 else ğ‘’2 : ğ‘¡1 âŠ” ğ‘¡2, G1 âŠ”ğ›¼ G2
Î“ âŠ¢ğ›¼ ğ‘š : (ğ‘¡, ğ‘ )
Î“ âŠ¢ğ›¼ init(ğ‘š) : stream(ğ‘¡, ğ‘ )
Î“ âŠ¢ğ›¼ ğ‘¥ : stream(ğ‘¡, stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’)) Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘¡in Î“ğ‘’, ğ‘state : ğ‘¡, ğ‘in : ğ‘¡in, G âŠ¢ğ›¼ ğ‘’ : ğ‘¡
â€² Ã— ğ‘¡out, G
â€²
Î“, G âŠ¢ğ›¼ unfold(ğ‘¥,ğ‘£) : ğ‘¡out Ã— stream(ğ‘¡
â€²
, stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’)), G
â€²
Î“ âŠ¢mc ğ‘š bounded Î“ âŠ¢up ğ‘š bounded
Î“ âŠ¢ğ›¼ infer(ğ‘š) : bounded
Î“ âŠ¢ğ›¼ ğ‘¥ : bounded Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘¡ ğ‘¡ â†˜ (âˆ…, âˆ…)
Î“ âŠ¢ğ›¼ unfold(ğ‘¥,ğ‘£) : (âˆ…, âˆ…) Ã— bounded
Fig. 14. Delayed sampling type system.
Streams and Inference. To facilitate typing of stream functions, we define the following auxiliary
judgment, which computes, for a stream function, the type of its initial state and the syntactic
fragment for its step function.
Î“ âŠ¢ğ›¼ ğ‘’
â€²
: ğ‘¡init ğ‘¡init â†˜ (âˆ…, âˆ…)
Î“ âŠ¢ğ›¼ stream { init = ğ‘’
â€²
; step(ğ‘state,ğ‘in) = ğ‘’ } : (ğ‘¡init, stepfn(ğ‘state, ğ‘in, Î“, ğ‘’))
Correspondingly, we define the context Î“,ğ‘š : (ğ‘¡init, stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’)) to map the stream
function name ğ‘š to its initial state type and step function.
Instances that are created by init expose the type of their internal state and their step function.
The unfold rule applies the step function to the current state, yielding an output and an instance
with the new state. It ensures that the argument ğ‘£ is compatible with the type of the step function.
An infer expression marks the entry point of a new sub-analysis for its new delayed sampling
graph. The premises of the typing rule for infer are the success conditions for both analyses that
must hold regardless of ğ›¼. This judgment, Î“ âŠ¢ğ›¼ ğ‘š bounded, states that the stream function ğ‘š can be
unfolded for an arbitrary number of iterations while satisfying property ğ›¼ starting with an empty
delayed sampling graph.
Instances created by infer possess a newly instantiated delayed sampling graph. Their internal
state contains the delayed sampling graph and bookkeeping information for the inference algorithm.
Thus, the state is hidden to the exterior and the instance is assigned the opaque type bounded.
unfold on a bounded type only requires that the input and output are purely deterministic.
m-consumed Success Condition. We conclude a stream function passes the ğ‘š-consumed analysis
when all variables that are introduced are consumed by the program. Because an introduced variable
may take several stream iterations to be consumed, we repeatedly execute the analysis until we
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:21
consume all variables and succeed or reach a fixed point and fail. Define the iteration judgment
Î“ âŠ¢ğ›¼ (ğ‘›) ğ‘š : ğ‘¡
â€²
, G, where ğ›¼ is either ğ‘šğ‘ or ğ‘¢ğ‘, as follows:
Î“ âŠ¢ğ›¼ ğ‘š : (ğ‘¡, stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’)) âŠ¢ğ‘ ğ‘in : ğ‘¡in Î“ğ‘’, ğ‘in : ğ‘¡in, ğ‘state : ğ‘¡, âŠ¥ğ›¼ âŠ¢ğ›¼ ğ‘’ : ğ‘¡out Ã— ğ‘¡
â€²
, G
Î“ âŠ¢ğ›¼ (0) ğ‘š : ğ‘¡
â€²
, G
Î“ âŠ¢ğ›¼ ğ‘š : (ğ‘¡, stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’)) Î“ âŠ¢ğ›¼ (ğ‘›âˆ’1) ğ‘š : ğ‘¡
â€²
, G
âŠ¢ğ‘ ğ‘in : ğ‘¡in Î“ğ‘’, ğ‘in : ğ‘¡in, ğ‘state : ğ‘¡
â€²
, G âŠ¢ğ›¼ ğ‘’ : ğ‘¡out Ã— ğ‘¡
â€²â€²
, G
â€²
Î“ âŠ¢ğ›¼ (ğ‘›) ğ‘š : ğ‘¡
â€²â€²
, G
â€²
On each iteration, this judgment applies the appropriate type rule for the step function and returns
the result, using the abstract graph from the previous iteration as the context for the step function
rule. The initial iteration uses an empty abstract graph as the context, represented by âŠ¥ğ›¼ . For the
ğ‘š-consumed analysis, we specialize the judgment to Î“ âŠ¢ğ‘šğ‘ (ğ‘›) ğ‘š : ğ‘¡
â€²
, G, and define âŠ¥mc to be (âˆ…, âˆ…).
The rule continues iterating until it reaches the success condition. The success condition states
that every variable introduced that is kept in the program state must be used with in a bounded
number of time steps. We formalize this as the following type rule:
Î“ âŠ¢ğ‘šğ‘ (0) ğ‘š : ğ‘¡, G ğ‘¡ â†˜ (ğ‘™ğ‘, ğ‘¢ğ‘) Î“ âŠ¢ğ‘šğ‘ (ğ‘›) ğ‘š : ğ‘¡
â€²â€²
, G
â€²
(G.ğ‘–ğ‘› \ Gâ€²
.con) âˆ© ub = âˆ…
Î“ âŠ¢mc ğ‘š bounded
Alternatively, if evaluating one more iteration does not consume any more variables, we reach a
fixed point and return failure. Since every iteration we either consume a variable or reach a fixed
point, the analysis is guaranteed to terminate.
Unseparated Paths Success Condition. Like the ğ‘š-consumed analysis, the unseparated paths
analysis is iterative, and we may need to repeat it for some number of iterations. We specialize the
iteration judgment defined in the previous section to Î“ âŠ¢ğ‘¢ğ‘ (ğ‘›) ğ‘š : ğ‘¡, G and define âŠ¥up to be a pair
of an empty map and an empty set. Define path(ğ‘¡, G) where ğ‘¡ â†˜ (lb, ub) to be the length of the
longest path from any random variable in ub to any other variable in G.ğ‘. Then we conclude the
program passes the unseparated path analysis when the length of the longest path converges after
some finite number of iterations:
Î“ âŠ¢ğ‘¢ğ‘ (ğ‘›) ğ‘š : ğ‘¡, G Î“ âŠ¢ğ‘¢ğ‘ (ğ‘›+(path(ğ‘¡,G)âˆ—size(ğ‘¡))+1) ğ‘š : ğ‘¡
â€²â€²
, G
â€²
path(ğ‘¡, G) = path(ğ‘¡
â€²â€²
, G
â€²
)
Î“ âŠ¢up ğ‘š bounded
The implementation of this rule repeatedly computes a new abstract graph starting from the
previous iterationâ€™s output. It exits when the longest path length at the current iteration is equal to
the longest path after (path(ğ‘¡, G) âˆ— size(ğ‘¡)) + 1 additional iterations. The function size determines,
for a given type ğ‘¡, how many values of base type are contained in ğ‘¡.
size(ğ‘Ÿ) = 1 size(ğ‘¡1 Ã— ğ‘¡2) = size(ğ‘¡1) + size(ğ‘¡2)
The extra iterations ensure that the path length has stabilized and the analysis can safely conclude
that there is a bound on the length of the longest unseparated path.
If the path length check fails, the implementation keeps iterating until a pre-specified bound is
reached. Upon reaching this bound, the implementation outputs an analysis failure. Note that the
analysis may be imprecise and reject correct programs if the bound is not sufficiently high.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:22 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
6.4 Example Type Derivation
This section presents an example type derivation used by the analysis to confirm that the program
in Figure 1 satisfies the ğ‘š-consumed property. In particular, we confirm that the stream function
kalman passes the ğ‘š-consumed analysis.
Using ğ‘’ as shorthand for the body of its step function, we derive the following ğ‘š-consumed
success condition for kalman:
Î“ âŠ¢mc(0) kalman : ( ({ğ‘‹}, {ğ‘‹}) Ã— ({ğ‘‹}, {ğ‘‹})) ( ({ğ‘‹}, {ğ‘‹}) Ã— ({ğ‘‹}, {ğ‘‹})) â†˜ ({ğ‘‹}, {ğ‘‹})
Î“ âŠ¢mc(0) kalman : ( ({ğ‘‹}, {ğ‘‹}) Ã— ({ğ‘‹}, {ğ‘‹})) ({ğ‘‹} \ {ğ‘‹}) âˆ© {ğ‘‹} = âˆ…
Î“ âŠ¢mc kalman bounded
The second and fourth premises follow immediately from definitions and set operations. The
derivation of the first and third premises is as follows:
Î“ âŠ¢mc kalman : ( (âˆ…, âˆ…), stepfn(pre_x, obs, Î“ğ‘’, ğ‘’)) âŠ¢ğ‘ obs : (âˆ…, âˆ…)
Î“ğ‘’, obs : (âˆ…, âˆ…), pre_x : (âˆ…, âˆ…), (âˆ…, âˆ…) âŠ¢mc ğ‘’ : ( ({ğ‘‹}, {ğ‘‹}) Ã— ({ğ‘‹}, {ğ‘‹})), ({ğ‘‹}, {ğ‘‹})
Î“ âŠ¢mc(0) kalman : ( ({ğ‘‹}, {ğ‘‹}) Ã— ({ğ‘‹}, {ğ‘‹}))
The second premise follows from definitions. The derivation of the first premise is as follows:
Î“ğ‘’ âŠ¢mc 0.0 : (âˆ…, âˆ…) (âˆ…, âˆ…) â†˜ (âˆ…, âˆ…)
Î“ âŠ¢mc stream { init = 0.0 ; step(pre_x,obs) = ğ‘’ } : ( (âˆ…, âˆ…), stepfn(pre_x, obs, Î“ğ‘’, ğ‘’))
where the premises follow immediately. Finally, let Î“
â€² be the context Î“ğ‘’, obs : (âˆ…, âˆ…), pre_x : (âˆ…, âˆ…)
and ğ‘¡ be the type (âˆ…, âˆ…). The derivation of the third premise is as follows.
Î“
â€²
, (âˆ…, âˆ…) âŠ¢ğ‘šğ‘ sample(gaussian(pre_x, 1.0)) : ğ‘¡, ({ğ‘‹}, âˆ…)
Î“
â€²
, x : ğ‘¡, ({ğ‘‹}, âˆ…) âŠ¢ğ‘šğ‘ let () = observe(gaussian(x, 1.0),obs) in (x, x) : (ğ‘¡ Ã— ğ‘¡), ({ğ‘‹}, {ğ‘‹})
Î“
â€²
, (âˆ…, âˆ…) âŠ¢ğ‘šğ‘ ğ‘’ : (ğ‘¡ Ã— ğ‘¡), ({ğ‘‹}, {ğ‘‹})
The first premise follows from the typing rule for sample. The second premise follows from the
typing rule for let as follows:
Î“
â€²
, x : ğ‘¡, ({ğ‘‹}, âˆ…) âŠ¢ğ‘šğ‘ observe(gaussian(x, 1.0),obs) : (), ({ğ‘‹}, {ğ‘‹})
Î“
â€²
, x : ğ‘¡, ({ğ‘‹}, {ğ‘‹}) âŠ¢ğ‘šğ‘ (x, x) : (ğ‘¡ Ã— ğ‘¡), ({ğ‘‹}, {ğ‘‹})
Î“
â€²
, x : ğ‘¡, ({ğ‘‹}, âˆ…) âŠ¢ğ‘šğ‘ let () = observe(gaussian(x, 1.0),obs) in (x, x) : (ğ‘¡ Ã— ğ‘¡), ({ğ‘‹}, {ğ‘‹})
where the first premise follows from the rule for observe and the second from the rule for pairs.
6.5 Soundness
Here, we outline how we show the type system is sound. We give a high-level overview of the
approach; the details are in Appendix E.
Entailment Relations. In Appendix E.2, we establish several entailment relations that relate semantic objects to their type-level counterparts. These relations are parameterized by ğ›¼ which is
either ğ‘šğ‘ for the ğ‘š-consumed relation or ğ‘¢ğ‘ for the unseparated path relation. We write ğ‘£ âŠ¨ğ›¼ ğ‘¡
to mean a value entails a type. We write ğ›¾ âŠ¨ğ›¼ Î“ to mean an environment entails a type context.
We write ğ‘£, (ğ‘”, ğœ) âŠ¨ğ›¼ ğ‘¡, G to mean a value and traced graph (see Section 5.2 for the definition of a
traced graph) entail a type and abstract graph. We write ğ›¾, (ğ‘”, ğœ) âŠ¨ğ›¼ Î“, G to mean an environment
and traced graph entail a type context and abstract graph.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:23
Soundness. The following theorems establish the soundness of the type system. The first theorem
states that the type system soundly ascribes types to values and soundly updates the abstract
delayed sampling graph:
Theorem 6.1 (ğ‘š-consumed and Unseparated Path Soundness). If ğ›¾, (ğ‘”, ğœ) âŠ¨ğ›¼ Î“, G and Î“, G âŠ¢ğ›¼
ğ‘’ : ğ‘¡, G
â€² and {[ğ‘’]}ğ›¾
(ğ‘”, ğœ),ğ‘¤ = ğ‘£, (ğ‘”
â€²
, ğœ â€²
),ğ‘¤â€²
, then ğ‘£, (ğ‘”
â€²
, ğœ â€²
) âŠ¨ğ›¼ ğ‘¡, G
â€²
.
Next, the type system soundly ensures a stream function maintains bounded memory.
Theorem 6.2 (Analysis Soundness). If ğ›¾ âŠ¨ğ›¼ Î“ and Î“ âŠ¢ğ›¼ ğ‘š : bounded, then âŸ¦ğ‘šâŸ§ğ›¾
âŠ¨ğ›¼ bounded.
We prove these theorems in Appendix E.2.
6.6 Implementation
We implemented our analysis framework and the ğ‘š-consumed and unseparated paths analyses in
OCaml. Our implementation takes as input a ğœ‡ğ¹ program and outputs either true or false for each
analysis. It also accepts a parameter for the iteration bound for the unseparated paths analysis.
The implementation goes beyond the type system laid out in the paper by supporting functions
that have probabilistic effects as well as interfaces for list and array operations. ğœ‡ğ¹ programs can
further be compiled to OCaml and executed using the ProbZelus delayed sampling runtime. The
code is available at https://github.com/psg-mit/probzelus-analysis-impl.
7 EVALUATION
To evaluate the ability of the analysis to accept only ğœ‡ğ¹ programs that can execute in bounded
memory, we executed it on several benchmarks reflective of real-world inference tasks.
Research Questions. We used our implementation to answer two research questions. For realistic
probabilistic programs, (1) does the type system precisely verify the properties required for boundedmemory execution, and (2) is a small iteration bound sufficient for the unseparated paths analysis?
7.1 Methodology
We executed the analysis on example programs from Baudart et al. [2020] originally written in
ProbZelus, a probabilistic programming language featuring probabilistic data streams and delayed
sampling. We manually translated them to ğœ‡ğ¹ , and they reflect a range of realistic control problems
with different memory usage characteristics. For the unseparated paths analysis, we set an iteration
count bound of 10, which was sufficient for these programs. We compared the outputs of the
analysis to our manual logical reasoning about the ability of each of the following programs to
execute in bounded memory. We provide source code for all benchmarks in Appendix F.
Kalman is the simplified core model of Figure 1 and models an agent that estimates position
from noisy observations. Applying delayed sampling on this model is equivalent to a Kalman filter
[Kalman 1960] where each particle returns the exact solution.
Kalman Hold-First is the example from Figure 3 with a reference to the output of the first iteration.
Gaussian Random Walk is a simplification of Kalman that does not observe of the true position,
effectively expressing a Gaussian random walk.
Robot is the full example from Figure 1 that includes the Kalman core model as well as a main
stream function that invokes a controller based on the inferred position.
Coin models an agent that estimates the bias of a coin. The model chooses the probability of
the coin from a uniform distribution, and thereafter chooses the observations by flipping a coin
with that probability. Applying delayed sampling to this model is equivalent to exact inference in a
Beta-Bernoulli conjugate model [Fink 1997] where each particle returns the exact solution.
Gaussian-Gaussian estimates the mean and variance of a Gaussian distribution.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:24 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Table 1. Bounded memory analysis on benchmark programs.
ğ‘š-consumed unsep. paths bounded mem.
output actual output actual output actual
Kalman âœ“ âœ“ âœ“ âœ“ âœ“ âœ“
Kalman Hold-First âœ“ âœ“ âœ— âœ— âœ— âœ—
Gaussian Random Walk âœ— âœ— âœ“ âœ“ âœ— âœ—
Robot âœ“ âœ“ âœ“ âœ“ âœ“ âœ“
Coin âœ“ âœ“ âœ“ âœ“ âœ“ âœ“
Gaussian-Gaussian âœ“ âœ“ âœ“ âœ“ âœ“ âœ“
Outlier âœ— âœ— âœ“ âœ“ âœ— âœ—
MTT âœ— âœ— âœ“ âœ“ âœ— âœ—
SLAM âœ— âœ“ âœ“ âœ“ âœ— âœ“
Outlier, adapted from Section 2 of [Minka 2001], models the same situation as the Kalman
benchmark, but with a sensor that can occasionally produce invalid readings. The model chooses
the probability of an invalid reading from a beta(100,1000) distribution, so that invalid readings
occur approximately 10% of the time. At each time step, with the previously chosen probability,
the model chooses the observation from either the invalid distribution gaussian(0,100) or the
Kalman model. Applying delayed sampling to this model is equivalent to a Rao-Blackwellized
particle filter [Doucet et al. 2000b] combining exact inference with approximate particle filtering.
MTT (Multi-Target Tracker) is adapted from [Murray and SchÃ¶n 2018] and involves a variable
number of targets with linear-Gaussian 2D position/velocity motion models that produce measurements of position at each time step. The model randomly introduces targets as a Poisson process
and deletes them with fixed probability at each step.
SLAM (Simultaneous Localization and Mapping) is adapted from [Doucet et al. 2000a] and models
an agent that estimates its position on a one-dimensional grid and also a map of its environment
associating each cell with black or white. The robot uses inference to decide its next move, but its
motion commands are noisy with some probability that its wheels may slip, and its observations
may also be incorrectly reported.
7.2 Analysis Results
Table 1 displays the analysis outputs for each of the benchmark programs. For each analysis, the
â€œoutputâ€ column is the result of the implementation, and the â€œactualâ€ column is the ground truth, i.e.,
whether the program satisfies the semantic property according to manual analysis. The â€œbounded
memoryâ€ columns are the logical conjunction of the two semantic properties.
For the first six benchmarks, the analysis implementation yielded the same answer as manual
analysis for whether the program satisfies both semantic properties and thus permits execution in
bounded memory. In every case, the output of the implementation is sound with respect to the
ground truth. Furthermore, all unseparated-path analyses converged within 10 iterations.
Kalman. For this program, every variable is ğ‘š-consumed for ğ‘š â‰¤ 1 and starts an unseparated
path of length at most 1, and thus it can execute in bounded memory.
Kalman Hold-First. For this program, every variable is ğ‘š-consumed for ğ‘š â‰¤ 1. However, the
analysis detects that unseparated paths starting from the initial value for x grow without bound
and fail to converge after 10 iterations, so this program cannot execute in bounded memory.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:25
Gaussian Random Walk. Here, every unseparated path has length at most 1. However, the analysis
detects that there is noğ‘š such that any variable isğ‘š-consumed because no variable is ever observed
or evaluated, so this program cannot execute in bounded memory.
Robot. Every variable is 1-consumed and every separated path has length at most 1. The analysis
succeeds and indicates this program can execute in bounded memory.
Coin. Every variable is 1-consumed and every unseparated path has length at most 1. The analysis
succeeds and indicates this program can execute in bounded memory.
Gaussian-Gaussian. Every variable is 1-consumed and every separated path has length at most 1.
The analysis succeeds and indicates this program can execute in bounded memory.
Outlier. Every unseparated path has length at most 1. However, in the event that samples
are indefinitely considered outliers, no observation will occur that causes the variable xt to be
consumed, so this program cannot execute in bounded memory.
MTT. Every unseparated path has length at most 1. However, not all random variables are
guaranteed to be consumed, as the final observe operation is only executed based on a dynamic
condition on the lengths of two list data structures. Because this condition is not guaranteed to be
met, this program cannot execute in bounded memory.
SLAM. Every unseparated path has length at most 1. The analysis concludes that the environment
map array is not consumed because the model makes random choices that are not guaranteed to
cover all the entries of the map. However, manual examination shows that an entry of the map
that is never covered by a random choice is 0-consumed by virtue of being never used. Thus, the
analysis soundly but imprecisely determines that the ğ‘š-consumed condition fails.
7.3 Discussion
For the Outlier and MTT benchmarks, even though both fail the ğ‘š-consumed semantic property
and therefore are not guaranteed to execute in bounded memory, they will almost certainly execute
in bounded memory. For example, in Outlier, the only way that the memory consumption of the
model will increase indefinitely is if a particular random choice always takes one branch, which is
a probability-zero event. In general, our semantic properties and analysis implementation reason
about the absence of any program execution that yields unbounded memory. However, in practice,
almost certain bounded-memory execution may also be a useful property of programs.
In general, the analysis can provide a sound guarantee that a program executes with bounded
memory. However, as we saw with SLAM, it is not always precise enough such that if it rejects a
program, then the program must have unbounded memory consumption. For example, it is possible
to deliberately construct pathological programs requiring a large number of iterations for the
unseparated paths analysis. Remaining limitations on precision include common static analysis
challenges such as path sensitivity due to if statements and aliasing due to complex data structures.
When facing conditional branches, the analysis takes a conservative approach that may not
utilize all statically available knowledge. Specifically, it cannot determine that certain branches
are taken at most once over the entire input stream or that only certain program paths are valid
over multiple sequential branches. The analysis also cannot accurately track variables that are
stored into complex data structures, meaning it cannot mark them as consumed. We discuss these
challenges in greater detail and provide specific examples in Appendix G.
8 RELATED WORK
Resource Analysis for Probabilistic Programs. Static resource analysis is capable of automatically
determining upper bounds for resources such as time or memory required to execute a probabilistic
program. Ngo et al. [2018] proposed a weakest-precondition approach to determine the expected
memory usage of a probabilistic program, which bounds the number of loop iterations executed
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:26 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
and number of explicit memory allocation ticks encountered. Our analysis, on the other hand,
extends static reasoning to the inherent memory usage of the inference algorithm itself.
Reactive Probabilistic Programming. Gupta et al. [1997] first introduced the idea of reactive
probabilistic programming. They extend a concurrent constraint language with random variables.
In contrast, our language is based on a synchronous dataflow model and focus on resource analysis.
Baudart et al. [2020] developed ProbZelus, a reactive probabilistic programming language which
operates over streams of data and supports inference at each stream iteration. It uses an implementation of delayed sampling designed to provide bounded-memory inference for a class of reactive
probabilistic programs. However, ProbZelus provides no static guarantee of bounded-memory
inference. In this work, we define a language that can be used as a target for the compilation
of ProbZelus and identify the semantic conditions and a static analysis that makes it possible to
provide a static guarantee.
Delayed Sampling and Bounded-Memory Inference. The mechanism of delayed sampling in probabilistic programs was introduced by Murray et al. [2018] and implemented in the Anglican and
Birch programming languages, neither of which supports inference over streams. Delayed sampling,
a form of Sequential Monte Carlo [Liu and Chen 1998], can execute in bounded memory because it
automates the construction of Rao-Blackwellized particle filters [Doucet et al. 2000b], a particularly
efficient variant of SMC. By comparison, Markov chain Monte Carlo techniques generally cannot
execute in bounded memory because they maintain a sample of the full history of program execution, the size of which can grow without bound for a probabilistic stream. Variational inference has
extensions that make it amenable to streaming [Broderick et al. 2013], but we are not aware of any
probabilistic programming system that makes use of them.
Other programming languages such as Hakaru [Narayanan et al. 2016] use static program
transformations to accomplish the same goal of deferring approximate inference as much as possible.
It is unclear if these transformations apply to a streaming context, where dynamic information is
necessary to reflect the evolution of the underlying model over many iterations.
9 CONCLUSION
Probabilistic programming has been augmented by constructs that perform inference over unbounded iterations on streams of data. Underlying this programming model is delayed sampling,
which combines the benefits of exact inference and the flexibility of sampling.
In our paper, we introduce the ğ‘š-consumed and unseparated path semantic properties, which
show that delayed sampling can execute in bounded memory for reactive probabilistic programs. We
present a sound static analysis that verifies these two properties with a type system and an abstract
delayed sampling graph. To the best of our knowledge, our work is the first to develop a resource
analysis for a probabilistic program in relation to its probabilistic programming systemâ€™s underlying inference algorithm. We hope this work will enable automatic inference mechanisms whose
performance is better understood by model developers in probabilistic programming languages.
ACKNOWLEDGMENTS
We would like to thank Cambridge Yang, Alex Renda, Jesse Michel, and Ben Sherman, who all
provided feedback on drafts of this paper. This work was supported in part by the MIT-IBM Watson
AI Lab and the Office of Naval Research (ONR N00014-17-1-2699). Any opinions, findings, and
conclusions or recommendations expressed in this material are those of the authors and do not
necessarily reflect the views of the Office of Naval Research.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:27
REFERENCES
Eric Atkinson, Cambridge Yang, and Michael Carbin. 2018. Verifying Handcoded Probabilistic Inference Procedures. In
arXiv e-prints.
Guillaume Baudart, Louis Mandel, Eric Atkinson, Benjamin Sherman, Marc Pouzet, and Michael Carbin. 2020. Reactive
Probabilistic Programming. In Conference on Programming Language Design and Implementation.
Leonard E. Baum and Ted Petrie. 1966. Statistical Inference for Probabilistic Functions of Finite State Markov Chains. The
Annals of Mathematical Statistics 37, 6 (1966).
Atilim GÃ¼neÅŸ Baydin, Lei Shao, Wahid Bhimji, Lukas Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid
Naderiparizi, Bradley Gram-Hansen, Gilles Louppe, Mingfei Ma, Xiaohui Zhao, Philip Torr, Victor Lee, Kyle Cranmer,
Prabhat, and Frank Wood. 2019. Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale. In
Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC â€™19).
Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh,
Paul Szerlip, Paul Horsfall, and Noah D. Goodman. 2019. Pyro: Deep Universal Probabilistic Programming. Journal of
Machine Learning Research 20, 28 (2019).
Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. 2013. Streaming Variational
Bayes. In International Conference on Neural Information Processing Systems.
Jean-Louis ColaÃ§o, Bruno Pagano, and Marc Pouzet. 2017. SCADE 6: A formal language for embedded critical software
development (invited paper). In TASE. IEEE Computer Society, 1â€“11.
Marco F Cusumano-Towner, Feras A Saad, Alexander K Lew, and Vikash K Mansinghka. 2019. Gen: a General-purpose
Probabilistic Programming System with Programmable Inference. In Conference on Programming Language Design and
Implementation.
Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. 2006. Sequential Monte Carlo samplers. J. Royal Statistical Society: Series
B (Statistical Methodology) 68, 3 (2006), 411â€“436.
Arnaud Doucet, Nando de Freitas, Kevin P. Murphy, and Stuart J. Russell. 2000a. Rao-Blackwellised Particle Filtering for
Dynamic Bayesian Networks. In UAI.
Arnaud Doucet, Nando de Freitas, Kevin P. Murphy, and Stuart J. Russell. 2000b. Rao-Blackwellised Particle Filtering for
Dynamic Bayesian Networks. In Conference on Uncertainty in Artificial Intelligence.
Daniel Fink. 1997. A Compendium of Conjugate Priors. (1997).
Hong Ge, Kai Xu, and Zoubin Ghahramani. 2018. Turing: Composable inference for probabilistic programming. In
International Conference on Artificial Intelligence and Statistics.
Andrew Gelman, Daniel Lee, and Jiqiang Guo. 2015. Stan: A probabilistic programming language for Bayesian inference
and optimization. Journal of Educational and Behavioral Statistics 40, 5 (2015), 530â€“543.
Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B. Tenenbaum. 2008. Church: A
language for generative models. In Conference on Uncertainty in Artificial Intelligence.
Noah D Goodman and Andreas StuhlmÃ¼ller. 2014. The Design and Implementation of Probabilistic Programming Languages.
http://dippl.org. Accessed: 2020-10-30.
Andrew D. Gordon, Thore Graepel, Nicolas Rolland, Claudio Russo, Johannes Borgstrom, and John Guiver. 2014. Tabular: a
schema-driven probabilistic programming language. In Symposium on Principles of Programming Languages.
Vineet Gupta, Radha Jagadeesan, and Vijay A. Saraswat. 1997. Probabilistic Concurrent Constraint Programming. In
CONCUR (Lecture Notes in Computer Science, Vol. 1243). Springer, 243â€“257.
Daniel Huang, Jean-Baptiste Tristan, and Greg Morisett. 2017. Compiling Markov Chain Monte Carlo Algorithms for
Probabilistic Modeling. In Conference on Programming Language Design and Implementation.
R. E. Kalman. 1960. A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering 82, 1 (1960).
Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models - Principles and Techniques. MIT Press.
Jun S. Liu and Rong Chen. 1998. Sequential Monte Carlo Methods for Dynamic Systems. J. Amer. Statist. Assoc. 93, 443
(1998), 1032â€“1044.
Daniel LundÃ©n. 2017. Delayed sampling in the probabilistic programming language Anglican. Masterâ€™s thesis. KTH Royal
Institute of Technology.
Vikash Mansingkha, Ulrich Schaechtle, Shivam Handa, Alexey Radul, Yutian Chen, and Martin Rinard. 2018. Probabilistic
Programming with Programmable Inference. In Conference on Programming Language Design and Implementation.
George H. Mealy. 1955. A method for synthesizing sequential circuits. The Bell System Technical Journal 34, 5 (1955),
1045â€“1079.
Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag, Daniel L. Ong, and Andrey Kolobov. 2007. BLOG: Probabilistic
models with unknown objects. Statistical relational learning (2007).
Thomas P. Minka. 2001. Expectation Propagation for Approximate Bayesian Inference. In Conference in Uncertainty in
Artificial Intelligence.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:28 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Lawrence M. Murray, Daniel LundÃ©n, Jan Kudlicka, David Broman, and Thomas B. SchÃ¶n. 2018. Delayed Sampling and
Automatic Rao-Blackwellization of Probabilistic Programs. In International Conference on Artificial Intelligence and
Statistics.
Lawrence M. Murray and Thomas B. SchÃ¶n. 2018. Automated learning with a probabilistic programming language: Birch.
Annual Reviews in Control 46 (2018).
Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh Shan, and Robert Zinkov. 2016. Probabilistic inference by
program transformation in Hakaru (system description). In International Symposium on Functional and Logic Programming.
Van Chan Ngo, Quentin Carbonneaux, and Jan Hoffmann. 2018. Bounded Expectations: Resource Analysis for Probabilistic
Programs. In Conference on Programming Language Design and Implementation.
Aditya V. Nori, Sherjil Ozair, Sriram K. Rajamani, and Deepak Vijaykeerthy. 2015. Efficient Synthesis of Probabilistic
Programs. In Conference on Programming Language Design and Implementation.
Avi Pfeffer. 2009. Figaro: An object-oriented probabilistic programming language. Vol. 137. 96.
Eduardo D Sontag. 2013. Mathematical control theory: deterministic finite dimensional systems. Vol. 6. Springer Science &
Business Media.
Sam Staton. 2017. Commutative Semantics for Probabilistic Programming. In European Symposium on Programming.
Dustin Tran, Matthew D Hoffman, Rif A Saurous, Eugene Brevdo, Kevin Murphy, and David M Blei. 2017. Deep probabilistic
programming. In International Conference on Learning Representations.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:29
A IDEAL SEMANTICS
In this section we present the complete semantics of the deterministic part of ğœ‡ğ¹ in Figure 16 and
the ideal semantics of the probabilistic part in Figure 17.
The probabilistic semantics of Figure 17 is a measure-based semantics similar to one presented
in [Staton 2017]. Given an environment ğ›¾, an expression is interpreted as a measure {[ğ‘’]}ğ›¾ : Î£ğ· â†’
[0, 1), that is, a function which associates a positive number to each measurable set ğ‘ˆ âˆˆ Î£ğ· , where
Î£ğ· denotes the Î£-algebra of the domain of the expression ğ·, i.e., the set of measurable sets of
possible values. sample(ğ‘£) returns the distribution âŸ¦ğ‘£âŸ§ğ›¾ . observe(ğ‘£1,ğ‘£2) weights execution paths
using the likelihood of the observation âŸ¦ğ‘£2âŸ§ğ›¾ w.r.t. the distribution âŸ¦ğ‘£1âŸ§ğ›¾ (for a distribution ğœ‡ we
note ğœ‡pdf its probability density function). Local definitions are interpreted as integration, and we
use the Dirac delta measure to interpret deterministic expressions.
B CORE TYPES IN ğœ‡ğ¹
This section describes a type system for ğœ‡ğ¹ programs. All programs we consider in this work
must type check according to this system. The type system ensures that if an expression ğ‘’ is
given a probabilistic typing judgment Î“ âŠ¢prob ğ‘’ : ğ‘‡ (which means that ğ‘’ will be evaluated using
its probabilistic semantics {[ğ‘’]} rather than its deterministic semantics âŸ¦ğ‘’âŸ§), then its type ğ‘‡ is a
measurable space that does not include nonmeasurable objects such as functions. The type system
also prohibits nested inference.
The types of ğœ‡ğ¹ are unit, Booleans, reals, functions, and pairs, as well as probability distributions,
and deterministic and probabilistic stream functions and stream instances.
ğ‘‡ ::= unit | bool | real | ğ‘‡ â†’ ğ‘‡ | ğ‘‡ Ã—ğ‘‡ | distr ğ‘‡
| dstreamfn(ğ‘‡ ,ğ‘‡ ) | dstream(ğ‘‡ ,ğ‘‡ ) | pstreamfn(ğ‘‡ ,ğ‘‡ ) | pstream(ğ‘‡ ,ğ‘‡ )
Only a subset of these types may act as the support of probability distributions, denoted by the
judgment measurable(ğ‘‡ ). These exclude function and stream types:
measurable(unit) measurable(bool) measurable(real)
measurable(ğ‘‡1) measurable(ğ‘‡2)
measurable(ğ‘‡1 Ã—ğ‘‡2)
measurable(ğ‘‡ )
measurable(distr ğ‘‡ )
We present the full type system of ğœ‡ğ¹ in Figures 18 and 19.
C DEFINITION OF graft
In this section, we review the definition of graft from Murray et al. [2018].
C.1 Preliminaries
This definition makes use of an alternative type of marginalized node that maintains its own
marginal distribution as well as a conditional distribution that relates the marginalized node to
its unique marginalized child. We use the notation Marginalized(ğœ‡marg, ğœ‡cond ) to refer to such a
marginalized node with marginal distribution ğœ‡marg and conditional distribution ğœ‡cond . We use the
notation ğ‘  = Marginalized(_) to mean that the node state ğ‘  is a marginalized node of any type.
The two types of marginalized nodes only differ in the distributions they store, and have the same
reachability and memory consumption properties.
Murray et al. [2018] defines invariants of delayed sampling runtimes. Namely, it specifies that
delayed sampling maintains that (1) all nodes in the delayed sampling graph have at most one
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:30 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
âŸ¦val ğ‘¥ = ğ‘’âŸ§ğ›¾ = ğ›¾ [ğ‘¥ â† âŸ¦ğ‘’âŸ§ğ›¾ ]
âŸ¦val ğ‘“ = fun ğ‘ -> ğ‘’âŸ§ğ›¾ = ğ›¾ [ğ‘“ â† (ğœ†ğ‘£. âŸ¦ğ‘’âŸ§ğ›¾+ [ğ‘£/ğ‘ ])]
âŸ¦ğ‘‘1 ğ‘‘2âŸ§ğ›¾ = let ğ›¾1 = âŸ¦ğ‘‘1âŸ§ğ›¾ in âŸ¦ğ‘‘2âŸ§ğ›¾1
âŸ¦val ğ‘š = stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }âŸ§ğ›¾
= ğ›¾ [ğ‘š â† stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
]
âŸ¦ğ‘âŸ§ğ›¾ = ğ‘
âŸ¦ğ‘¥âŸ§ğ›¾ = ğ›¾ (ğ‘¥)
âŸ¦(ğ‘£1,ğ‘£2)âŸ§ğ›¾ = (âŸ¦ğ‘£1âŸ§ğ›¾,âŸ¦ğ‘£2âŸ§ğ›¾)
âŸ¦op(ğ‘£)âŸ§ğ›¾ = op(âŸ¦ğ‘£âŸ§ğ›¾ )
âŸ¦ğ‘“ (ğ‘£)âŸ§ğ›¾ = ğ›¾ (ğ‘“ ) (âŸ¦ğ‘£âŸ§ğ›¾ )
âŸ¦let ğ‘ = ğ‘’1 in ğ‘’2âŸ§ğ›¾ = let ğ‘£ = âŸ¦ğ‘’1âŸ§ğ›¾ in âŸ¦ğ‘’2âŸ§ğ›¾+ [ğ‘£/ğ‘ ]
âŸ¦if ğ‘£ then ğ‘’1 else ğ‘’2âŸ§ğ›¾ = if âŸ¦ğ‘£âŸ§ğ›¾ then âŸ¦ğ‘’1âŸ§ğ›¾ else âŸ¦ğ‘’2âŸ§ğ›¾
âŸ¦init(ğ‘š)âŸ§ğ›¾ = let stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
â€² = âŸ¦ğ‘šâŸ§ğ›¾ in
let ğ‘ init = âŸ¦ğ‘’initâŸ§ğ›¾
â€² in
(ğ‘ init, ğœ†(ğ‘ , ğ‘£). âŸ¦ğ‘’âŸ§ğ›¾
â€²+ [ğ‘ /ğ‘state,ğ‘£/ğ‘input ]) if ğ‘’ is deterministic
âŸ¦init(ğ‘š)âŸ§ğ›¾ = let stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
â€² = âŸ¦ğ‘šâŸ§ğ›¾ in
let ğ‘ init = âŸ¦ğ‘’initâŸ§ğ›¾
â€² in
(ğ‘ init, ğœ†(ğ‘ , ğ‘£). {[ğ‘’]}ğ›¾
â€²+ [ğ‘ /ğ‘state,ğ‘£/ğ‘input ]) if ğ‘’ is probabilistic
âŸ¦unfold(ğ‘¥,ğ‘£)âŸ§ğ›¾ = let ğ‘£state, ğ‘“ = âŸ¦ğ‘¥âŸ§ğ›¾ in
let ğ‘£output, ğ‘£â€²
state = ğ‘“ (ğ‘£state, âŸ¦ğ‘£âŸ§ğ›¾ ) in
(ğ‘£output, (ğ‘£
â€²
state, ğ‘“ ))
âŸ¦infer(ğ‘š)âŸ§ğ›¾ = let stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
â€² = âŸ¦ğ‘šâŸ§ğ›¾ in
let ğ‘ init = âŸ¦ğ‘’initâŸ§ğ›¾
â€² in (ğ›¿ğ‘ init , infer(ğœ†(ğ‘ , ğ‘£). {[ğ‘’]}ğ›¾
â€²+ [ğ‘ /ğ‘state,ğ‘£/ğ‘input ]))
where infer(ğ‘“ ) = ğœ†(ğœ, ğ‘£). let ğœ‡ = ğœ†ğ‘ˆ . âˆ«
ğ‘†
ğœ(ğ‘‘ğ‘ )ğ‘“ (ğ‘ , ğ‘£) (ğ‘ˆ ) in
let ğœˆ = ğœ†ğ‘ˆ . ğœ‡(ğ‘ˆ )/ğœ‡(âŠ¤) in
(ğœ‹1âˆ— (ğœˆ), ğœ‹2âˆ— (ğœˆ))
Fig. 16. Deterministic semantics of ğœ‡ğ¹ .
parent, and (2) all marginalized nodes in the graph have at most one marginalized or realized child.
In the following definitions, we use the notation parent(ğ‘‹, ğ¸) to mean a function that returns the
unique parent of ğ‘‹ in the edge set ğ¸. We also use the notation child(ğ‘‹, ğ¸) to mean a function that
returns the unique realized or marginalized child of the marginalized node ğ‘‹ in the edge set ğ¸.
C.2 Definitions
We define the graft function as follows. When called on an initialized node, graft recursively
marginalizes every initialized ancestor of the given node. This means that it performs integration
to incorporate parent information into the distributions of each node in the initialized chain. When
called on a marginalized node, graft calls the prune function.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:31
{[ğ‘£]}ğ›¾ = ğœ†ğ‘ˆ . ğ›¿âŸ¦ğ‘£âŸ§ğ›¾
(ğ‘ˆ )
{[op(ğ‘£)]}ğ›¾ = ğœ†ğ‘ˆ . ğ›¿op(âŸ¦ğ‘£âŸ§ğ›¾ )
(ğ‘ˆ )
{[ğ‘“ (ğ‘£)]}ğ›¾ = ğœ†ğ‘ˆ . ğ›¿ğ›¾ (ğ‘“ ) (âŸ¦ğ‘£âŸ§ğ›¾ )
(ğ‘ˆ )
{[let ğ‘ = ğ‘’1 in ğ‘’2]}ğ›¾ = ğœ†ğ‘ˆ . âˆ«
ğ‘‡
{[ğ‘’1]}ğ›¾ (ğ‘‘ğ‘¢){[ğ‘’2]}ğ›¾+ [ğ‘¢/ğ‘ ] (ğ‘ˆ )
{[if ğ‘£ then ğ‘’1 else ğ‘’2]}ğ›¾ = ğœ†ğ‘ˆ . if âŸ¦ğ‘£âŸ§ğ›¾ then {[ğ‘’1]}ğ›¾ (ğ‘ˆ ) else {[ğ‘’2]}ğ›¾ (ğ‘ˆ )
{[unfold(ğ‘¥,ğ‘£)]}ğ›¾ = ğœ†ğ‘ˆ . let ğ‘£state, ğ‘“ = âŸ¦ğ‘¥âŸ§ğ›¾ in
let ğœ‡ = ğ‘“ (ğ‘£state, âŸ¦ğ‘£âŸ§ğ›¾ ) in
âˆ«
ğœ‡(ğ‘‘ğ‘£output, ğ‘‘ğ‘£â€²
state)ğ›¿(ğ‘£output,(ğ‘£
â€²
state,ğ‘“ )) (ğ‘ˆ )
{[sample(ğ‘£)]}ğ›¾ = ğœ†ğ‘ˆ . âŸ¦ğ‘£âŸ§ğ›¾ (ğ‘ˆ )
{[observe(ğ‘£1,ğ‘£2)]}ğ›¾ = ğœ†ğ‘ˆ . let ğœ‡ = âŸ¦ğ‘£1âŸ§ğ›¾ in ğœ‡pdf(âŸ¦ğ‘£2âŸ§ğ›¾ ) âˆ— ğ›¿() (ğ‘ˆ )
Fig. 17. Probabilistic semantics of ğœ‡ğ¹ .
Î“ âŠ¢det ğ‘’ : ğ‘‡
Î“ âŠ¢decl val ğ‘ = ğ‘’ : Î“, ğ‘ : ğ‘‡
Î“, ğ‘ : ğ‘‡ âŠ¢det ğ‘’ : ğ‘‡
â€²
Î“ âŠ¢decl val ğ‘“ = fun ğ‘ -> ğ‘’ : Î“, ğ‘ : ğ‘‡ â†’ ğ‘‡
â€²
Î“ âŠ¢decl ğ‘‘1 : Î“
â€²
Î“
â€²
âŠ¢decl ğ‘‘2 : Î“
â€²â€²
Î“ âŠ¢decl ğ‘‘1 ğ‘‘2 : Î“
â€²â€²
Î“ âŠ¢det ğ‘’init : ğ‘‡state Î“, (ğ‘state, ğ‘input) : ğ‘‡state Ã—ğ‘‡input âŠ¢det ğ‘’step : ğ‘‡state Ã—ğ‘‡out
Î“ âŠ¢decl val ğ‘š = stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’step } : Î“,ğ‘š : dstreamfn(ğ‘‡input,ğ‘‡out)
Î“ âŠ¢prob ğ‘’init : ğ‘‡state Î“, (ğ‘state, ğ‘input) : ğ‘‡state Ã—ğ‘‡input âŠ¢prob ğ‘’step : ğ‘‡state Ã—ğ‘‡out
Î“ âŠ¢decl val ğ‘š = stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’step } : Î“,ğ‘š : pstreamfn(ğ‘‡input,ğ‘‡out)
Fig. 18. Typing rules for programs in ğœ‡ğ¹ . The judgment Î“ âŠ¢decl ğ‘‘ : Î“
â€² means that the ğœ‡ğ¹ declaration ğ‘‘, when
typed under the typing context Î“, produces the typing context Î“
â€²
.
graft(ğ‘‹, ğ‘”) = let (ğ‘‰ , ğ¸, ğ‘) = ğ‘” in
if ğ‘(ğ‘‹) = Initialized(ğœ‡) then
let ğ‘‹par = parent(ğ‘‹, ğ¸) in
let ğœ‡prior, ğ‘”â€² =
if ğ‘(ğ‘‹par) = Marginalized(ğœ‡par) or
ğ‘(ğ‘‹par) = Initialized(ğœ‡par)
then let (ğ‘‰
â€²â€², ğ¸â€²â€², ğ‘â€²â€²) = graft(ğ‘‹par, ğ‘”) in
let Marginalized(ğœ‡par) = ğ‘
â€²â€²(ğ‘‹par) in
ğœ‡par, (ğ‘‰
â€²â€², ğ¸â€²â€², ğ‘â€²â€²[ğ‘‹par â† Marginalized(ğœ‡par, ğœ‡)])
else if ğ‘(ğ‘‹par) = Realized(ğ‘£) then ğ›¿ (ğ‘£), (ğ‘‰ , ğ¸ âˆ’ (ğ‘‹, ğ‘‹par), ğ‘)
in
let ğœ‡
â€²
, (ğ‘‰
â€²â€², ğ¸â€²â€², ğ‘â€²â€²) =
âˆ«
ğœ‡ dğœ‡prior, ğ‘”â€²
in
(ğ‘‰
â€²â€², ğ¸â€²â€², ğ‘â€²â€²[ğ‘‹ â† Marginalized(ğœ‡
â€²
)])
else if ğ‘(ğ‘‹) = Marginalized(ğœ‡, ğœ‡child ) then prune(ğ‘‹, ğ‘”)
else ğ‘” Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:32 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Î“ âŠ¢det () : unit
ğ‘ âˆˆ {true, false}
Î“ âŠ¢det ğ‘ : bool
ğ‘ âˆˆ R
Î“ âŠ¢det ğ‘ : real
Î“ âŠ¢det ğ‘’1 : ğ‘‡1 Î“ âŠ¢det ğ‘’2 : ğ‘‡2
Î“ âŠ¢det (ğ‘’1,ğ‘’2) : ğ‘‡1 Ã—ğ‘‡2
Î“, ğ‘1 : ğ‘‡1, ğ‘2 : ğ‘‡2 âŠ¢k ğ‘’ : ğ‘‡
â€²
Î“, (ğ‘1, ğ‘2) : ğ‘‡1 Ã—ğ‘‡2 âŠ¢k ğ‘’ : ğ‘‡
â€²
Î“, ğ‘¥ : ğ‘‡ âŠ¢k ğ‘¥ : ğ‘‡
Î“ âŠ¢det ğ‘’ : ğ‘‡ Î“ âŠ¢k ğ‘“ : ğ‘‡ â†’ ğ‘‡
â€²
Î“ âŠ¢k ğ‘“ (ğ‘’) : ğ‘‡
â€²
Î“ âŠ¢det ğ‘’1 : bool Î“ âŠ¢k ğ‘’2 : ğ‘‡ Î“ âŠ¢k ğ‘’3 : ğ‘‡
Î“ âŠ¢k if ğ‘’1 then ğ‘’2 else ğ‘’3 : ğ‘‡
Î“ âŠ¢k ğ‘’1 : ğ‘‡1 Î“, ğ‘ : ğ‘‡1 âŠ¢k ğ‘’2 : ğ‘‡2
Î“ âŠ¢k let ğ‘ = ğ‘’1 in ğ‘’2 : ğ‘‡2
Î“ âŠ¢det ğ‘š : dstreamfn(ğ‘‡input,ğ‘‡out)
Î“ âŠ¢det init(ğ‘š) : dstream(ğ‘‡input,ğ‘‡out)
Î“ âŠ¢det ğ‘š : pstreamfn(ğ‘‡input,ğ‘‡out)
Î“ âŠ¢det infer(ğ‘š) : pstream(ğ‘‡input,ğ‘‡out)
Î“ âŠ¢det ğ‘’1 : dstream(ğ‘‡input,ğ‘‡out) Î“ âŠ¢det ğ‘’2 : ğ‘‡input
Î“ âŠ¢k unfold(ğ‘’1,ğ‘’2) : ğ‘‡out Ã— dstream(ğ‘‡input,ğ‘‡out)
Î“ âŠ¢det ğ‘’1 : pstream(ğ‘‡input,ğ‘‡out) Î“ âŠ¢det ğ‘’2 : ğ‘‡input
Î“ âŠ¢k unfold(ğ‘’1,ğ‘’2) : distr ğ‘‡out Ã— pstream(ğ‘‡input,ğ‘‡out)
Î“ âŠ¢det ğ‘’ : ğ‘‡ measurable(ğ‘‡ )
Î“ âŠ¢prob ğ‘’ : ğ‘‡
Î“ âŠ¢det ğ‘’ : distr ğ‘‡
Î“ âŠ¢prob sample(ğ‘’) : ğ‘‡
Î“ âŠ¢det ğ‘’1 : distr ğ‘‡ Î“ âŠ¢det ğ‘’2 : ğ‘‡
Î“ âŠ¢prob observe(ğ‘’1,ğ‘’2) : unit
Fig. 19. Deterministic and probabilistic type systems for ğœ‡ğ¹ . The typing judgment Î“ âŠ¢det ğ‘’ : ğ‘‡ means that the
ğœ‡ğ¹ expression ğ‘’ under the context Î“ has the deterministic type ğ‘‡ . The judgment Î“ âŠ¢prob ğ‘’ : ğ‘‡ means that the
ğœ‡ğ¹ expression ğ‘’ under context Î“ has the probabilistic type ğ‘‡ . The judgment Î“ âŠ¢k
ğ‘’ : ğ‘‡ stands for either the
deterministic or the probabilistic judgment, where ğ‘˜ is instantiated to be ğ‘‘ğ‘’ğ‘¡ or ğ‘ğ‘Ÿğ‘œğ‘. These rules state that
sample and observe can only be used inside the body of a probabilistic stream.
We define the prune function as follows. When called on a marginalized node with a marginalized
or realized child, the function first recursively prunes that child if the child itself is marginalized. If
the node is marginalized, it samples a value for that node and then conditions the current node on
the child taking on that value. If the child node is realized, the function proceeds to immediately
condition the current node on the child nodeâ€™s value.
In either case, the conditioning proceeds as follows. The prune function first extracts probability density functions from the relevant measures using the pdf function. It then follows Bayesâ€™
rule, multiplying the prior and conditional density functions and normalizing the result with the
normalize function. It finally updates the marginal distribution of the given node and removes the
edge connecting the node to its child.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:33
prune(ğ‘‹, ğ‘”) = let (ğ‘‰ , ğ¸, ğ‘) = ğ‘” in
if ğ‘(ğ‘‹) = Marginalized(ğœ‡ğ‘‹, ğœ‡) then
let ğ‘‹child = child(ğ‘‹, ğ¸) in
let ğ‘”
â€² = prune(ğ‘‹child, ğ‘”) in
if ğ‘(ğ‘‹child ) = Marginalized(ğœ‡child ) then
let ğ‘£, (ğ‘‰
â€²â€², ğ¸â€²â€², ğ‘â€²â€²) = value(ğ‘‹child, ğ‘”â€²
) in
let ğ‘ğ‘‹, ğ‘child |ğ‘‹ = pdf (ğœ‡ğ‘‹ ), pdf (ğœ‡) in
let ğœ‡
â€²
ğ‘‹
= normalize(ğœ†ğ‘¥.ğ‘ğ‘‹ (ğ‘¥) âˆ— ğ‘child |ğ‘‹ (ğ‘£|ğ‘¥)) in
let ğ‘
â€²â€²â€² = ğ‘
â€²â€²[ğ‘‹child â† Realized(ğ‘£), ğ‘‹ â† Marginalized(ğœ‡
â€²
ğ‘‹
)] in
(ğ‘‰
â€²â€², ğ¸â€²â€² âˆ’ (ğ‘‹, ğ‘‹child ), ğ‘â€²â€²â€²)
else if ğ‘(ğ‘‹child ) = Realized(ğ‘£) then
let ğ‘£, (ğ‘‰
â€²â€², ğ¸â€²â€², ğ‘â€²â€²) = ğ‘”
â€²
in
let ğ‘ğ‘‹, ğ‘child |ğ‘‹ = pdf (ğœ‡ğ‘‹ ), pdf (ğœ‡) in
let ğœ‡
â€²
ğ‘‹
= normalize(ğœ†ğ‘¥.ğ‘ğ‘‹ (ğ‘¥) âˆ— ğ‘child |ğ‘‹ (ğ‘£|ğ‘¥)) in
let ğ‘
â€²â€²â€² = ğ‘
â€²â€²[ğ‘‹ â† Marginalized(ğœ‡
â€²
ğ‘‹
)] in
(ğ‘‰
â€²â€², ğ¸â€²â€² âˆ’ (ğ‘‹child, ğ‘‹), ğ‘â€²â€²â€²)
else ğ‘”
else ğ‘”
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:34 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
D COMPLETE ANALYSIS TYPE SYSTEM
The following is the complete definition of the typing judgment Î“, G âŠ¢ğ›¼ ğ‘’ : ğ‘¡, G
â€² describing the
types and abstract graph transitions of expressions.
Î“ âŠ¢ğ›¼ ğ‘ : (âˆ…, âˆ…) Î“, ğ‘¥ : ğ‘¡ âŠ¢ğ›¼ ğ‘¥ : ğ‘¡
Î“,ğ‘š : (ğ‘¡init, stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’)) âŠ¢ğ›¼ ğ‘š : (ğ‘¡init, stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’))
Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘Ÿ ğ‘‹ = fresh(G)
Î“, G âŠ¢ğ›¼ sample(ğ‘£) : ({ğ‘‹}, {ğ‘‹}), assumeğ›¼ (ğ‘‹, ğ‘Ÿ, G)
Î“, G âŠ¢ğ›¼ sample(ğ‘£1) : ({ğ‘‹}, {ğ‘‹}), G
â€²
Î“ âŠ¢ğ›¼ ğ‘£2 : ğ‘Ÿ2
Î“, G âŠ¢ğ›¼ observe(ğ‘£1,ğ‘£2) : (), observeğ›¼ (ğ‘‹, ğ‘Ÿ2, valueğ›¼ (ğ‘Ÿ2, G
â€²
))
Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘¡ ğ‘¡ â†˜ ğ‘Ÿ
Î“ âŠ¢ğ›¼ op(ğ‘£) : ğ‘Ÿ
Î“, G âŠ¢ğ›¼ ğ‘’ : ğ‘¡, G
â€²
Î“, ğ‘ : ğ‘¡, G
â€²
âŠ¢ğ›¼ ğ‘’
â€²
: ğ‘¡
â€²
, G
â€²â€²
Î“, G âŠ¢ğ›¼ let ğ‘ = ğ‘’ in ğ‘’
â€²
: ğ‘¡
â€²
, G
â€²â€²
Î“ âŠ¢ğ›¼ ğ‘£1 : ğ‘¡1 Î“ âŠ¢ğ›¼ ğ‘£2 : ğ‘¡2
Î“ âŠ¢ğ›¼ (ğ‘£1,ğ‘£2) : ğ‘¡1 Ã— ğ‘¡2
Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘Ÿ G
â€² = valueğ›¼ (ğ‘Ÿ, G) Î“, G
â€²
âŠ¢ğ›¼ ğ‘’1 : ğ‘¡1, G1 Î“, G
â€²
âŠ¢ğ›¼ ğ‘’2 : ğ‘¡2, G2
Î“, G âŠ¢ğ›¼ if ğ‘£ then ğ‘’1 else ğ‘’2 : ğ‘¡1 âŠ” ğ‘¡2, G1 âŠ”ğ›¼ G2
âŠ¢ğ‘ ğ‘ : ğ‘¡ Î“, ğ‘ : ğ‘¡ âŠ¢ğ›¼ ğ‘’ : ğ‘¡
â€²
Î“ âŠ¢ğ›¼ fun ğ‘ -> ğ‘’ : ğ‘¡ â†’ ğ‘¡
â€²
Î“ âŠ¢ğ›¼ ğ‘“ : ğ‘¡ â†’ ğ‘¡
â€²
Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘¡
Î“ âŠ¢ğ›¼ ğ‘“ (ğ‘£) : ğ‘¡
â€²
Î“ âŠ¢ğ›¼ ğ‘š : (ğ‘¡, ğ‘ )
Î“ âŠ¢ğ›¼ init(ğ‘š) : stream(ğ‘¡, ğ‘ )
Î“ âŠ¢ğ‘šğ‘ ğ‘š bounded Î“ âŠ¢ğ‘¢ğ‘ ğ‘š bounded
Î“ âŠ¢ğ›¼ infer(ğ‘š) : bounded
Î“ âŠ¢ğ›¼ ğ‘¥ : stream(ğ‘¡, stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’))
Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘¡in Î“ğ‘’, ğ‘state : ğ‘¡, ğ‘in : ğ‘¡in, G âŠ¢ğ›¼ ğ‘’ : ğ‘¡
â€² Ã— ğ‘¡out, G
â€²
Î“, G âŠ¢ğ›¼ unfold(ğ‘¥,ğ‘£) : ğ‘¡out Ã— stream(ğ‘¡
â€²
, stepfn(ğ‘state, ğ‘in, Î“ğ‘’, ğ‘’)), G
â€²
Î“ âŠ¢ğ›¼ ğ‘¥ : bounded Î“ âŠ¢ğ›¼ ğ‘£ : ğ‘¡ ğ‘¡ â†˜ (âˆ…, âˆ…)
Î“ âŠ¢ğ›¼ unfold(ğ‘¥,ğ‘£) : (âˆ…, âˆ…) Ã— bounded
ğœ‡ğ¹ programs consist of a series of value, function, and stream function declarations. Thus, we also
define a top-level judgment Î“ âŠ¢ğ›¼ ğ‘‘ :: program that states that a ğœ‡ğ¹ program ğ‘‘ contains declarations
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:35
that are all well-formed. This judgment is defined as follows:
Î“ âŠ¢ğ›¼ ğœ– :: program
Î“, G âŠ¢ğ›¼ ğ‘’ : ğ‘¡, G
â€²
Î“, ğ‘ : ğ‘¡ âŠ¢ğ›¼ ğ‘‘ :: program
Î“ âŠ¢ğ›¼ val ğ‘ = ğ‘’;ğ‘‘ :: program
Î“, G âŠ¢ğ›¼ fun ğ‘ -> ğ‘’ : ğ‘¡, G
â€²
Î“, ğ‘“ : ğ‘¡ âŠ¢ğ›¼ ğ‘‘ :: program
Î“ âŠ¢ğ›¼ val ğ‘“ = fun ğ‘ -> ğ‘’;ğ‘‘ :: program
Î“ âŠ¢ğ›¼ stream { init = ğ‘’
â€²
; step(ğ‘state,ğ‘in) = ğ‘’ } : (ğ‘¡init, stepfn(ğ‘state, ğ‘in, Î“, ğ‘’))
Î“,ğ‘š : (ğ‘¡init, stepfn(ğ‘state, ğ‘in, Î“, ğ‘’)) âŠ¢ğ›¼ ğ‘‘ :: program
Î“ âŠ¢ğ›¼ val ğ‘š = stream { init = ğ‘’
â€²
; step(ğ‘state,ğ‘in) = ğ‘’ };ğ‘‘ :: program
where if ğ‘‘ is empty (i.e. all streams including main are valid) the judgment holds trivially.
E SOUNDNESS
E.1 Executions
During the execution of a program, the only constructs that can dynamically allocate memory
are sample and observe which add a new node to the delayed sampling graph using the assume
operation. These two probabilistic constructs can only be used in a model, i.e., the argument of the
infer operator. We thus focus on the memory footprint of inferâ€™s transition function.
The execution of the transition function infer of infer comprises three steps (see Section 4.1):
(1) draw a set of particles, i.e., pairs (state, graph), (2) execute the model for each particle, (3) extract
the distributions of state and outputs. The only operation that can dynamically allocate memory is
the second one, where the delayed sampling graph can be altered.
At iteration ğ‘›, for each particle, the current pair (state, graph) is obtained from a succession of
application of the model transition function from the initial state ğ‘ 0 and an empty graph ğ‘”0 = âˆ…
(step (1) in the definition of infer can only drop some execution paths). We call this sequence
(ğ‘ 0, ğ‘”0), (ğ‘ 1, ğ‘”1), . . . an execution of the model. The following properties states that if only boundedmemory execution are possible, then the infer function executes in bounded-memory.
Lemma E.1 (Execution Sufficiency). For all stream functions ğ‘š and environments ğ›¾, let
stream { init = ğ‘’init ; step(ğ‘state,ğ‘input) = ğ‘’ }ğ›¾
â€² = âŸ¦ğ‘šâŸ§ğ›¾ and let ğ‘ ğ‘š = âŸ¦ğ‘’initâŸ§ğ›¾
â€² and let ğ‘“ğ‘š =
ğœ†(ğ‘ , ğ‘£). {[ğ‘’]}ğ›¾+ [ğ‘ /ğ‘state,ğ‘£/ğ‘input ] and let ğ‘ ğ‘–
, ğ‘“ğ‘– = âŸ¦infer(ğ‘š)âŸ§ğ›¾ . We say that ğ‘“ğ‘– produces a sequence
of distributions (ğœ‡ğ‘›)ğ‘›âˆˆN given an input sequence (ğ‘–ğ‘›)ğ‘›âˆˆN if ğ‘“ğ‘–(ğœ‡ğ‘›,ğ‘–ğ‘›) = (ğœ”ğ‘›, ğœ‡ğ‘›+1) for some sequence of output distributions (ğœ”ğ‘›)ğ‘›âˆˆN. Similarly, we say ğ‘“ğ‘š produces the execution (ğ‘”ğ‘›, ğ‘ ğ‘›)ğ‘›âˆˆN if
ğ‘“ğ‘š (ğ‘ ğ‘›,ğ‘–ğ‘›) (ğ‘”ğ‘›, 1) = ( (ğ‘œğ‘›, ğ‘ ğ‘›+1),ğ‘¤ğ‘›, ğ‘”ğ‘›+1) for some sequences of outputs (ğ‘œğ‘›)ğ‘›âˆˆN and weights (ğ‘¤ğ‘›)ğ‘›âˆˆN.
The lemma states that for all input sequences (ğ‘–ğ‘›)ğ‘›âˆˆN, if ğ‘“ğ‘– produces the sequence (ğœ‡ğ‘›)ğ‘›âˆˆN, then for
all ğ‘› and (ğ‘”ğ‘›, ğ‘ ğ‘›) âˆˆ support(ğœ‡ğ‘›), there exists an execution (ğ‘”
â€²
ğ‘›
, ğ‘ â€²
ğ‘›
)ğ‘›âˆˆN such that ğ‘”
â€²
ğ‘› = ğ‘”ğ‘›, ğ‘ 
â€²
ğ‘› = ğ‘ ğ‘›, and
ğ‘“ğ‘š produces (ğ‘”
â€²
ğ‘›
, ğ‘ â€²
ğ‘›
)ğ‘›âˆˆN.
Proof. Proceed by induction on ğ‘›. If ğ‘› = 0, the distribution ğœ‡0 is obtained by the execution
of ğ‘“ğ‘š by each particle on the initial state and the empty graph. So the support of ğœ‡0 is obtained
by the execution of ğ‘“ğ‘š. If ğ‘› > 0, by definition of infer, each pair (ğ‘”ğ‘›, ğ‘ ğ‘›) from the support of the
distribution ğœ‡ğ‘› is obtained by the application of ğ‘“ğ‘š on (ğ‘”ğ‘›âˆ’1, ğ‘ ğ‘›âˆ’1) drawn from the distribution
ğœ‡ğ‘›âˆ’1. By application of the induction hypothesis, (ğ‘”ğ‘–
, ğ‘ ğ‘–)0â‰¤ğ‘–<ğ‘› is an execution produced by ğ‘“ğ‘š, and
therefore (ğ‘”ğ‘–
, ğ‘ ğ‘–)0â‰¤ğ‘– â‰¤ğ‘› is also produced by ğ‘“ğ‘š. â–¡
Corollary E.2. If all executions of ğ‘“ğ‘š are bounded-memory, then for any input sequence (ğ‘–ğ‘›)ğ‘›âˆˆN
and any execution (ğ‘”ğ‘›, ğ‘ ğ‘›)ğ‘›âˆˆN such that for all ğ‘›, every (ğ‘”ğ‘›, ğ‘ ğ‘›) âˆˆ support(ğœ‡ğ‘›), (ğ‘”ğ‘›, ğ‘ ğ‘›)ğ‘›âˆˆN has bounded
memory when ğœ‡ğ‘› is produced by ğ‘“ğ‘–
.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:36 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
E.2 Type Soundness
In this section, we show that the type system is sound. We first define the âŠ¨ relations referenced in
Section 6.5. We then prove the soundness theorems stated in Section 6.5.
Variable Mappings. Both delayed sampling and the type system use a set of fresh variable names
to label random variables. Because the type system and the delayed sampling execution may each
use a different name for conceptually the same random variable, we define an association that maps
between these namespaces. We use the notation â„“ to refer to a function that maps a delayed sampling
variable to a type system variable, and â„“âˆ— to the extension of â„“ to sets: â„“âˆ— (ğ‘‹Ë†) = {â„“(ğ‘‹) | ğ‘‹ âˆˆ ğ‘‹Ë† }.
E.2.1 Entailment. Here we establish what it means for a value to entail a type. A value entails a
type if the type accurately captures the random variables the value could refer to, as well as the
shape of the value (i.e. whether the value is a scalar, a pair, or a stream function). Because step
function types include type contexts, we also establish what it means for an environment to entail
a type context.
A stream value entails bounded if it produces a sequence of states in which every delayed
sampling graph is bounded. We formalize this as follows. Given a sequence of inputs (ğ‘–ğ‘›)ğ‘›âˆˆN and
an initial state ğ‘ 0, we say a stream function ğ‘“ produces the sequence of state (ğ‘ ğ‘›)ğ‘›âˆˆN on (ğ‘ 0,ğ‘–), if
ğ‘“ (ğ‘–ğ‘›, ğ‘ ğ‘›) = (ğ‘œğ‘›, ğ‘ ğ‘›+1) for some output sequence (ğ‘œğ‘›)ğ‘›âˆˆN. We say ğ‘ , is bounded if every sequence
delayed sampling graphs contained in ğ‘  is low-level bounded-memory.
Definition E.3 (Type Entailment). A value ğ‘£ entails a type ğ‘¡, written ğ‘£ âŠ¨
â„“
ğ‘¡, under the following
circumstances:
ğ‘ âŠ¨
â„“
(âˆ…, ub)
ğ‘‹ âŠ¨
â„“
(lb, ub) â‡â‡’ lb âŠ† {â„“(ğ‘‹)} âŠ† ub
app(op, ğ‘£) âŠ¨
â„“
(lb, ub) â‡â‡’ lb âŠ† â„“âˆ— (frv(ğ‘£)) âŠ† ub
(ğ‘£1,ğ‘£2) âŠ¨
â„“
ğ‘¡1 Ã— ğ‘¡2 â‡â‡’ ğ‘£1 âŠ¨
â„“
ğ‘¡1 and ğ‘£2 âŠ¨
â„“
ğ‘¡2
stream { init = ğ‘’init ; step(ğ‘in,ğ‘state) = ğ‘’state }ğ›¾ğ‘’
âŠ¨
â„“
(ğ‘¡init, stepfn(ğ‘in, ğ‘state, Î“ğ‘’, ğ‘’state)) â‡â‡’ ğ‘’init âŠ¨
â„“
ğ‘¡init âˆ§ğ›¾ğ‘’ âŠ¨
â„“
Î“ğ‘’
(ğ‘ 0, ğ‘“ ) âŠ¨
â„“
stream(ğ‘¡init, ğ‘†) â‡â‡’ ğ‘ 0 âŠ¨
â„“
ğ‘¡init and ğ‘“ âŠ¨
â„“
ğ‘†
ğ‘“ âŠ¨
â„“
stepfn(ğ‘in, ğ‘state, Î“ğ‘’, ğ‘’) â‡â‡’ âˆƒğ›¾ . ğ›¾ âŠ¨
â„“
Î“ğ‘’ and ğ‘“ = ğœ†(ğ‘ , ğ‘£). {[ğ‘’]}ğ›¾+ [ğ‘ /ğ‘state,ğ‘£/ğ‘in ]
ğ›¾ âŠ¨
â„“
Î“ â‡â‡’ âˆ€ğ‘¥ : ğ‘¡ âˆˆ Î“. ğ›¾ (ğ‘¥) = ğ‘£ s.t. ğ‘£ âŠ¨
â„“
ğ‘¡
(ğ‘ 0, ğ‘“ ) âŠ¨ bounded â‡â‡’ âˆ€i. ğ‘“ produces ğ‘  on (ğ‘ 0,ğ‘–)
â‡’ ğ‘  is bounded
We further define a version of type entailment that only applies to a restricted set of variables.
Definition E.4 (Restricted Type Entailment). A value ğ‘£ entails a type ğ‘¡ â€“ restricted to the variable
set ğ‘‹Ë†, written ğ‘£ âŠ¨
â„“
ğ‘‹Ë†
ğ‘¡, under the following circumstances:
ğ‘ âŠ¨
â„“
ğ‘‹Ë†
(âˆ…, ub)
ğ‘‹ âŠ¨
â„“
ğ‘‹Ë†
(lb, ub) â‡â‡’ ğ‘‹ âˆˆ ğ‘‹Ë† â‡’ lb âŠ† {â„“(ğ‘‹)} âŠ† ub
app(op, ğ‘£) âŠ¨
â„“
ğ‘‹Ë†
(lb, ub) â‡â‡’ ğ‘‹ âˆˆ ğ‘‹Ë† â‡’ lb âŠ† â„“âˆ— (frv(ğ‘£)) âŠ† ub
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:37
The rules for any other values are similar to those in Definition E.3, but pass the set ğ‘‹Ë† through
unchanged for recursive definitions.
The fold operation â†˜ is designed to generate a scalar type that encapsulates the free variables
of a value while disregarding its shape.
Lemma E.5 (Fold Entailment). If ğ‘£ âŠ¨
â„“
ğ‘¡ and ğ‘¡ â†˜ (lb, ub), then lb âŠ† â„“âˆ— (frv(ğ‘£)) âŠ† ub.
A traced graph entails an ğ‘š-consumed abstract graph if the abstract graph soundly approximates
the variables that are not used.
Definition E.6 (ğ‘š-consumed Graph Entailment). A traced graph (ğ‘”, ğœ) entails an ğ‘š-consumed
abstract graph G, written (ğ‘”, ğœ) âŠ¨
â„“
mc G, if for every variable ğ‘‹ not in G.in \ G.con and for every ğ‘‹0
such that â„“(ğ‘‹0) = ğ‘‹, ğ‘‹0 is used in ğœ.
A traced graph entails an unseparated-path abstract graph if the path function soundly approximates the unseparated paths in the traced graph and the separator set soundly approximates the
set of variables that are observed or valued.
Definition E.7 (Unseparated Path Graph Entailment). A graph (ğ‘”, ğœ) entails an unseparated-path
abstract graph G, written (ğ‘”, ğœ) âŠ¨
â„“
up G if for every ğ‘‹1, ğ‘‹2 that are referenced in ğœ, G.ğ‘(â„“(ğ‘‹1), â„“(ğ‘‹2))
is at least the length of the unseparated path between ğ‘‹1 and ğ‘‹2 in ğœ, and, for all ğ‘‹ referenced in ğœ,
G.ğ‘ ğ‘’ğ‘(â„“(ğ‘‹)) is only true if ğ‘‹ is a separator in ğœ.
Entailment from Section 6.5. Here, we define the entailment relations that are referenced in
Section 6.5. These definitions are defined in terms of the relevant definitions in this section with
the variable map â„“ existentially quantified:
ğ‘£ âŠ¨ğ›¼ ğ‘¡ â‡â‡’ âˆƒâ„“. ğ‘£ âŠ¨
â„“
ğ‘¡
ğ›¾ âŠ¨ğ›¼ Î“ â‡â‡’ âˆƒâ„“. ğ›¾ âŠ¨
â„“
Î“
ğ‘£, (ğ‘”, ğœ) âŠ¨ğ›¼ ğ‘¡, G â‡â‡’ âˆƒâ„“. ğ‘£ âŠ¨
â„“
ğ‘¡ âˆ§ (ğ‘”, ğœ) âŠ¨
â„“
ğ›¼ G
ğ›¾, (ğ‘”, ğœ) âŠ¨ğ›¼ Î“, G â‡â‡’ âˆƒâ„“. ğ›¾ âŠ¨
â„“
Î“ âˆ§ (ğ‘”, ğœ) âŠ¨
â„“
ğ›¼ G
We further extend these definitions to incorporate a restricted variable set ğ‘‹Ë†.
ğ‘£ âŠ¨ğ›¼,ğ‘‹Ë† ğ‘¡ â‡â‡’ âˆƒâ„“. ğ‘£ âŠ¨
â„“
ğ‘‹Ë†
ğ‘¡
ğ›¾ âŠ¨ğ›¼,ğ‘‹Ë† Î“ â‡â‡’ âˆƒâ„“. ğ›¾ âŠ¨
â„“
ğ‘‹Ë†
Î“
ğ‘£, (ğ‘”, ğœ) âŠ¨ğ›¼,ğ‘‹Ë† ğ‘¡, G â‡â‡’ âˆƒâ„“. ğ‘£ âŠ¨
â„“
ğ‘‹Ë†
ğ‘¡ âˆ§ (ğ‘”, ğœ) âŠ¨
â„“
ğ›¼ G
ğ›¾, (ğ‘”, ğœ) âŠ¨ğ›¼,ğ‘‹Ë† Î“, G â‡â‡’ âˆƒâ„“. ğ›¾ âŠ¨
â„“
ğ‘‹Ë†
Î“ âˆ§ (ğ‘”, ğœ) âŠ¨
â„“
ğ›¼ G
E.2.2 Soundness Theorems. Anğ‘š-consumed type judgment is sound if it abstracts theğ‘š-consumed
property of the semantics according to the entailment relations.
Special Case of Theorem 6.1 (ğ‘š-consumed Type Soundness). If ğ›¾, (ğ‘”, ğœ) âŠ¨mc Î“, G and Î“, G âŠ¢mc
ğ‘’ : ğ‘¡, G
â€² and {[ğ‘’]}ğ›¾
(ğ‘”, ğœ),ğ‘¤ = ğ‘£, (ğ‘”
â€²
, ğœ â€²
),ğ‘¤â€²
, then ğ‘£, (ğ‘”
â€²
, ğœ â€²
) âŠ¨mc ğ‘¡, G
â€²
Proof. By structural induction on derivations of âŠ¢mc. â–¡
Proving the soundness of the ğ‘š-consumed judgment producing the bounded type requires
strengthening this theorem to work with partial traces, meaning the abstract graph applies only to
the tail end of the trace rather than the whole trace. Using the notation ğœ1 âŠ• ğœ2 to mean the trace ğœ1
appended with ğœ2, we formalize this as follows:
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:38 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
Lemma E.8. ğ‘š-consumed Soundness on Partial Traces If ğ›¾, (ğ‘”, ğœ2) âŠ¨mc Î“, G and Î“, G âŠ¢mc ğ‘’ : ğ‘¡, G
â€²
and {[ğ‘’]}ğ›¾
(ğ‘”, ğœ1 âŠ• ğœ2),ğ‘¤ = ğ‘£, (ğ‘”
â€²
, ğœ â€²
),ğ‘¤â€²
, then ğœ
â€² = ğœ1 âŠ• ğœ
â€²
2
and ğ‘£, (ğ‘”
â€²
, ğœ â€²
2
) âŠ¨mc ğ‘¡, G
â€²
Proof. By structural induction on derivations of âŠ¢mc. The individual steps are the same as the
previous theorem, except that they also use the associativity of âŠ•. â–¡
An unseparated-path type judgment is sound if it abstracts the unseparated path property of the
semantics according to the entailment relations.
Special Case of Theorem 6.1 (Unseparated Path Type Soundness). If ğ›¾, (ğ‘”, ğœ) âŠ¨up Î“, G and
Î“, G âŠ¢up ğ‘’ : ğ‘¡, G
â€² and {[ğ‘’]}ğ›¾
(ğ‘”, ğœ),ğ‘¤ = ğ‘£, (ğ‘”
â€²
, ğœ â€²
),ğ‘¤â€²
, then ğ‘£, (ğ‘”
â€²
, ğœ â€²
) âŠ¨up ğ‘¡, G
â€²
We also strengthen this theorem to aid in proving the soundness of the bounded judgment.
Lemma E.9. Unseparated Paths Soundness on Partial Traces If ğ›¾, (ğ‘”, ğœ1 âŠ• ğœ3) âŠ¨up,frv (ğœ1âŠ•ğœ3) Î“, G
and Î“, G âŠ¢up ğ‘’ : ğ‘¡, G
â€² and {[ğ‘’]}ğ›¾
(ğ‘”, ğœ1 âŠ• ğœ2 âŠ• ğœ3),ğ‘¤ = ğ‘£, (ğ‘”
â€²
, ğœ â€²
),ğ‘¤â€²
, then ğœ
â€² = ğœ1 âŠ• ğœ2 âŠ• ğœ
â€²
3
and
ğ‘£, (ğ‘”
â€²
, ğœ1 âŠ• ğœ
â€²
3
) âŠ¨up,frv (ğœ1âŠ•ğœ
â€²
3
)
ğ‘¡, G
â€²
Proof. By structural induction on derivations of âŠ¢up. The individual steps are the same as the
previous theorem, except that they also use the associativity of âŠ•. â–¡
Theorem E.10 (Analysis Soundness). If ğ›¾ âŠ¨ğ›¼ Î“ and Î“ âŠ¢ğ›¼ ğ‘š : bounded, then âŸ¦ğ‘šâŸ§ğ›¾
âŠ¨ğ›¼ bounded.
Proof. We first show that any execution of a stream function ğ‘š satisfying Î“ âŠ¢mc ğ‘š bounded
satisfies the high-level ğ‘š-consumed semantic property. We then show that any execution of a
stream function ğ‘š satisfying Î“ âŠ¢up ğ‘š bounded satisfies the high-level unseparated paths semantic
property. Then, by Theorem 5.11 and Lemma E.1, if ğ‘š satisfies both these properties, then calling
infer on ğ‘š must be bounded.
ğ‘š-consumed. Here, we show that any execution of a stream functionğ‘š satisfying Î“ âŠ¢mc ğ‘š bounded
satisfies the ğ‘š-consumed semantic property. We show this using the definition of âŠ¢mc. Let (ğ‘”ğ‘–
, ğœğ‘–)
be the ğ‘–th step of the execution. By Lemma E.8, G captures all variables introduced at time ğ‘–. Also
by Lemma E.8, G
â€²
captures the variables that are guaranteed to be consumed between ğ‘– and ğ‘– + ğ‘›.
Thus, any variable introduced at time step ğ‘– must either be consumed within ğ‘› steps (where ğ‘› is
a static bound) or must is not stored in the program state. If it is consumed, the variable will be
ğ‘š-consumed at all future time steps where ğ‘š is at most ğ‘› times a constant bound based on the
number of sample statements in the stream function. If it is not stored in the program state, it can
never be used and is therefore always 0-consumed.
Unseparated Paths. We proceed by contradiction. Assume that ğ‘ ğ‘–
, (ğ‘”ğ‘–
, ğœğ‘–)ğ‘– âˆˆN is an execution that
violates the unseparated paths semantic property. At some time step ğ‘—, the execution must a) add
a variable to the delayed sampling graph in such a way that it increases the unseparated path
starting from some variable in the graph, and b) store the variable starting the increased path in ğ‘ ğ‘—+1.
Otherwise, the execution would easily satisfy the property. According to Lemma E.9, we must have
that after each iteration the abstract graph also has a variable with starting an increased path and
that some reference ğ‘Ÿ
âˆ—
contained in the type ğ‘¡
â€²
references this variable. Letting ğ‘…Ë† be the set of all
possible references ğ‘Ÿ
âˆ—
, by the pidgeonhole principle, after ğ‘˜ â‰¥ size(ğ‘¡) = size(ğ‘¡
â€²
) â‰¥ |ğ‘…Ë†| iterations,2
the longest path in the abstract graph starting from a variable referenced by an element of ğ‘…Ë† must
have increased by at least 1. Similarly, after ğ‘› â‰¥ path(ğ‘¡, G) instances of this pattern, the longest
path in the abstract graph starting from a variable referenced in ğ‘…Ë† must have increased by at least
path(G, ğ‘¡) and thus be the longest such graph starting from a state variable. This contradicts the
termination condition that path(ğ‘¡, G) = path(ğ‘¡
â€²â€²
, G
â€²
). â–¡
2
the equality of size(ğ‘¡) and size(ğ‘¡
â€²
) is enforced by the type rules in Figure 18 of Appendix B
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:39
F BENCHMARKS
Each of these benchmarks are followed by a main stream that serves as the entry point of the
program:
val main = stream {
init = infer f;
step (f, args) = unfold (f, args)
}
F.1 Kalman
val f = stream {
init = 0.;
step (pre_x, obs) =
let x = sample (gaussian (pre_x, 1.0)) in
let () = observe (gaussian (x, 1.0), obs) in
(x, x)
}
F.2 Kalman Hold-First
val kalman = stream {
init = (true, 0., 0.);
step ((first, i, pre_x), obs) =
let (i, pre_x) =
if first then (let i = sample (gaussian(0., 1.)) in (i, i))
else (i, pre_x) in
let x = sample (gaussian (pre_x, 1.)) in
let () = observe (gaussian (x, 1.), obs) in
(x, (false, i, x))
}
F.3 Gaussian Random Walk
val f = stream {
init = (true, 0.);
step ((first, x), ()) =
let x = if first then sample (gaussian (0., 1.)) else sample (gaussian (x, 1.)) in
(x, (false, x))
}
F.4 Coin
val f = stream {
init = (true, 0.);
step ((first, xt), yobs) =
let xt = if first then sample (beta (1., 1.)) else xt in
let () = observe (bernoulli (xt), yobs) in
(xt, (false, xt))
}
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:40 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
F.5 Outlier
val f = stream {
init = (true, 0., 0.);
step ((first, xt, outlier_prob), yobs) =
let (xt, outlier_prob) =
if first then
(sample (gaussian (0., 100.)), sample (beta (100., 1000.)))
else (sample (gaussian (xt, 1.)), outlier_prob) in
let is_outlier = sample (bernoulli (outlier_prob)) in
let () =
if is_outlier then (observe (gaussian (0., 100.), yobs))
else (observe (gaussian (xt, 1.), yobs)) in
(xt, (false, xt, outlier_prob))
}
F.6 MTT
val f = stream {
init = (true, List.nil);
step ((first, t), (inp, cmd)) =
let last_t = t in
let t_survived =
List.filter (fun (_, _) -> eval (sample (bernoulli (0.5))), last_t) in
let n_new = sample (poisson (1.0)) in
let t_new = List.init (n_new, fun _ -> (0, sample (bernoulli (0.5)))) in
let t_tot = List.append (t_survived, t_new) in
let t = List.map (fun (tr_num, tr) -> (tr_num, sample (bernoulli (tr))), t_tot) in
let obs = List.map (fun (_, tr) -> bernoulli (tr), t) in
let n_clutter = sub (List.length (inp), List.length (obs)) in
let () = observe (poisson (0.5), n_clutter) in
let clutter = List.init (n_clutter, fun _ -> bernoulli (tr)) in
let obs_shuffled = sample (shuffle (List.append (obs, clutter))) in
let () =
if (not (lt (n_clutter, 0))) then
List.iter2 (fun (var, value) ->
observe (gaussian (0.5, var), value), obs_shuffled, inp)
else () in
(t, (false, t))
}
F.7 SLAM
val f = stream {
init = (true, 0., Array.empty);
step ((first, x, map), (obs, cmd)) =
let map =
if first then Array.init (100, fun _ -> sample (bernoulli (0.5))) else map in
let wheel_slip = sample (bernoulli (0.5)) in
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
Statically Bounded-Memory Delayed Sampling for Probabilistic Streams 115:41
let x = if first then 0. else if wheel_slip then x else plus (x, cmd) in
let o = Array.get (map, x) in
let _ = observe (bernoulli (ite (o, 0.9, 0.1)), obs) in
((x, map), (false, x, map))
}
G PRECISION LIMITATIONS
The precision of the analysis is limited by path and complex sensitivity, two common challenges
for static analysis. The analysis can be overly conservative when facing conditional branches, for
example in the following snippet:
let x = sample bernoulli(0.5) in
let y = sample gaussian (0., 1.) in
let () = if x then observe (gaussian (y, 1.), 1.) else () in
let () = if x then () else observe (gaussian (y, 1.), -1.) in
y
According to the analysis, y is not consumed because each branch is separately and conservatively
judged to not consume y, even though there is no path where y is unobserved. A more sophisticated
analysis that reasons about actual values, not just affected variables, would be more precise here.
Similarly, the analysis can be imprecise in the presence of complex data such as tuples. Consider
the following snippet:
let x = sample (gaussian(0., 1.)) in
let y = sample (gaussian(0., 1.)) in
let (a, b) =
if (sample (bernoulli(0.5))) then
(gaussian (x, 1.), gaussian (y, 1.))
else (gaussian (y, 1.), gaussian (x, 1.)) in
let () = observe (a, 1.) in
let () = observe (b, 2.) in
(x, y)
Like the previous example, x, y are not considered consumed even though there is no path that
does not observe both. The analysis can determine that both a and b may reference x and y but
neither alone must do so. Knowledge about a and b taken as a pair is lost when they are stored into
the tuple. In this case, some kind of alias or shape analysis might recover the relationship between
the fields of a tuple.
Without executing for multiple iterations, the ğ‘š-consumed analysis would be occasionally too
conservative due to requiring that all variables be used before the end of the current iteration of
the step function. Consider:
stream {
init = 0.;
step (x_prev, obs) =
let _ = observe (gaussian (x_prev, 1.), obs) in
let x = sample (gaussian (x_prev, 1.)) in
(x, x)
}
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
115:42 Eric Atkinson, Guillaume Baudart, Louis Mandel, Charles Yuan, and Michael Carbin
In this example, every sample is eventually consumed but only on the subsequent iteration of
the step function. If the ğ‘š-consumed analysis only considered one iteration, it would reject this
example. Allow introduced variables to be consumed over multiple iterations as we do allows this
example to pass the analysis.
Most examples do not require a significant number of iterations for the unseparated paths
analysis to converge. However, the analysis may fail to detect convergence in programs with many
variables if the iteration bound parameter is too low, as in the following program which requires
four iterations:
stream {
init = (0., 0., 0., 0.);
step ((x_p, x_pp, x_ppp, x_pppp), obs) =
let x = sample (gaussian (x_p, 1.)) in
let _ = observe (gaussian (x, 1.), 1.0) in
(x_pppp, (x, x_p, x_pp, x_ppp))
}
In this program, the longest unseparated path increases over four iterations, after which variables
start being dropped from the state and the maximum length converges. We suggest that the
parameter should be set to be comfortably larger than the number of variables or statements in the
program to avoid this issue. Since each iteration is fast to run, it should not cause performance
degradation.
Finally, the analysis could incorporate higher-order functions, though they would be hard to
analyze statically, and the storage of chains of closures built over many iterations could itself violate
a bound on memory usage.
Proc. ACM Program. Lang., Vol. 5, No. OOPSLA, Article 115. Publication date: October 2021.
