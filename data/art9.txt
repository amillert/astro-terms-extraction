Machine learning synthetic spectra for probabilistic redshift estimation:
ABSTRACT
Photometric redshift estimation algorithms are often based on representative data from observational campaigns. Data-driven
methods of this type are subject to a number of potential deficiencies, such as sample bias and incompleteness. Motivated by these
considerations, we propose using physically motivated synthetic spectral energy distributions in redshift estimation. In addition,
the synthetic data would have to span a domain in colour-redshift space concordant with that of the targeted observational
surveys. With a matched distribution and realistically modelled synthetic data in hand, a suitable regression algorithm can be
appropriately trained; we use a mixture density network for this purpose. We also perform a zero-point re-calibration to reduce the
systematic differences between noise-free synthetic data and the (unavoidably) noisy observational data sets. This new redshift
estimation framework, syth-z, demonstrates superior accuracy over a wide range of redshifts compared to baseline models
trained on observational data alone. Approaches using realistic synthetic data sets can therefore greatly mitigate the reliance on
expensive spectroscopic follow-up for the next generation of photometric surveys.
Key words: methods: statistical â€“ galaxies: distances and redshifts â€“ galaxies: statistics
1 INTRODUCTION
Ongoing and near-future photometric surveys such as the Dark Energy Survey (DES, Abbott et al. 2005), the Kilo-Degree Survey
(KiDS, de Jong et al. 2013), the Hyper Suprime-Cam Survey (HSC,
Aihara et al. 2018a,b), Euclid (LaureÄ³s et al. 2011), the Vera C. Rubin
Observatoryâ€™s Legacy Survey of Space and Time (LSST, Abell et al.
2009), and the Roman Space Telescope (Green et al. 2012) encounter
several hurdles that were not faced by previous, more shallow, surveys such as the Sloan Digital Sky Survey (SDSS, York et al. 2000).
As galaxy catalogues continue to be substantially larger in size and
fainter in magnitude, spectroscopic follow-ups to obtain confirmation of galactic properties become prohibitively expensive (Newman
et al. 2015a; Stanford et al. 2021). This necessitates alternatives to
spectroscopic follow-ups via synthetic modeling, using a combination of stellar population synthesis (SPS) models and star formation
histories (SFHs) from parametric models (e.g., Alsing et al. 2020),
semi-analytic models (e.g., Pacifici et al. 2012), or hybrid approaches
combining empirical and semi-analytic models (e.g., LSST Dark Energy Science Collaboration (LSST DESC) et al. 2021).
Photometric redshift estimation of individual galaxies typically
relies upon data either in the form of Spectral Energy Distributions
(SED) templates for matching (FernÃ¡ndez-Soto et al. 1999; BenÃ­tez
2000a), or as training sets for machine learning algorithms (Firth
â˜… E-mail:nramachandra@anl.gov
et al. 2003). The supervised learning algorithms are designed to find
a mapping between sets of dependent variables based on a training
set with input-target examples. In an ideal case, these algorithms can
predict outputs (like galaxy redshifts) for combinations of input variables (like galaxy colours) never previously encountered, therefore
generalizing training data to unseen situations. However, in a more
realistic situation, supervised learning algorithms inherit biases presented in the training data. Therefore, to prevent these biases, it is
mandatory to optimise supervised learning models using training
sets encompassing all cases expected to be found in the envisaged
application scenario.
The majority of machine learning (ML) algorithms designed to
infer photometric redshifts use observational photometric data for
training (Carrasco Kind & Brunner 2013a, 2014; Graff et al. 2014;
Almosallam et al. 2016a; Almosallam et al. 2016b; Cavuoti et al.
2016; Sadeh et al. 2016; Graham et al. 2018; Fadikar et al. 2021).
This approach is not ideal, however, because spectroscopic surveys
can only estimate redshifts at the bright end of sources detected
by photometric surveys, given their significantly slower observing
speeds. Furthermore, aiming to reduce the level of star-galaxy misclassification, most spectroscopic surveys incorporate strict selection
criteria on colour space that result in not being able to observe some
galaxy types. Taken together, these issues may lead to unrealistic
predictions for galaxy types, especially faint galaxies with uncommon properties. One approach to overcome biases originating from
Â© 2021 The Authors
arXiv:2111.12118v1 [astro-ph.CO] 23 Nov 2021
2 Ramachandra et al.
incomplete observations is to employ stellar population synthesis
models to generate more complete training data sets.
The viability of stellar population synthesis modelling in photometric redshift estimation is currently explored in template fitting
codes. For instance, HernÃ¡n-Caballero et al. (2021) use the stellar population synthesis code cigale (Boquien et al. 2019) with
delayed-exponential star formation histories to create templates, and
then introduce these in a customized version of the template-fitting
code lephare (Arnouts et al. 1999) to compute photometric redshifts
for galaxies of the miniJPAS survey (Bonoli et al. 2021), which observed ' 1 deg2
of the northern sky using the 56 narrow-band filters
of the J-PAS survey (Benitez et al. 2014). A similar approach is
also utilized in the COSMOS-2020 (Weaver et al. 2021) catalogue
or the PAUS survey (Alarcon et al. 2021), where log-normal star
formation histories are used to create 17 generated synthetic SEDs
from Flexible Stellar Population Synthesis (fsps, Conroy et al. 2009;
Conroy & Gunn 2010). These are also utilized in the template fitting
code eazy (Brammer et al. 2008) to estimate the redshift. Both these
approaches, while demonstrating the usefulness of synthetic SEDs,
also highlight the need for more realistic galaxy formation models in
the future surveys of LSST, Euclid and SPHEREx (DorÃ© et al. 2014).
Furthermore, expanding the template set to include more realistic
and diverse galaxy formation physics often leads to an unaffordable
increase in compute time for current and future surveys.
Another issue with photometric redshift estimation is that the mapping is not an injective function, due to the relatively sparse input
dimension, resulting in degeneracies. Hence a simple point-estimate
for a galaxyâ€™s â€œphoto-zâ€ is less desirable than a probability distribution function (PDF) that is conditioned on the observed photometric colours, i.e, ğ‘(ğ‘§|ğ‘) (Mandelbaum et al. 2008; Almosallam
et al. 2016b; BenÃ­tez 2000b; Izbicki et al. 2017; Carrasco Kind &
Brunner 2013b), and possibly other properties, such as morphology
and galaxy clustering. However, a full Bayesian posterior estimation
of ğ‘(ğ‘§|ğ‘) may not be practically feasible with traditional statistical
approaches. This is primarily due to nontrivial issues with scaling
the inference problem to billions of galaxies. Despite having a robust forward model and massive parallelization abilities, obtaining
full redshift posteriors for individual galaxies is still prohibitively
expensive.
Artificial neural networks could be used as inference machines
as alternatives to traditional posterior estimation techniques, by sampling over the model parameters of network weights and biases (Neal
2012). However, the numerical expense of sampling over a large number of model parameters can be numerically expensive, as is the case
with deep neural networks. Consequently, neural networks employing Bayesian inference techniques for robust uncertainty estimates
(Mackay 1995; Gal 2016; Wenzel et al. 2020) are an active field of
research.
It should also be noted that for photometric redshift estimation
and downstream analyses, a simpler conditional density estimator
that quantifies the uncertainty in the predictions with respect to the
data-prior suffices for most purposes. Approximating the likelihoods
using parameterized models could further reduce the computational
expense. This could provide the much-needed scalability to billions
of galaxies, whilst providing robust uncertainty estimations for redshift predictions. Hence we adopt this approach in our photometric redshift estimation by approximating the conditional distribution
ğ‘(ğ‘§|ğ‘) using a mixture of Gaussian distributions. Our redshift estimation algorithm syth-z is constructed with both the above considerations: the synthesis of physically modelled mock photometric data,
and adaptation of synthetic data in a probabilistic framework that is
scalable to a large number of galaxies. In this paper, we show that
with robust physical modelling and meticulous sampling, we may
achieve high levels of accuracy in photo-z estimation over a baseline
of observation-based training.
The methodology described above is a hybrid approach that
bridges aspects of both ML and template methods of photometric redshift estimation. The ML interpretation is straightforward â€“
weights of a neural network are optimized to learn the mapping between broad-band photometry and redshifts. From a template-fitting
methods perspective, syth-z is built upon complex empirical star formation histories (SFHs) and SEDs, and these are approximated using
an interpolation technique. This novel combination makes syth-z a
powerful framework for predicting galactic redshifts.
The paper is organized as follows. The process of creating the
synthetic galaxy colours is introduced in Section 2.1. Details of the
data from the photometric surveys of SDSS, VIPERS and DEEP2 are
described in Section 2.2. In Section 3 we explain the probabilistic
machine learning algorithm used for computing the redshift PDF,
given galaxy colours. We also introduce zero-point adjustments performed to reduce the systematic differences between synthetic and
observed colours. In Section 4, we demonstrate the advantage of
using synthetically-trained photometric redshift estimation against a
baseline training scheme of using survey data for training. We discuss
our findings and summarize the major results in Section 5.
Throughout this work, we use extinction-corrected magnitudes in
the AB system and Planck 2015 cosmological parameters (Planck
Collaboration et al. 2014): Î©m = 0.314, Î©Î› = 0.686, Î©b = 0.049,
ğœ8 = 0.83, â„0 = 0.67, and ğ‘›s = 0.96.
2 TRAINING AND VALIDATION DATA
We begin with the curation of the training and validation data used
throughout this paper. In contrast to the traditional machine learning
frameworks, the testing or validation set is different from the training data. That is, the syth-z training is entirely carried out with the
synthetic data, and â€˜out-of-setâ€™ data is used for benchmarking. In this
section, we begin by presenting the process of modelling synthetic
training data from stellar population synthesis codes. Next, we tabulate the observational galaxy samples used for validating the photo-ğ‘§
algorithm introduced in Section 3.2.
2.1 Synthetic spectral energy distributions
Modelling galaxy SEDs is a long-standing problem in astronomy
due to the dependence of the light of a galaxy on several properties
and their correlations, such as star formation history, metallicity, dust
type and abundance, and gas properties. The most common approach
to predict galaxy SEDs is by using stellar population synthesis (SPS)
models, which rely upon stellar evolution theory to produce precise
galaxy spectral energy distributions in the ultraviolet, optical, and
infrared ranges (for a review see Conroy 2013). Throughout this
work, we generate galaxy colours using the SPS model fsps (Conroy
et al. 2009; Conroy & Gunn 2010) together with its python interface
(Foreman-Mackey et al. 2014), which include a plethora of parameters controlling galaxy properties. Our strategy is to produce synthetic
galaxy photometry approximately spanning the same region of the
colour space as galaxies from observations (see Section 2.2).
Producing synthetic colours representative of the colours of the
observed galaxies is challenging due to the large diversity of properties influencing colours. For example, Pacifici et al. (2015) show that
precise modelling of star formation histories, metallicity enrichment
MNRAS 000, 1â€“14 (2021)
Synthetic spectra for redshift estimation: SYTH-Z 3
histories, dust attenuation, and nebular emission is necessary to produce colours covering the same region of the colour space as observed
galaxies. Among these properties, the star formation history presents
the strongest influence on broadband colours (e.g., Chaves-Montero
& Hearin 2020). The two main approaches for modelling SFHs are
parametric functional forms and star formation rates tabulated at a
set of cosmic times. Sufficiently flexible parametric models capture
SFHs predicted by simulations with reasonable precision (Simha
et al. 2014; Diemer et al. 2017) and accommodate episodic bursts
of star formation or other transient features. But simple models may
fail to do so entirely (Lower et al. 2020). Another crucial concern of
the parametric models is that they may produce SFHs that are not
physically motivated.
Motivated by the above issues, we use SFHs predicted by galaxy
formation models to produce physically-motivated colours. The
main drawback of this approach is that the results become modeldependent. To alleviate this problem, we draw SFHs from two models
that reproduce a broad range of summary statistics of galaxy populations but follow completely different approaches to simulate galaxies.
The first is the empirical model universemachine (Behroozi et al.
2019), which simulates galaxies using a set of scaling relations between galaxy and halo properties. We generate universemachine
SFHs by running the publicly available version of the code1 on
merger trees identified in the Bolshoi-Planck simulation with Rockstar and Consistent trees (Behroozi et al. 2013a,b; Klypin et al. 2016;
RodrÃ­guez-Puebla et al. 2016) up to redshift ğ‘§ = 0 and 1. The second is based on the cosmological hydrodynamical simulation IllustrisTNG (Marinacci et al. 2018; Naiman et al. 2018; Nelson et al.
2018a; Pillepich et al. 2018; Springel et al. 2018), which models
the joint evolution of dark matter, gas, stars, and supermassive black
holes by incorporating a comprehensive galaxy formation model
with radiative gas cooling, star formation, galactic winds, and AGN
feedback. In particular, we draw IllustrisTNG SFHs from the largest
hydrodynamical simulation of the suite, TNG300-3 (Nelson et al.
2018b).
Due to the non-parametric nature of SFHs drawn from universemachine and IllustrisTNG, it is challenging to select a set
of SFHs representative of a whole galaxy population. To facilitate
this process, we label each SFH by the maximum mass ever attained
by its host halo, ğ‘€peak, and the slope of the host halo mass accretion
history, ğ›½MAH. It is natural to use ğ‘€peak because galaxies hosted by
more massive haloes present larger masses, reach the peak of their
SFH at higher redshift, and become quenched at earlier times (e.g.,
Chaves-Montero & Hearin 2020). We use ğ›½MAH because this parameter captures the dependence of galaxy properties on the assembly
history of its host halo; for example, at fixed halo mass, the galaxies
with a steeper slope reach the peak of their SFH at higher redshift
and become quenched at earlier times (Montero-Dorta et al. 2021).
We also model the impact of three other galaxy properties influencing colours: metallicity, dust attenuation, and nebular emission.
We use a time-independent parameter ğ‘ to model the stellar and gas
metallicity, ğ›¾ğœISM
ğ‘‰
and ğœ
ISM
ğ‘‰
to specify the dust attenuation of light
coming from stars younger and older than 10 Myr (Charlot & Fall
2000), respectively, and log10 ğ‘ˆğ‘† to set the logarithm of zero-age
ionization at the StrÃ¶mgren radius. Therefore, our model presents six
free parameters: ğ‘ and log10 ğ‘ˆğ‘† are defined at 22 and 7 values within
the ranges ğ‘ âˆˆ [0.0002, 0.03] and log10 ğ‘ˆğ‘† âˆˆ [âˆ’4, âˆ’1] respectively,
ğ›¾ and ğœ
ISM
ğ‘‰
are continuous to within the intervals ğ›¾ âˆˆ [1, 4] and
ğœ
ISM
ğ‘‰
âˆˆ [0, 1.5], and ğ‘€peak and ğ›½MAH vary to within the ranges
1 https://bitbucket.org/pbehroozi/universemachine/
predicted by universemachine and IllustrisTNG. We note that we
also use a Chabrier initial mass function (Chabrier 2003) to predict
colours.
To produce galaxy colours, we begin by sampling the model parameters according to a Latin hypercube design (McKay et al. 1979)
defined by the priors specified above. To reduce the impact of shortterm star formation fluctuations on colours, we compute the average
of the 20 SFHs with closest ğ‘€peak and ğ›½MAH to the sampled values.
Note that the resulting SFHs are more representative of the colours
of galaxy populations (Chaves-Montero & Hearin 2021). Then, we
use fsps to generate colours at 50 randomly selected redshifts from
ğ‘§ = 0.002 to 1.25 for each combination of model parameters. In this
manner, we end up producing synthetic colours forâˆ¼200 000 different
galaxies. In Appendix A, we compare the spectral energy distribution
of some of these galaxies and that of galaxies from observations.
2.2 Data from observations
The standard approach to evaluate the quality of photometric redshifts is to use spectroscopic redshift estimates as ground truth,
which is motivated by the much better precision of spectroscopic
redshifts relative to photometric redshifts. Nonetheless, the precision
of spectroscopic redshifts decreases significantly for low signal-tonoise sources; consequently, this technique may lead to catastrophic
redshift solutions for faint sources. In this section, we describe the
main properties of the galaxies with secure spectroscopic redshift
estimates that are used to validate our methodology.
In order to benchmark syth-z with a broadly representative sample of galaxies with spectroscopic redshifts and broadband photometry, we collate publicly available spectroscopic sources from the
Sloan Digital Sky Survey (SDSS, York et al. 2000; Eisenstein et al.
2011; Blanton et al. 2017), the VIMOS Public Extragalactic Redshift
Survey (VIPERS, Scodeggio et al. 2018), and the DEEP2 Redshift
Survey (DEEP2, Newman et al. 2013; Matthews et al. 2013). We
detail the properties of each of the samples below, having tabulated
the primary statistics in Table 1.
(i) SDSS: This sample comprises the 1,911,919 galaxies from
the SDSS database2 presenting the spectroscopic redshifts with precision superior to Î”ğ‘§spec = 0.003, extinction-corrected ğ‘–-band magnitude brighter than 20 mag, ğ‘–-band magnitude error smaller than
0.1 mag, and clean photometry in all bands. For these sources, we
use extinction-corrected magnitudes from the public database. These
galaxies present redshifts typically smaller than ğ‘§spec = 0.6.
(ii) VIPERS: We select the 40,718 galaxies from the second
public data release of the VIPERS survey3 with secure redshifts.
VIPERS galaxies present redshifts slightly larger than the SDSS
galaxies, but typically smaller than ğ‘§spec = 1.
(iii) DEEP2: We use the 13,163 galaxies from the fourth data release of DEEP24 with spectroscopic redshifts. These galaxies present
redshifts covering approximately the same redshift range as VIPERS
galaxies.
Some galaxies from the VIPERS and DEEP2 surveys present
2 SDSS data are available from: http://skyserver.sdss.org/dr15/
en/tools/search/sql.aspx
3 VIPERS data are available from: http://vipers.inaf.it/rel-pdr2.
html
4 DEEP2 data are available from: http://deep.ps.uci.edu/DR4/home.
html
MNRAS 000, 1â€“14 (2021)
4 Ramachandra et al.
Table 1. Properties of the spectroscopic galaxy samples that we use for model
validation. The columns i95%
max and ğ‘§
95%
max indicate the ğ‘–-band magnitude and
redshift of the galaxy in the 95th percentile of each of these properties.
Survey Number i95%
max [mag] ğ‘§
95%
max
SDSS 1 911 919 19.8 0.64
VIPERS 40 718 22.5 0.97
DEEP2 13 163 22.9 0.97
photometric data taken by the Canada-France-Hawaii Legacy Survey (CFHTLS, Gwyn 2012). This surveyâ€™s filter transmission curves
have slightly different band-passes relative to the SDSS filters. To
avoid biases arising from such differences, we transform the photometry of these galaxies from the CFHTLS into the SDSS filter
system using Equations (1-5) from (Matthews et al. 2013). We also
correct the magnitude of all these galaxies for Galactic extinction.
3 MODELLING AND FRAMEWORK
Unlike the majority of ML-based photo-z estimation codes, our
framework is trained on noiseless, simulated data. The discrepancy
in training and testing samples requires calibrations and validations
beyond standard training and hold-out testing routines. In this section, we describe the major components in the syth-z framework that
aim to address these concerns: experimental design and synthesis of
training data, construction and training of the probabilistic mapping
algorithm, and survey-specific systematic offset corrections.
3.1 Colour distribution of observational and synthetic galaxies
The architecture of data-driven ML methods, e.g., artificial neural
networks, is usually agnostic to the underlying physics. Only the
training data includes the intrinsic information about the input and
target distributions, and the underlying relationships between them.
Thus any incorrect modelling or biases in the training data will
be learned by the network. This is particularly important when the
training data is created using simulations â€“ the assumptions in the
synthetic modelling may create intractable biases in the estimations.
In our work, the mixture density network (MDN) learns the information about mapping between the colours and redshift entirely from
the training data. Since the broadband colours for training syth-z are
synthetically generated via the steps prescribed in Section 2.1, the
validity of synthetic data has to be meticulously checked. Primarily,
the coverage of synthetic data has to be compared to that of observational samples. In the colour-redshift space, if the distribution
training set does not significantly overlap with that of the observational data, the resulting extrapolation is highly prone to biases in the
redshift estimations.
In Figure 1, we display the distribution of colours and spectroscopic redshifts (ğ‘¢ âˆ’ ğ‘”, ğ‘” âˆ’ ğ‘Ÿ, ğ‘Ÿ âˆ’ ğ‘–, ğ‘– âˆ’ ğ‘§, ğ‘–-band, ğ‘§spec) projected
onto 2D slices. The first, second, and third panels show the colour
distribution of observed and synthetic data at 0.0 < ğ‘§spec < 0.3,
0.3 < ğ‘§spec < 0.7, and 0.7 < ğ‘§spec < 1.0, respectively. We find that
the distribution of SDSS colours spans the same region of the colour
space as that of synthetic colours for the first redshift bin but not
for the higher redshift bin of 0.7 < ğ‘§spec < 1.0. This is because the
SDSS low-redshift sample is primarily a magnitude-limited sample,
while at intermediate and high redshift the survey only observed red
and bright galaxies. In fact, we can see that the distribution of SDSS
colours in the second redshift bin agrees with just the distribution of
colours for the reddest synthetic galaxies.
In the third redshift bin 0.7 < ğ‘§spec < 1.0, we can readily see that
SDSS galaxies present, on an average, redder colours than DEEP2
galaxies. The red colours of SDSS galaxies are once again explained
by the flux limit selection, which at higher redshifts preferentially
selects luminous red galaxies. DEEP2 selects much fainter galaxies
(up to ğ‘– âˆ¼ 24), with a particular colour selection that targets galaxies
with ğ‘§ > 0.7, yielding a different colour distribution with a larger
population of blue galaxies. Furthermore, we can see that the distribution of galaxy colours for observed data is more extended than
for the case of synthetic data. This is simply because we produce
noiseless synthetic colours and observed colours suffer from significant photometric uncertainties, especially for faint galaxies. Possible
systematic errors by training on such noiseless data may require corrections (as explained in Section 3.3) and calibrations (as explained
in Section 4.1) when adapting to real observations.
We can also see that synthetic galaxies span the same region of the
colour-redshift space as observed galaxies, which strongly suggests
that our methodology produces synthetic colours representative of
the observed colours of spectroscopic galaxies. This, in conjunction
with physically motivated information regarding the colour-redshift
correlation in the training set would enable an ML model to learn
the associated mapping. Furthermore, given that observed data from
different surveys do not span the same region of the colour space,
an ML-model trained on one survey may not be apt for inferring
redshifts from another. With a training prior different from that of
testing colour-redshift space, the mapping learned could be inherently biased, and a leading order zero-point correction would not
sufficiently correct for the discrepancy. On the other hand, synthetic
data overlaps substantially with all observed data, ensuring that a
single model trained on the synthetic data may satisfactorily estimate
photometric redshifts, independently of which survey is used for the
inference.
3.2 The probabilistic neural network algorithm
The mapping of broad-band filter values to photometric redshift is
complicated due to multiple factors. Primarily, degeneracies arise in
the redshift prediction due to the presence of multiple clusters in the
colour space C. This results in non-Gaussian posteriors for photometric redshifts ğ‘(ğ‘§phot|ğ‘ âˆˆ C), which is typically ignored in ğœ’
2 optimization schemes in template fitting or regression neural networks.
In order to capture multi-modal information within the inference
pipeline, one may explicitly include a hard clustering method (like
k-means clustering) to separate the whole galaxy sample sub-types
and then perform regression analysis within individual subtypes. Alternatively, one can also perform soft clustering for determining a
probability of association of a given datapoint with a specific subtype, and perform regression without separate training models. The
latter approach is employed in syth-z: specifically, a Gaussian mixture model coupled with a fully connected neural network for redshift
estimation.
A Gaussian mixture function (McLachlan & Basford 1988) is a
linear combination of several Gaussian components, each identified
by ğ‘– âˆˆ 1, 2, ..., ğ‘š, where ğ‘š is the total number of clusters, as defined
in Equation (1).
ğ‘(ğ‘§ğ‘˜ |ğ‘ğ‘˜ ) =
âˆ‘ï¸ğ‘š
ğ‘–=1
ğœ‹ğ‘–(ğ‘ğ‘˜ )N (ğ‘§ğ‘˜ |ğœ‡ğ‘–(ğ‘ğ‘˜ ), ğœğ‘–(ğ‘ğ‘˜ )) (1)
For each galaxy with index ğ‘˜, the PDF of redshift conditioned on
MNRAS 000, 1â€“14 (2021)
Synthetic spectra for redshift estimation: SYTH-Z 5
0 2 4
u âˆ’ g
0
1
2
g
âˆ’
r
0 1 2
g âˆ’ r
0.0
0.5
r
âˆ’
i
0.0 0.5
r âˆ’ i
0.0
0.2
0.4
0.6
i
âˆ’
z
0.0 0.2 0.4 0.6
i âˆ’ z
15
20
25
mag
(
i
)
Synthetic SDSS 0.0 < zspec < 0.3
(a) Comparison of the synthetic and observed (SDSS) colour distributions for galaxies in the redshift range 0.0 < ğ‘§ < 0.3.
0 2 4
u âˆ’ g
0
1
2
g
âˆ’
r
0 1 2
g âˆ’ r
0.0
0.5
1.0
1.5
r
âˆ’
i
0.0 0.5 1.0 1.5
r âˆ’ i
0.0
0.5
1.0
i
âˆ’
z
0.0 0.5 1.0
i âˆ’ z
17.5
20.0
22.5
mag
(
i
)
Synthetic SDSS VIPERS 0.3 < zspec < 0.7
(b) Comparison of the synthetic and observed (SDSS and VIPERS) colour distributions, over the redshift range 0.3 < ğ‘§ < 0.7.
0 2 4
u âˆ’ g
0
1
2
3
g
âˆ’
r
0 1 2 3
g âˆ’ r
0.0
0.5
1.0
1.5
r
âˆ’
i
0.0 0.5 1.0 1.5
r âˆ’ i
0.0
0.5
1.0
1.5
i
âˆ’
z
0.0 0.5 1.0 1.5
i âˆ’ z
18
20
22
24
mag
(
i
)
Synthetic SDSS DEEP2 0.7 < zspec < 1.0
(c) Colour distribution comparison following panel (b), for synthetic and observed (SDSS and DEEP2) colour distributions for galaxies in the redshift
range 0.7 < ğ‘§ < 1.0.
0 1 2 3 4
u âˆ’ g
0.0
0.5
1.0
1.5
zspec
0 1 2
g âˆ’ r
0.0 0.5 1.0 1.5
r âˆ’ i
0 1
i âˆ’ z
16 20 24
mag(i)
Synthetic SDSS DEEP2
(d) Distribution of galaxy colours and i-magnitude with redshift for the synthetic, SDSS and DEEP2 data sets.
Figure 1. Comparison of synthetic and observational data: Panels show different projections of the joint distributions of colours (panels (a), (b), (c)),
and of the colours and i-magnitude and redshift (panel (d)).
input photometry ğ‘(ğ‘§ğ‘˜ |ğ‘ğ‘˜ ) is a combination of Gaussian distributions. This Gaussian mixture is characterized by the means ğœ‡ğ‘–(ğ‘ğ‘˜ )
and standard deviations ğœğ‘–(ğ‘ğ‘˜ ) of the Gaussian components, and
the mixing probability of association of the datapoint with a specific
cluster ğœ‹ğ‘–(ğ‘ğ‘˜ ) , satisfying Ãğ‘š
ğ‘–=1
ğœ‹ğ‘–(ğ‘ğ‘˜ ) = 1. While the number of
components in the mixture, ğ‘š, is generally pre-specified or independently optimized, the rest of the parameters are learned using an
unsupervised clustering algorithm.
MNRAS 000, 1â€“14 (2021)
6 Ramachandra et al.
c p(zk| ck) k
Figure 2. Neural network architecture for the Mixture Density Framework.
The colour-magnitude inputs are mapped to the Gaussian mixture model
parameters in order to obtain a conditional density distribution ğ‘(ğ‘§phot |ğ‘).
3.2.1 Mixture Density Networks
In order to learn the scalars that parameterise the Gaussian mixture model ğœ‡(ğ‘ğ‘˜ ), ğœ(ğ‘ğ‘˜ ), ğœ‹(ğ‘ğ‘˜ ), we implement a Mixture Density
Network (MDN; Bishop 1994) by combining a conventional neural network with a mixture density model to predict the conditional
probability in Equation (1) for the redshift ğ‘(ğ‘§ğ‘˜ |ğ‘ğ‘˜ ) for given broad
band colours ğ‘ğ‘˜ âˆˆ C. It has to be noted the this training itself is
supervised, i.e., the target ğ‘§spec is provided for every ğ‘ğ‘˜ âˆˆ C, but the
clustering itself is inherently unsupervised.
A crucial improvement in this prediction model is achieved by
replacing the mean-squared error type of penalty (or a loss function),
ğ‘™(Î˜) âˆ |{ğ‘§phot}(Î˜) âˆ’ {ğ‘§spec}|2
(equivalent to negative log-likelihood
for the case of a single-component Gaussian distribution) as a function of networkâ€™s model parameters Î˜ with a more generic negative
log likelihood function given in Equation (2):
ğ‘™(Î˜) = âˆ’ logÃ–
ğ‘˜
ğ‘(ğ‘§ğ‘˜ |ğ‘ğ‘˜ , Î˜) = âˆ’
âˆ‘ï¸
ğ‘˜
log ğ‘(ğ‘§ğ‘˜ |ğ‘ğ‘˜ , Î˜) (2)
= âˆ’
âˆ‘ï¸
ğ‘˜
logâˆ‘ï¸ğ‘š
ğ‘–=1
ğœ‹ğ‘–(ğ‘ğ‘˜ , Î˜)N (ğ‘§ğ‘˜ |ğœ‡ğ‘–(ğ‘ğ‘˜ , Î˜), ğœğ‘–(ğ‘ğ‘˜ , Î˜)) (3)
Since the likelihood of the dataset is simply Ã
ğ‘˜ ğ‘(ğ‘§ğ‘˜ |ğ‘ğ‘˜ ), the loss
ğ‘™(Î˜) as a function of network parameters Î˜ (weights and biases of the
network) reduces to a convenient summation form whose derivative
terms are easily obtained during training of the neural network. We
train the network to maximize the sum of the log-likelihoods of the
output PDFs given in Equation (2).
The prescription of minimizing a mean-squared (or similar pointwise) error function lacks a probabilistic view since one does not
explore the likelihood surface around the minimum. Hence error
bars on the estimates are typically absent in the outputs of such neural networks. In contrast, the MDN provides an inexpensive approach
to quantify the epistemic uncertainties in the predictions, as demonstrated further in Section 4.2. Hence MDN is the primary estimation
component of the syth-z framework.
3.2.2 Training procedure
The first step towards training an ML-based prediction model lies in
curating the data. We first split the synthetic data into training and â€˜insetâ€™ validation sets. A fraction (âˆ¼10%) of the âˆ¼200,000 synthetic datapoints is used for validation and testing purposes. These â€˜hold-outâ€™
datapoints are randomly chosen, and do not contribute to the training
of the algorithm. With the remaining datapoints, we also perform a
re-sampling before training the network. We choose âˆ¼200 redshift
bins in the range 0.0 < ğ‘§ < 1.2 and randomly select 50 galaxies in
each bin. The resulting re-sampled datapoints are roughly uniform.
Since the neural networks are generally sensitive to the underlying
distribution of the training data, such bias-mitigation techniques can
overcome issues related to imbalanced training schemes.
Next, we also re-scale both inputs and targets of the neural network
model. We standardize the broad band input by removing the mean
and scaling to unit variance. The mean and variance are computed
across all bands, so that the correlations between the bands are preserved. We then transform the target redshifts by scaling them in the
range of 0 to 1. Such pre-processing steps are a common requirement for machine learning tasks. However, the specific choices of
the re-scaling transformations can be made heuristically. We have
tested between various choices of transformations and settled on the
above, based on the rate of convergence and accuracy of the resulting
estimations.
The mixture density network (explained in Section 3.2.1) in this
implementation is fundamentally a mapping estimator from galaxy
colours to redshift. The sample network (Figure 2) is a fully connected dense network with 11 layers with number of neurons per
layer as: [5(input) â†’ 512 â†’ 1024 â†’ 2048 â†’ 1024 â†’ 512 â†’
256 â†’ 128 â†’ 64 â†’ 32 â†’ 1Ã—3]. The last layer [1Ã—3] corresponds
to (ğœ‹, ğœ‡, ğœ) for each grid point that parameterise the predictive conditional distribution ğ‘(ğ‘§phot|ğ‘). Each hidden node in the network has
a nonlinear hyperbolic tangent (or a Rectified Linear Unit) activation
(Nwankpa et al. 2018).
The error function in Equation (2) is optimized during the training,
and the weights w are updated using an Adam optimizer (Kingma &
Ba 2014). The training procedure of the mixture density network is
summarized in Algorithm 1.
Algorithm 1 Training routine for mixture density networks
1: w â† Initialize network parameters
2: while not converged do
3: ğ‘¥ â† Subset of the complete training data set (a batch)
4: for ğ‘¥ğ‘˜ in ğ‘¥ do
5: Forward propagation i.e., map the input colours to Gaussian
mixture model parameters: (ğœ‹ğ‘˜ , ğœ‡ğ‘˜ , ğœğ‘˜ ) â† ğ‘ğ‘˜ ; w
6: Compute predictive distribution: ğ‘(ğ‘§ğ‘˜,phot|ğ‘ğ‘˜ ) â†
(ğœ‹ğ‘˜ , ğœ‡ğ‘˜ , ğœğ‘˜ )
7: end for
8: Compute negative log-likelihood: E â† âˆ’ log L
9: Update network parameters using back-propagation: w â†
Adam(âˆ‡wE, w)
10: end while
The hyper-parameters such as the learning rate, decay rate, and
network parameters are optimized heuristically by monitoring the
changes in loss (Equation (2) with different epochs. However, one
may also consider the use of theoretical optimization methods such
as hyperopt (Bergstra et al. 2013) and Bayesian optimization (Snoek
et al. 2012) so that the accuracy of the estimation can be improved.
3.3 Zero-point calibration
Adjusting the zero-points of the observational colours to obtain better redshift predictions is a common approach in the template-based
MNRAS 000, 1â€“14 (2021)
Synthetic spectra for redshift estimation: SYTH-Z 7
photo-z literature (e.g. BenÃ­tez 2000a; Coe et al. 2006; Hildebrandt
et al. 2012; Molino et al. 2014; Laigle et al. 2016; Eriksen et al. 2019;
Alarcon et al. 2021). It attempts to correct relative zero-point errors
remaining from the zero-point calibration of the data itself, or errors resulting from incorrect point spread function (PSF) modelling
(Hildebrandt et al. 2012). It will also absorb any leading order systematic offsets between the templates and the data. When the MDN
is trained using observations, such zero-point errors are naturally
learned by the model connecting redshift with colours. However,
since the synthetic data generated to train the network is noiseless,
and does not emulate any of the effects imprinted by the observation
process, an overall zero-point mismatch is, in general, to be expected.
We parameterise the zero-point correction with four colour offsets
for each survey (one for each colour used as input by the MDN).
We estimate the zero-point set {ğœ†ğ‘˜ ; ğ‘˜ = ğ‘¢ âˆ’ ğ‘”, ğ‘” âˆ’ ğ‘–, ğ‘– âˆ’ ğ‘Ÿ, ğ‘Ÿ âˆ’ ğ‘§}
from the measured spectroscopic redshifts {ğ‘§
ğ‘—
spec; ğ‘— = 1 . . . ğ‘ğ‘”} and
measured features for a subset of the training sample. The measured features include the measured expected value of the magnitudes
and colours, {hğ‘Ë†
ğ‘—
ğ‘˜
i} and their measured root-mean-squared values,
{ğœ(ğ‘Ë†
ğ‘—
ğ‘˜
)}. Then,
ğ‘({ğœ†ğ‘˜ }|{hğ‘Ë†
ğ‘—
ğ‘˜
i}, {ğœ(ğ‘Ë†
ğ‘—
ğ‘˜
)}, {ğ‘§
ğ‘—
spec}) =
Ã–
ğ‘ğ‘”
ğ‘—
ğ‘(ğ‘§
ğ‘—
spec |hğ‘Ë†
ğ‘—
ğ‘˜
i, ğœ(ğ‘Ë†
ğ‘—
ğ‘˜
), {ğœ†ğ‘˜ })
Ã— ğ‘({ğœ†ğ‘˜ })
(4)
where we assume all galaxies to be measured independently from
each other and note that the zero-points do not explicitly depend on
the measured features. The prior ğ‘({ğœ†ğ‘˜ }) is uniform. The redshift
probability is given by
ğ‘(ğ‘§
ğ‘—
spec |hğ‘Ë†
ğ‘—
ğ‘˜
i, ğœ(ğ‘Ë†
ğ‘—
ğ‘˜
), {ğœ†ğ‘˜ }) =
âˆ«
ğ‘‘ğ‘ğ‘˜ ğ‘(ğ‘§
ğ‘—
spec |ğ‘
ğ‘—
ğ‘˜
, hğ‘Ë†
ğ‘—
ğ‘˜
i, ğœ(ğ‘Ë†
ğ‘—
ğ‘˜
), {ğœ†ğ‘˜ })
Ã— ğ‘(ğ‘
ğ‘—
ğ‘˜
|hğ‘Ë†
ğ‘—
ğ‘˜
i, ğœ(ğ‘Ë†
ğ‘—
ğ‘˜
), {ğœ†ğ‘˜ })
=
âˆ«
ğ‘‘ğ‘ğ‘˜ ğ‘(ğ‘§
ğ‘—
spec |ğ‘
ğ‘—
ğ‘˜
)
Ã— ğ‘(ğ‘
ğ‘—
ğ‘˜
|hğ‘Ë†
ğ‘—
ğ‘˜
i, ğœ(ğ‘Ë†
ğ‘—
ğ‘˜
), {ğœ†ğ‘˜ })
(5)
where ğ‘ğ‘˜ are the noiseless features from the synthetic templates and
ğ‘(ğ‘§
ğ‘—
spec |ğ‘
ğ‘—
ğ‘˜
) is given by the MDN, which only depends explicitly
on the synthetic colours (going from the first to the second line in
Equation (5)). The probability of observing the synthetic colours is
proportional to the measurement likelihood of each object, which is
usually assumed to be a normal distribution given by the measured
colour and colour errors. In this case, the normal distribution is
centered at the colours plus the zero-point shift,
ğ‘(ğ‘
ğ‘—
ğ‘˜
|hğ‘Ë†
ğ‘—
ğ‘˜
i, ğœ(ğ‘Ë†
ğ‘—
ğ‘˜
), {ğœ†ğ‘˜ }) âˆ exp
Â©
Â­
Â­
Â«
âˆ’

ğ‘
ğ‘—
ğ‘˜
âˆ’ (hğ‘Ë†
ğ‘—
ğ‘˜
i + ğœ†ğ‘˜ )
2
2(ğœ(ğ‘Ë†
ğ‘—
ğ‘˜
))2
Âª
Â®
Â®
Â¬
. (6)
In Equation (6) we assume that the magnitude and colours are uncorrelated. Also, note that the zero point for the ğ‘–-band magnitude is
fixed at a value of zero (equivalent to assuming a prior distribution
of a Dirac delta function centred at zero).
We run an MCMC and keep the zero-point values that maximize
the likelihood from Equation (4). We calculate Equation (5) by drawing samples of synthetic colours from ğ‘(ğ‘
ğ‘–
ğ‘˜
|ğ‘Ë†
ğ‘–
ğ‘˜
, {ğœ†ğ‘˜ }), weighting
them by their likelihood at the spectroscopic redshift according to
0.00
0.25
0.50
0.75
1.00
p(zphot|c)
mi = 14.8 mag
MDN output (before ZP)
MDN output (after ZP)
Marginalized over photometric errors
(before ZP)
Marginalized over photometric errors
(after ZP)
0.0 0.1 0.2 0.3 0.4 0.5 0.6
zspec
0.00
0.25
0.50
0.75
1.00
p(zphot|c)
mi = 18.9 mag
Figure 3. PDFs for a bright, low redshift galaxy (top panel) and a faint galaxy
(bottom panel) from SDSS. The ğ‘–-band magnitudes of the galaxies are 14.8
mag and 18.9 mag respectively. The dashed lines show the spectroscopic
redshift of the galaxies. The blue solid lines show the MDN output before
zero-point (ZP) calibration and the red lines show the post-calibration output.
The corresponding shaded regions demonstrate the broadening of the photo-z
PDF due to marginalization over observational errors.
MDN, and then taking the average. Calculating the integral from
Equation (5) exactly can be expensive. An alternative method we
follow here is to approximate Equation (5) by assuming a normal
distribution, which we evaluate at the spectroscopic redshift value.
We draw samples from ğ‘(ğ‘
ğ‘–
ğ‘˜
|ğ‘Ë†
ğ‘–
ğ‘˜
, {ğœ†ğ‘˜ }) and calculate the mode of
ğ‘(ğ‘§|ğ‘
ğ‘–
ğ‘˜
) according to MDN. The mean and variance of the approximate normal distribution are the mean and variance of the set of mode
values. We have found the zero-point MCMC to converge much faster
with this approximate method than the exact calculation for fainter
samples of galaxies, for which a larger integration time is needed to
calculate Equation (5) exactly. Furthermore, since there can be galaxies with large photometric errors, inaccurate spectroscopic redshift,
or some misclassified stars, we clip the minimum value the likelihood
of a galaxy can take to prevent these outliers from dominating the
overall zero-point likelihood.
4 RESULTS
With the development and training of this probabilistic framework,
we now present the validation results. Due to the discrepancy between
the training data and observational data, we investigate the effect of
photometric noise and systematic offsets, and compare the sythz performance against baseline algorithms. The validations in this
section are done against SDSS, VIPERS and DEEP2 galaxies, and
not with respect to hold-out synthetic data.
4.1 Effect of calibration
The output of the MDN trained from noiseless synthetic data is
generally a narrow conditional density estimate ğ‘(ğ‘§phot|ğ‘). This is
expected, since the input colours are point values, while the MDN
is trained on tightly sampled noiseless data (as explained in Section
3.2.2). Once the MDN training is complete, we perform a zero-point
calibration, as detailed in Section 3.3. The resulting conditional density estimate is also narrow, the motivation for zero-point calibration
MNRAS 000, 1â€“14 (2021)
8 Ramachandra et al.
âˆ’0.3 âˆ’0.2 âˆ’0.1 0.0 0.1 0.2 0.3
(zphot âˆ’ zspec)/(1 + zspec)
0
2
4
6
8
PDF
Testing surveys
SDSS
VIPERS
DEEP2
No zero-point
optimization
Zero-point
optimization
Figure 4. Effect of zero-point correction: The PDF shows the relative difference between the predicted and true redshifts before and after the zero point
correction. For each survey, the zero-point optimization (solid lines) improves
the accuracy compared to the original MDN results (dashed lines).
is to correct for the systematic offsets arising due to noiseless synthetic training data. Both these PDFs are shown for two SDSS galaxies in Figure 3. The calibration in the colour space results, on average,
in the PDFs (the red lines in both panels) agreeing better with the
spectroscopic redshift ğ‘§phot from the SDSS catalogue, than the direct
output of MDN before zero-point calibration (the blue lines).
Next, we demonstrate the effect of the colour calibration process
on all three observational surveys of interest. The SDSS, VIPERS,
and DEEP2 relative differences (ğ‘§phot âˆ’ ğ‘§spec)/(1 + ğ‘§spec) before
calibration peak away from zero, as shown in Figure 4. With the zeropoint correction, this distribution peaks closer to zero, signifying
improved accuracy in the estimated redshifts.
4.2 Uncertainty estimates
When the MDN measurements are marginalised over the observational measurement error, the ğ‘(ğ‘§phot|ğ‘) is broader. We investigate
this by running 1000 MDN evaluations per galaxy, where the input
colours are drawn from Gaussian distributions corresponding to the
SDSS photometric error (i.e. we use Equation 5). Figure 3 shows
the broadening of the photo-z PDF due to averaging over the 1000
conditional density measurements, before (blue shaded region) and
after (red shaded lines) the zero-point correction.
Once the systematic offsets are corrected using the zero-point corrections, the pre-marginalized PDF output of the MDN represents
the epistemic uncertainty and is described via the distribution of the
weights of the neural network. Epistemic uncertainties are usually
model dependent, i.e., they can be reduced by an optimal training process or a better statistical model. Due to the tight sampling of training
data points and near-optimal training of the MDN, the epistemic uncertainty is highly minimized, as demonstrated in Figure 3. Next, the
PDF obtained by marginalizing over the observational colours (after
zero-point correction) represents the aleatoric uncertainty, arising
from the inherent colour variability. The stochasticity in the prediction is limited by the quality of the observational photometry, and
cannot be improved by a better training scheme or a more flexible
neural network.
The two panels in Figure 3 also show the difference between redshift estimates for a bright, low redshift galaxy and a fainter galaxy
12 13 14 15 16 17 18 19 20 21 22
mag(i)
10âˆ’2
10âˆ’1
100
Width of
p(z|c)
Marginalized over
photometric errors (after ZP)
SDSS
VIPERS
DEEP2
MDN output
(before ZP)
SDSS
VIPERS
DEEP2
Figure 5. Widths of photo-z PDFs at different ğ‘– band magnitude bins. The
widths are shown for the three observational surveys - SDSS, VIPERS and
DEEP2 â€“ used for testing. The widths are shown for two types of PDFs. The
box plots with hatches show the MDN output before zero-point calibration,
and the solid box plots are for photo-z PDFs marginalized over observational
errors (with zero-point correction).
at a higher redshift. Marginalization over the observed photometric
errors results in a wider broadening for the faint SDSS object in the
lower panel. In Figure 5, the effect of obtaining less confident predictions with fainter redshifts is demonstrated for all the galaxies in our
testing sample. For the SDSS, VIPERS and DEEP2 surveys, the width
of marginalized ğ‘(ğ‘§phot|ğ‘) increases with larger i-band magnitudes.
In other words, the aleatoric uncertainty of the redshift prediction
increases with increasing magnitude. On the other hand, the corresponding width of the MDN density estimates is both smaller, and
roughly consistent across varying brightness of the galaxies. This
pre-marginalization ğ‘(ğ‘§phot|ğ‘) includes the uncertainty due to the
training data prior. That is, the width is primarily due to the limitations in the training process, the information loss due to the small
number of broad bands, and their colour-redshift degeneracies in the
limited number of bands. However, the MDN output alone does not
capture the effect of measurement error in the surveys.
The process of zero-point calibration implemented within the
syth-z framework (from Section 3.3) involves an assessment of
model uncertainty using the noisy galaxy samples and a consequent
updating of the redshift prediction model. This inverse uncertainty
quantification is often neglected in machine learning regression algorithms; most prediction codes instead quantify the forward uncertainty propagation (i.e., the influence on the outputs from the parametric variability in the input space) by assessing the mean, variance,
or the distribution of outputs. An alternative to zero point calibration
is a bias correction, where a discrepancy function can be determined
based on spectroscopic redshift values for a small number of observational data points. Within the current syth-z framework, we only
perform the parameter calibration. However, the bias correction may
be applied independently, or in combination with the calibration.
Hence, the syth-z framework has a two-fold treatment of the uncertainties. First, it provides an inverse assessment of uncertainty and
calibration in the input space, to account for the systematic offsets
between the trained MDN model and true photometric redshift mapping. Second, it includes a forward uncertainty propagation, identifying the sources of both aleatoric and epistemic uncertainties in the
redshift estimates.
MNRAS 000, 1â€“14 (2021)
Synthetic spectra for redshift estimation: SYTH-Z 9
4.3 Comparison with observational training
We now compare the novel photometric redshift estimation technique
of syth-z against baseline schemes where the training is done purely
on observational datasets. To ensure uniformity over the comparison
models, we use a similar scheme for training while using SDSS and
VIPERS data. That is, both SDSS and VIPERS datasets (explained
in Section 2.2) are divided into training and testing sets.
While our method provides a PDF parameterised by Gaussian
mixtures for prediction of redshift, several popular metrics rely on
point estimates. In order to get the best redshift point estimate, we
choose the mean of the Gaussian component with the highest weight.
This procedure is commonly used to reduce photo-z PDFs to a point
estimate (see Appendix B of Schmidt et al. 2020). The standard
deviation corresponding to the highest weighted component is taken
as the best Gaussian uncertainty estimate.
For comparisons, we use the same observational test set for all three
training schemes. Photometric redshift estimates from the three training schemes are shown in Figure 6. The top panel shows the results
from the synthetic colour-trained syth-z framework, where the estimates for SDSS, DEEP2 and VIPERS show reasonable agreement
with the spectroscopic redshifts. In comparison, an SDSS-trained
model in the lower left panel performs well with the test SDSS
dataset, but the estimations for VIPERS and DEEP2 are significantly
biased. This bias arises because SDSS galaxies are selected to be
preferentially red at intermediate and high redshifts, while DEEP2
and VIPERS contain mostly blue galaxies (Figure 1). A similar trend
is seen when an ML model is trained with VIPERS (lower right
panel of Figure 6): the estimation for hold-out VIPERS and DEEP2
data sets are both unbiased, whereas redshifts are under-estimated
for SDSS colours.
Notable differences across different training schemes can be seen
in the standard deviations of the redshift predictions ğ‘(ğ‘§ğ‘˜,pred|ğ‘ğ‘˜ ),
depicted by the error bars in Figure 6. The conditional density estimate of the MDN is conditioned on the training prior. In the case
of synthetic training, the training data is noiseless and densely samples the colour-redshift space. Hence the upper panel of Figure 6
shows small error bars for predictions for all three noisy observational datasets. This was obtained by using the pre-marginalization
PDF (after zero-point calibration), hence only the epistemic uncertainties are captured. By sampling the PDFs based on the photometric
errors in the observational data, the conditional density estimation is
much broader, as shown in Figure 5. On the other hand, the lower panels show broader error bars, since the training data sparsely sample
the colour-redshift space. They also capture the photometric error in
the observations within the training scheme. This is a crucial difference in the uncertainty quantification of synthetic- and observationaltrained models. While with syth-z, we are able to isolate epistemic
and aleatoric uncertainties simply by pre- and post-marginalization
stages, it is simply not possible when the training data is noisy. The
error bars of SDSS- and VIPERS- trained models in Figure 5 include
epistemic uncertainty (like the pre-marginalized syth-z PDF) and
a part of aleatoric uncertainty (unlike the pre-marginalized sythz PDF), without a clear interpretation of their contribution to the
overall uncertainty of the MDN. An additional marginalization over
photometric errors during the testing phase would provide another
contribution to aleatoric uncertainty of the SDSS- and VIPERStrained model.
The aleatoric uncertainty in the redshift predictions depends on
the internal randomness in the inputs, i.e., the photometric errors.
This can not be improved for a given survey. However, the epistemic
uncertainty in the redshift predictions can be improved easily in a 0.0 0.2 0.4 0.6 0.8 1.0
zspec
0.0
0.2
0.4
0.6
0.8
1.0
zphot
Training: Synthetic
Testing: SDSS
Testing: VIPERS
Testing: DEEP2 (a) Synthetic training (with zero-point correction).
0.0 0.2 0.4 0.6 0.8 1.0
zspec
0.0
0.2
0.4
0.6
0.8
1.0
zphot
Training: SDSS
(b) SDSS training
0.0 0.2 0.4 0.6 0.8 1.0
zspec
0.0
0.2
0.4
0.6
0.8
1.0
zphot
Training: VIPERS
(c) VIPERS training
Figure 6. Comparison of predicted photometric redshifts and spectroscopic
redshifts for observed galaxies. The training is performed using synthetic data
only (top panel), VIPERS (bottom left) and SDSS (bottom right), respectively.
All the error bars shown in the plots correspond to MDN outputs without
marginalizing over the photometric errors.
synthetic-training scheme, plainly by increasing the size of simulated
SED templates. This is not a viable option in models trained on observational data, where the available galactic spectra are limited. This
limitation is more pronounced for fainter galaxies at high redshift,
where the spectroscopic follow-up is prohibitively expensive.
In addition, a neural network trained on SDSS may yield lessexplainable error bars when tested on a survey with a different error
model, such as VIPERS. Hence, understanding the uncertainty propagation and interpreting the error bars with estimation techniques
like neural networks can be relatively more difficult in noisy training
schemes.
4.4 Aggregate metrics for performance evaluation
In order to quantitatively compare the performance of our
synthetically-trained photometric redshift estimation model sythz with baseline comparison methods using observational training,
we consider a few commonly-employed metrics. These benchmarks
MNRAS 000, 1â€“14 (2021)
10 Ramachandra et al.
0.2 0.4 0.6 0.8
zspec
0.0
0.1
0.2
0.3
ÏƒNMAD
(
zspec
)
Training
Synthetic
SDSS
VIPERS
0.2 0.4 0.6 0.8
zspec
0.0
0.2
0.4
0.6
Î·
(
zspec
)
Training
Synthetic
SDSS
VIPERS
0.2 0.4 0.6 0.8
PIT
0.0
0.2
0.4
0.6
0.8
1.0
N
Training
Synthetic
SDSS
VIPERS
0.2 0.4 0.6 0.8
zspec
0
1
2
3
4
n
(
z
)
Training
Synthetic
SDSS
VIPERS
Observed distribution
Figure 7. Comparisons between 3 different training schemes, i.e., VIPERS-, SDSS- and synthetically-based training. Testing is done on a combined observational
dataset that is held-off from all 3 training schemes. Metrics used are the normalized median absolute deviation ğœNMAD (top left panel), The catastrophic outlier
fraction ğœ‚ (top right panel), Probability integral transform (bottom left panel) and Galaxy redshift distribution ğ‘›(ğ‘§) (bottom right panel). The shaded dash-dotted
lines in the bottom left panel show the PIT curves for perfect estimations. The shaded region of the bottom right panel shows the Poisson errors in the bin-counts
of the ğ‘›(ğ‘§) distribution.
utilize redshift estimates without a marginalization over photometric
errors.
(i) Normalized median absolute deviation: As a measure of
the accuracy of the model, we use the Normalized median absolute
deviation ğœNMAD (Ilbert et al. 2009) to measure photo-z precision.
ğœNMAD = 1.48 Ã— Median 
|ğ‘§spec âˆ’ ğ‘§phot|
1 + ğ‘§spec 
(7)
The scale factor of 1.48 is used for re-interpreting ğœNMAD as the
standard deviation of normally distributed data. The median of the
relative difference is taken because it is less sensitive to extreme
values. Here, ğ‘§phot is taken as the mean of the highest weighted
Gaussian mixture component.
(ii) Catastrophic Outlier fraction: The fraction of galaxies that
satisfy
|ğ‘§phot âˆ’ ğ‘§spec |
(1 + ğ‘§spec)
> ğ‘“cut (8)
are regarded as catastrophic failures. The Catastrophic Outlier fraction ğœ‚ is used to quantify the number of failed photometric redshift
estimations (defined by ğ‘“cut).
(iii) Probability integral transform: The cumulative distribution
function (CDF) of each source evaluated at ğ‘§spec is defined as the
Probability integral transform (PIT).
PIT = CDF[ğ‘, ğ‘§spec] =
âˆ« ğ‘§spec
âˆ’âˆ
p(ğ‘§)dğ‘§, (9)
where ğ‘§spec is the true redshift of the source ğ‘–, and p(ğ‘§) is the probability distribution function, i.e., the 3-component Gaussian mixture
model. The integral is computed from âˆ’âˆ because the p(ğ‘§) can have
non-zero values for negative ğ‘§ values. The CDF values at the true
redshift over the redshift predictions indicate the over-dispersion,
under-dispersion and the biases. The PIT histogram for accurate
predictions is uniform between 0 and 1, and the overestimated and
underestimated uncertainties are concave and convex respectively.
(iv) Galaxy redshift distribution. The overall redshift distribution of the sample, ğ‘›(ğ‘§) provides a cumulative metric for accuracy
of the photometric redshift estimation. The point redshift estimate
ğ‘§phot of the MDN is chosen based on the weights of the individMNRAS 000, 1â€“14 (2021)
Synthetic spectra for redshift estimation: SYTH-Z 11
ual Gaussian components, similar to the redshift estimations used
in ğœNMAD(ğ‘§spec) and ğœ‚(ğ‘§spec). However, we note one could use a
more appropriate ğ‘›(ğ‘§) estimation (using approaches similar to Malz
& Hogg 2020) using the probabilistic estimate ğ‘(ğ‘§pred|ğ‘) for individual galaxies.
We expect the performance of the photometric redshift estimation
scheme to change as a function of redshift. Therefore, we explore the
above metrics in redshift bins, as shown in Figure 7. The redshift dependence of metrics such as ğœNMAD or ğœ‚(ğ‘§spec) are more indicative
of the performance of neural networks, rather than an average over
all the testing samples.
The top left panel of Figure 7 shows ğœNMAD variation with spectroscopic redshift. The synthetically-trained model value lies consistently under ğœNMAD(ğ‘§spec) < 0.1. On the other hand, the SDSStrained model has lower ğœNMAD(ğ‘§phot) for ğ‘§spec < 0.3, but rises over
ğœNMAD(ğ‘§spec) > 0.1 at higher redshifts. Similarly, at low redshifts
(ğ‘§spec < 0.25) the VIPERS-trained model has higher ğœNMAD(ğ‘§phot)
than that of the synthetic model.
As in the case with ğœNMAD(ğ‘§spec), a point redshift estimate ğ‘§phot
of the MDN is chosen based on the weights of the individual Gaussian components. In our comparisons, the cutoff for the failures is
chosen to be ğ‘“ğ‘ğ‘¢ğ‘¡ = 0.15. While the VIPERS-trained model has more
catastrophic outliers (up to 60%) at ğ‘§spec < 0.1, the SDSS-trained
model has a large number of outliers in the range 0.5 < ğ‘§spec < 0.8.
The synthetically-trained model performs well across the entire range
of ğ‘§spec, with the outlier fraction ğœ‚(ğ‘§spec) being consistently under
10%.
In the bottom left panel of Figure 7, we show the PIT curve in the
absence of the estimation outliers by discarding the PIT < 0.05 and
PIT > 0.95. Both SDSS- and VIPERS-training show significantly
more biased, and under-dispersed redshift-estimation PDFs. This is
seen in the deviation from the PIT curves corresponding to perfect estimations (shown in dash-dotted lines). The synthetic-training yields
in a PIT curve that is closest to a uniform distribution, showing a
better-calibrated redshift estimation.
As a final comparison, the bottom right panel of Figure 7 shows
a comparison of ğ‘›(ğ‘§) for the three training schemes, along with the
standard deviation of the galaxy counts per bin. The VIPERS-trained
photometric redshift distribution deviated from that of the observed
sample (which is the â€˜hold-outâ€™ sample of SDSS and VIPERS galaxies) at low redshifts. Similarly the SDSS-trained sample deviates
significantly at ğ‘§spec > 0.5.
With these aggregate comparisons, the performance of syth-z is
shown to be superior to that of the baseline models trained on observational data points. While the metrics are by no means exhaustive,
they are sufficient to demonstrate the advantages of using realistic
synthetic data in photo-z modelling. A more complete analysis with
performance metrics, in the context of future surveys like LSST or
Euclid, is reserved for later investigations.
5 DISCUSSIONS AND CONCLUSIONS
We have presented a hybrid framework for probabilistic photometric
redshift estimation, syth-z, with 3 notable differences from existing
photo-z codes:
â€¢ The training set comprises entirely of synthetic photometry
generated from fsps and star formation histories from realistic galaxy
formation models (as shown in Section 2.1).
â€¢ A probabilistic network using Gaussian mixture modelling (described in Section 3.2), and marginalization over noisy observational
photometry isolates the aleatoric and epistemic uncertainties in the
redshift prediction (explained in Section 4.2).
â€¢ Survey-specific calibration is provided to mitigate the leading
order offsets between noiseless synthetic photometry and noisy observations (formulated in Section 3.3 and the results shown in Section
4.1.)
Development of such a synthetic-data based photometric redshift estimation framework requires meticulous experimental design,
where both modelling of synthetic SEDs (shown in Figure A1) and
their colour-redshift distribution (in Figure 1) has to match with the
observational surveys. Once a surrogate model for mapping photometry to redshifts is trained, systematic offsets in the prediction
can be corrected (Figure 4) via a calibration technique inspired by
template-fitting algorithms.
Utilizing this framework, we have demonstrated, using Figure 7,
that for a choice of inference method with uncertainty quantification, physical modelling of synthetic data can outperform purely
observation-based training. This work motivates the development of
robustness requirements for synthetic forward modelling efforts and
error propagation modelling in the context of upcoming astronomical
surveys.
The spectroscopic observational campaigns are expensive, which
results in restricted coverage over sky area or over faint objects. As
a result, such samples are incomplete and prone to selection effects,
which lead to biased training for ML techniques. On the other hand,
template-based methods usually rely on simple libraries, as physically realistic galaxy SED models quickly become too computationally costly. In this paper, we provide a bridge between these worlds by
using a set of realistic synthetic templates based on SFHs from stateof-art hydrodynamical simulations and empirical galaxy formation
models, together with a probabilistic neural network framework to
estimate fast galaxy redshifts conditioned on LSST-like photometry.
Each individual component of syth-z (in Section 3 can be replaced, say, by a more realistic template library, with a better ML
algorithm, or a higher-order offset mitigation technique. However,
the principles of designing such a framework â€“ the meticulous experimental design, a physical forward model for synthesizing galaxy
colours, usage of uncertainty quantifiable ML models, and marginalization of estimates over stochastic measurements of observational
colours â€“ remain the same.
Investigation into synthetic galaxy models, simulations and realistic galaxy mock catalogues are crucial for astronomical studies in
the near future. Stage IV dark energy surveys like Euclid and Roman
Space Telescope would require about 5000 spectroscopic observations to enable photometric calibrations (Stanford et al. 2021). Training and calibration for surveys like the Rubin Observatoryâ€™s LSST
may require an even higher number (about 30,000) of spectroscopic
objects over roughly 15 widely separated regions (Newman et al.
2015b). Larger, more complete samples reaching the faintest objects
would be required to reduce the scatter in photometric redshift estimates. Our approach of synthetic data creation circumvents the need
for the highly expensive spectroscopic follow-up surveys needed for
photo-z calibration. In addition, it also avoids the issue of training
with existing observational data that are neither representative nor
complete.
In addition, quantification of error propagation is increasingly important for future sky surveys that probe faint galaxies with broad
photometric bands. Degeneracies resulting from the limited number
of filters results in both biased estimations of redshift, and catastrophic outliers in the predictions. With the Gaussian mixture model
we have employed in syth-z, followed by the marginalization over
MNRAS 000, 1â€“14 (2021)
12 Ramachandra et al.
observational errors, we are able to model complex photo-z PDFs,
two of which are shown in Figure 3. Incorporating the mixture models
not only allows for modelling arbitrary prediction posteriors that are
not simple Gaussian distributions but also accounts for degeneracy
in the colour-redshift space.
The possible offsets in the predictions of the MDN can be traced
to the lack of modelling of the observational processes, such as the
PSF response. This offset is specific to synthetic training models,
and calibration in the input space is necessary to ensure an unbiased
redshift estimation via the syth-z framework. Stringent requirement
of unbiased redshift estimation in weak lensing studies (Ma et al.
2006) necessitates such an offset-mitigation treatment.
Finally, the computational expense of photometric redshift estimation is also worth noting. Classical template fitting codes can take
over 1 second per object. In comparison, MDN prediction takes less
than 0.02 milliseconds per galaxy with a single NVIDIA V100 Tensor Core GPU. This speed-up is reached because the input colours
can be passed to the trained network in arbitrarily large batches (only
limited by the device memory). In the context of LSST and Euclid,
where photometric data of billions of galaxies are expected, this level
of robustness is required for most photometric analyses.
Next, we discuss the limitations of the syth-z technique, which
are applicable to a broader family of photometric redshift estimation
codes that rely on synthetic SED templates:
â€¢ One possible shortcoming of our synthetic data creation is that
the universemachine or IllustrisTNG models may not generate
SFHs representative enough for all galaxies from the observations.
To solve this problem, new approaches to model the galaxy-halo connection that brings together the advantages of SFH parametric and
empirical models (Alarcon et al. 2021, in prep.) are currently being
investigated. Furthermore, the development of surrogate SPS models
based on such empirical models to speed up the generation of galaxy
photometry (Hearin et al. 2021, in prep.) is also underway.
â€¢ Another notable issue with approaches that may use synthetic
data is that the template library may not be representative of the
galaxies from the observations. That is, the templates may not be
compatible with any survey galaxy SED. We believe our approach
of using ML circumvents this issue by learning the overall mapping, rather than the individual one-on-one matching of galaxies.
Regardless, the validity of the synthetic templates used in syth-z is
ensured only up to redshift of ğ‘§spec â‰ˆ 1 (see Appendix A). Extensive
benchmarking of synthetic templates will be necessary to broaden
the scope of syth-z, where proper scoring rules or other evaluation
metrics would be required.
â€¢ In addition, one should also ensure that the synthetic-trained ML
model does not extrapolate in the colour-redshift space of the observational surveys. The black-box nature of over-parameterised statistical models (e.g., deep neural networks) renders the interpretability
of the mapping exceedingly difficult whilst extrapolating. Within this
study, we have only applied limiting constraints on our data to ensure
overlap between synthetic and observed colours. However, agreement of training and testing data distributions in a higher-dimensional
space needs to be investigated.
â€¢ Finally, our treatment for dealing with photometric errors involves large numbers of draws in the colour space, followed by averaging over PDFs from the MDN. The plain estimations of MDN
(with zero-point calibration) are much faster, and provide mean redshift predictions with associated epistemic uncertainties, but not the
effect of observational errors. The effect of aleatoric uncertainty
becomes increasingly important in fainter galaxies. This trade-off
between inference speed and accounting for all the prediction errors
can be challenging while scaling the model to billions of galaxies.
We are currently exploring models based on these requirements, including adaptability to high-performance computing resources, and
ML techniques of transfer learning and Bayesian neural networks.
This work facilitates exploration into multiple research avenues.
In this work, we have only discussed the synthetic data corresponding to 5 (broad) bands. Detailed analyses with various numbers of
bands (for example, 5 LSST colours, 40 narrow bands of PAUS,
56 narrow bands from J-PAS, or 101 SPHEREx channels) are easily
possible. Investigations of correlations between individual bands and
their effects on photometric redshift estimation can help in designing
future surveys. Second, ML inference algorithms can be designed
specifically for cross-matched catalogues using synthetic data. In
real observational surveys, obtaining a pristine cross-matched sample depends on sky coverage, resolution, and sensitivity of individual
surveys. This is prone to multiple cross-matches for single sources
and mismatches. However, the synthetic SEDs will only have to be
convolved with filter transmission curves of different telescopes in
order to get perfectly matched galaxies. Third, synthetic colours also
enable the possibility of inferring star formation rates, metallicities
and other galactic parameters. A nonlinear mapping between these
galactic properties and photometric colours can be machine-learned
since the synthetic data involves these parameters as inputs. Such
synthetic forward models are highly valuable if the computational
expense is reduced considerably. The above analyses are under active development and will be presented in the near future.
3500 4000 4500 5000 5500 6000 6500 7000
[Ã…]
0.2
0.4
0.6
0.8
1.0
F
NGC_4450
FSPS, fiducial
FSPS, MAH
FSPS, Z
FSPS, V, o
3500 4000 4500 5000 5500 6000 6500 7000
[Ã…]
0.5
1.0
1.5
2.0
2.5
3.0
F
NGC_0855
FSPS, fiducial
FSPS, MAH
FSPS, Z
FSPS, V, o
Figure A1. Comparison of observed (blue) and synthetic spectra (orange, green, red, and purple). Left panel: Observed spectra from a quenched galaxy in
Brown et al. (2014) compared with the closest fitting synthetic spectra (orange) generated from FSPS. The spectra obtained by perturbing the slope of the mass
accretion history ğ›½MAH, stellar and gas metallicity ğ‘, and the dust attenuation of old stars ğœ
ISM
ğ‘‰
from those of the best fitting-template are shown in green, red
and purple respectively. Right panel: Similar spectra as the left panel, but for a star forming galaxy.
APPENDIX A: THE PRECISION OF FSPS SPECTRA
In this appendix, we show that the synthetic SEDs produced following
the methodology described in Section 2.1 are indeed representative
of SEDs from observed galaxies. Figure A1 shows the comparison
between synthetic and observed spectra. In the left and right panels,
blue lines depict the spectra of a randomly selected quenched and
star-forming nearby galaxy Brown et al. (2014), respectively. Orange
lines show synthetic spectra that are representative of the spectra of
the previous galaxies. Even though we selected these spectra based
on visual inspection, we can readily see that there is a reasonable
match between observed and synthetic spectra.
It is important to note that the resolution of synthetic spectra is
worse than that of observed spectra, which explains the smoothing
of spectral features like emission and absorption lines. We do not
produce spectra with a higher resolution because we are only interested in broad-band colours throughout this work, and the result of
convolving high- and low-resolution spectra with broad-band transmission curves is essentially the same.
In Figure A1, we also explore the sensitivity of synthetic spectra
to the parameters of our model. To do so, we perturb one of the
parameters corresponding to the best-fitting model while holding
the others fixed to the best-fitting value. We can see that the largest
deviation between the unperturbed and perturbed spectra arises when
varying the slope of the host halo mass accretion history, ğ›½MAH,
which is indicated by a green line. In comparison, the stellar and gas
metallicity ğ‘, and the attenuation of old stars (parameterised as given
in Charlot & Fall 2000) present smaller impacts on the spectra.
